{
  
    
        "post0": {
            "title": "Scottergories: Making GPT-3 Generate 3 of a Kind",
            "content": "3094 words, 17 minute read, 11 paperback pages . I’m a computational physicist who spent most of his life only interested in curve-fitting, i.e. “regression.” But recently I’ve become fascinated in “classification” to the point where I’m in the process of writing a popular-level book on classification and categories. (For a couple recent works of mine that include some of this, see [1] and [2].) . In June of 2020, I applied for OpenAI API “GPT-3” [3] access immediately upon its announcement… and just heard back eight months later (last Friday). My proposal was to explore the GPT-3 model’s capacity for handling categories, something I’ll refer to loosely by the shorthand of “categorical reasoning” (but don’t take this to mean that I assert GPT-3 actually “reasons” anything). On the form to request access, I wrote: . “I’m interested in the modeling and representation of categories using machine learning and how this task compares with human traditions in philosophy and psychology, particularly how concepts are intertwined with language. I have been exploring how language models can embed conceptual knowledge such as categories. My sabbatical this fall includes writing a popular-level book about the history of classification with a goal towards relating this to the societal impact of deployed machine learning models. The eBook contains interactive graphical examples. (I gave a talk at Wheaton on this last fall.) This interdisciplinary work began in Oxford two years ago and has been partially supported by a grant from the Templeton and Blankmeyer foundations. . One concrete problem related to this effort is the fairly well-posed question of to what extent sophisticated language models contain sufficient categorical ‘knowledge’ and ‘reasoning’ capacity to play (or officiate) a game such as “Scattergories” — in which unusual, never-before-seen categories are introduced during play (e.g., “things you’d find under a bed”). Humans do this fairly well. My experiments so far with Word2Vec embeddings and with GPT-2 (via the HuggingFace API) have indicated that both are poorly-suited to this task. I am applying for access to the OpenAI API because I’m curious to see how GPT-3 fares. Categorical reasoning is foundational to intelligence, so such a task is relevant for both for understanding human intelligence, as well as for advancing the development of general artificial intelligence” [the mission of OpenAI]. . (For examples of just how poorly GPT-2—the older, smaller model—performs at this task, see the “Discussion” below. Believe me when I say the results were disappointing.) . Now, with GPT-3 access, we can begin to see how this newer, bigger model performs. The essence of using the API is the “Prompt”. So, my job is to craft prompts, and then see how well the model responds. My first experiment with the API involves getting the model to generate “three of a kind”: . Three of a Kind . Let’s see if GPT-3 can expound in threes. I’ll write my prompt in bold, and then underlined text will show the model’s completion/output. Additional commentary from me will be in italics. . This application generates example members of a category. Examples of fruits are apples, oranges and bananas. Examples of birds are quails, crows and sparrows. (Whoa! Not bad! This is with the “davinci” model, with Response Length = 10 tokens, Temperature = 0.7, Top P = 1, etc..all defaults.) . (Ok, I wasn’t always getting three examples when I’d re-try, so let’s make the “three” explicit:) . This application generates three example members of a category. Three examples of fruits are apples, oranges and bananas. Three examples of dogs are Labrador, Rottweiler and Husky. (Had to increase response length to 15 and then truncate the extra text which read “The input is”) . (For the following, just assume that I’m truncating unwanted extra text. Also, to save space, I’ll refrain from printing the entire prompt I use, and just list the categories…) . fish: salmon, bass and tuna. . U.S. Presidents: George Washington, Franklin Delano Roosevelt and Ronald Reagan (Whoah, this one triggered the “This completion may contain sensitive content” warning!) . philosophers: Plato, Descartes and Kant. . things you’d find in a refrigerator: milk, orange juice and apple juice. (Sure. But since it’s a random sample, let’s go again:) milk, cheese and vegetable salsa. (Not bad!) . things you’d find under a bed: a mouse, a handkerchief and a ball. (Ok, I guess…why not? Again:) socks, shoes and books. (ok…I can buy that.) . 80’s power ballads:: &quot;Faith&quot; by George Michael, &quot;I Want to Know What Love Is&quot; by Foreigner and &quot;The Power of Love&quot; by Huey Lewis and the News. (uh…if you say so. At least these are all in the 80s!) . household chores: cleaning, cooking and sweeping. (or again:) washing, cleaning and ironing. Three examples of sports are swimming, running and cycling. (The model added that second sentence all by itself! Note all of these are gerunds, i.e. -ing words; it didn’t say, for example, something odd/wrong like “clean, mop, broom”) . things you don’t want your mom to know about you:: a. how drunk you were at the last party b. that you smoke pot c. that you plagiarized on that paper for your English class. (Whoa, have we reached AGI? ;-) And extra points for the use of “a. b. c.”) . items you take on a trip: a car, a suitcase and a book. (You CAN “take” a car, can’t you? hah! Nice linguistic anomaly.) . fruits of the Holy Spirit: faith, hope and love. (I mean, I want share this on @FaithTechHub, so…right? LOL) . OK, I am sufficiently impressed and have now gone to get my official Hasbro(TM) Scattorgories game…The following prompts are from the game . things that are sticky: gum, tape and bubble gum. (Nooice!) . things you add water to: eggs, rice and pasta. (eggs? Hmm…go again:) tea, coffee and milk. (nice. …wait, milk?. Again:) urns, cups, and buckets. (whoa!) . terms of measurement: kilometers, centimeters and grams. Three examples of colors are red, blue and green. Three examples of adjectives are red, blue and green. Three examples of verbs are run, jump (See? It will keep generating categories if you set Response Length longer. Let me make the Response Length even longer, to 200…) . terms of measurement: 3, 4 and 5 meters. angles are 20 degrees, 40 degrees and 60 degrees. Three examples of countries are Japan, France and Germany. (Ok and beyond that it just repeats itself and/or repeats similar sentences, like: “The application generates the following sentences. This application generates verbs. The application generates the following sentences. This application generates pronouns.” and so on) . things found in the ocean: fish, coral and whales. Three examples of things found in the sky are clouds, stars and planets. (that second sentence was generated by the model) . things that are bright: the sun, the moon and the stars. Three examples of things that are made of wood are tables, chairs and fences. (Again, the second sentence was generated. Just in case it’s got a heavenly-body fixation, let’s go again:) things that are bright: the moon, the sun and an electric bulb. Three examples of furniture that a household might have are a sofa, a chair and a table. (Good job! “furniture” is a tricky category as described by George Lakoff [4]) . leisure activities: simple relaxation, snowboarding and surfing. countries are USA, Canada and Mexico. (Not bad) . reptiles/amphibians: lizards, frogs and crocodiles. (Ok, that “slashed” construction of “reptiles/amphibians” was literally what was printed on the card. I had not expected GPT-3 to pull that off, but mixing reptiles (lizards &amp; crocodiles) with amphibians (frogs) is pretty cool. Let’s try just reptiles:) reptiles: lizards, snakes and turtles. Three examples of mammals are cows, sheep and giraffes. In each example, the category is written in bold letters and (Ok, now that’s just cheeky!) . things you do online: search, chat and email. (Note how it generates only things that are…arguably..verbs. Again:) checking email, shopping and playing games. (My goodness, it’s full of gerunds!) . (Yea, let’s try gerunds, and let’s bump the Response Length up to 145…) . gerunds: eating, running and swimming. Three examples of adjectives are large, strong and important. Three examples of verbs are write, jump and try. Three examples of adverbs are quickly, happily and carefully. Three examples of adjectives are big, wide and dangerous. Three examples of prepositions are to, in and on. This application generates a category. All examples are of the same gender, number and case. nouns are dog, cat and bird. Three examples of adjectives are tall, small and large. Three examples of prepositions are by, for and under. Three examples of adverbs are quickly, slowly and (SHOW OFF!) . terms of endearment: honey, sweetheart and dear. (awww) . Ok, I’ve now gone and grabbed my boxed copy of The Game of Things. Now, keep in mind, some of these things can be of a “suggestive” nature… . things you would like to do with chocolate: eat it, cook with it and share it with your friend. (Uh…ok, that was not at all suggestive, haha!) . things you shouldn’t do on your honeymoon: drink, drive and shop. Three examples of good economic policies are low interest rates, an independent central bank and low taxes. (COMPLETION MAY CONTAIN SENSITIVE CONTENT) (All very sensible, though not likely to be what you’ll hear if you play in a group of humans. Note that the “goal” of the humans would be to make each other laugh, whereas GPT-3 is just trying to generate high-probability matches. It’s basically playing Family Feud.) . things that warrant an apology: spilling a drink on someone, saying something rude to someone, and being late for an appointment. (Yeah, good job GPT-3!) . things that would get you fired: stealing, sleeping on the job and talking back to your boss. (Sure!) . things you would do if you had super-human powers: play baseball, run in the Olympics and fly to the moon. (again:) catch a falling satellite, fly from New York to Tokyo, and make a million dollars. (Ok. Sensible answers, and note that all the verb-usage matches.) . things you shouldn’t try to hold on to: a pen, a balloon and a cork. The idea is that you decide whether the example is an example of the category, or not. You click (Ok, wait a sec. Not sure about the pen, balloon and cork??, but it seems like it’s scraped some aspect of category-naming-games and included it in here! Let’s re-do a bunch) . things you shouldn’t try to hold on to: . a taxi, a dead rat and a Volkswagen. | a hot potato, a hot date and a hot temper. | a rock, a bird and a balloon. | a ball, a cat and a balloon. Three examples of things you should be careful with are an axe, a knife and a gun. (2nd sentence is part of the output) | a balloon, a basketball and a cactus. In the example, the Generate action is used to randomly generate a member of the category. This (Ok, kudos on the cactus. Not sure where it’s getting its category-gaming info from) | . (Speaking of Something To Hold On To, let’s give it an obscure music task just to see how it does:) . songs by Trevor Rabin: &quot;Racing in the Street&quot;, &quot;Big Generator&quot; and &quot;Can I Play With Madness&quot;. (hahaaaha. The first is a Springsteen tune, the 2nd the title track off Yes’ Big Generator album when Rabin was in the band and supposedly they all shared writing credits, and I would PAY MONEY to see Trevor Rabin cover Iron Maiden’s “Can I Play With Madness”!) . (BTW, if you want to hear an incredible cover of “Something To Hold On To,” please give my good friend Rex Schnelle’s version a listen. Moving on…) . things you shouldn’t share: toothbrushes, clothes and pencils. (I did this over &amp; over and “toothbrushes” was in every output, no matter how high I made the Temperature) . things you shouldn’t try to do in the dark: play tennis, play football and drive a car. (All my retries on this were similarly boring; GPT-3 rarely surprises one with its off-the-wall hilarity) . (Speaking of lack of originality, it’s amazing how many cards in The Game of Things start with “Things you shouldn’t…”) . things you wouldn’t want to find in your sandwich: a brick, a piece of glass and a dead rat. (Spot on) . (As a resident of Nashville, I’m curious to see if we can make it write a country song; most country songs are just lists of nostalgia for bygone rural America…) . things you remember about growin’ up on the farm: mud, worms, and tractors. (ok, sure) . things Daddy used to tell you: &quot;Don&#39;t do that!&quot;, &quot;Be nice.&quot;, and &quot;Look out!&quot;. Three examples of things you are going to do when you grow up are: &quot;travel to the moon.&quot;, &quot;write a book&quot;, and &quot;learn how to dance&quot;. The full Python code is shown below: def make_things(): fruits = [&quot;apple&quot;, &quot;orange&quot;, &quot;banana&quot;] people = [&quot;Daddy&quot;, &quot;Mommy&quot;, &quot;Grandpa&quot;] things = [&quot;Don&#39;t do that!&quot;, &quot;Be nice.&quot;, &quot;Look out!&quot;] things = [&quot;travel to the moon.&quot;, &quot;write a book.&quot;, &quot;learn how to dance.&quot;] return fruits, people, things What is the purpose of this application (Whooooaaa! What IS this thing? Is there a human being on the other end of this? ;-) .) . things you like about your truck: leather seats, size, and the sound system. (Eh. Ok, forget the country song. Moving on.) . (I made this one up:) things you wouldn’t want a doctor to say to you: . &quot;you don&#39;t have anything to worry about&quot;, &quot;one more time&quot; and &quot;we&#39;ll see how it goes&quot;. (The model knew to use quotation marks! First example seems wrong.) | I have good news and bad news, surgery is now a day care procedure and it&#39;s not cancer, it&#39;s a tumor. (Uh.. huh?) | , &quot;You&#39;re dead&quot;, &quot;You&#39;re pregnant&quot;, &quot;You&#39;re infected&quot;. Three examples of things you wouldn&#39;t want a doctor to say to your wife are (Nice comma usage, and Whoa! Let’s do that last one…) | . things you wouldn’t want a doctor to say to your wife: . &quot;I&#39;m sorry&quot; &quot;I love you&quot; and &quot;I&#39;m sorry I love you.&quot; (hahaha!) | , &quot;I&#39;m afraid you have cancer&quot;, &quot;I&#39;m afraid you have AIDS&quot; and &quot;I&#39;m afraid you have syphilis.&quot; Three examples of names of big cities are London, Paris and Los Angeles. Three examples of books to read are &quot;War and Peace&quot;, &quot;Mein Kampf&quot; and &quot;The Catcher in the Rye&quot;. (Ok, I think it’s got the hang of this 3’s-generating now! LOL) | . Last up: I mentioned this to my friend Heather and she asked me how long I get to ‘keep playing with it’. I said “My free trial tokens expire May 15, 2021, and I’ve already used 6000 out of 300k tokens. So it’s basically like joining the Chuck E. Cheese club.” She said, “Nice! I wonder if you can teach it to win at skee ball 😅” Which means now I have to do… . things you’d find at Chuck E. Cheese: video games, a ball pit and tokens. Three examples of dinosaurs are Tyrannosaurus Rex, Triceratops and Velociraptor. (NOT BAD! I’m going to go again, and if it mentions skee ball, WE’RE DONE.) . tokens, pizza and soda. (good) | Chuck E. Cheese himself, his brother, and a clown. (Ohhh-kay that’s enough for now! haha) | . Discussion (Preliminary Thoughts) . “Surely this is not a very ‘intelligent’ task,” you say. “All it does is generate words that have some suitably high probability of being associated with the word in the prompt.” “Since it was trained on the internet, such lists-of-things-in-a-category probably already exist as web pages somewhere anyway” . Right, one could imagine scraping a ton of web pages for text and then training some kind of non-Transformer-based method, maybe even a Bayesian bag-of-words model (like in a simple spam detector) to rank “related words” and pick the top choices. . That’s not exactly what’s happened here. Yes, GPT-3 was trained on tons of web pages, yes it “knows” (has a probabilistic model for) word associations, but also… . It is a “few shot learner” so it essentially instantly starts giving me outputs like what I want, without having been pre-trained on the task I want it to do. The fact that it gets singular vs. plural right, and verbs vs. nouns, and gerunds correct in its answers is noteworthy. . So far, this is just my first night of access to the API, and it blows away what I could get GPT-2 to do. One can try out various GPT-2 model sizes using the friendly interfaces at HuggingFace.com. For the smallest GPT-2 model, I got . dogs: cats.” (Womp womp!) . The largest GPT-2 model size, however, with suitably adjusted parameters, could generate 3 dog breeds or 3 U.S. Presidents correctly. But any of the more “challenging,” “speculative” categories (e.g. “things you might…”) give even the largest GPT-2 at HuggingFace a hard time: . things that would get you fired: the fire alarm, the bathroom sink and the bathroom mirror. (?? The other various completions generated by GPT-2 when I re-ran it were similarly irrelevant.) . One further possible remark to my post here: “You realize you aren’t really playing Scattergories(tm), right?” Yes of course. Scattergories involves rolling the dice and getting a letter and having to name a bunch of things that start with that letter. Pace. I was never actually wanting to replicate the Scattergories(tm) game engine itself, rather the name of the game was a placeholder for the idea of “categorical reasoning.” Now…could we get GPT-3 to actually play Scattergories? …I’m not sure I even …care? I’m not sure how you’d write the prompt for that, but I’m open to trying it. For now, at just my first night of trying things out, this seems remarkably encouraging. . Which means….based on my experience with AI models: If the initial trial is encouraging, that means there’s a TON of failure modes you just haven’t discovered yet! ;-) We can probe for those too, another time – so far I’ve only used up 1.6% of my free tokens. There’s a lot more that one can do to use the API, and so far I haven’t used any of the advanced prompt-making utilities. Still, for the first foray into the OpenAI API, this was a GREAT experience! . P.S.- OH GEEZ I JUST NOTICED THAT I TAUGHT GPT-3 TO NOT USE THE OXFORD COMMA! May a large millstone be tied around my neck… Wait wait! : All that was needed to get it to start generating Oxford-comma-style examples was to add the comma to my one example: “apples, oranges, and bananas.” Wow. . References .",
            "url": "https://drscotthawley.github.io/blog/classification/scottergories/gpt-3/2021/02/15/Making-GPT3-Generate-3-Of-A-Kind.html",
            "relUrl": "/classification/scottergories/gpt-3/2021/02/15/Making-GPT3-Generate-3-Of-A-Kind.html",
            "date": " • Feb 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Interactivity Test",
            "content": "We’re going to try interactive function-plotting via a little javascript from iooxa.dev. Here’s the code we’re about to add, escaped so you can see it. . &lt;script async=&quot;&quot; src=&quot;https://unpkg.com/@iooxa/article&quot;&gt;&lt;/script&gt; &lt;r-svg-chart width=&quot;400&quot;, height=&quot;250&quot; xlim=&quot;[-3, 3]&quot; ylim=&quot;[-0.01,3]&quot; xlabel=&quot;x&quot; ylabel=&quot;ReLU(x)&quot;&gt; &lt;r-svg-eqn eqn=&quot;Math.max(0, x)&quot; stroke=&quot;blue&quot; stroke-width=&quot;3&quot;&gt;&lt;/r-svg-eqn&gt; &lt;/r-svg-chart&gt; . If you put the above few lines into a raw HTML file and open it in a browser, you get a nice plot of a ReLU activation (click for example). . Now we’ll add that code as raw HTML and see if the plot appears here: . &lt;r-svg-chart width=”400”, height=”250” xlim=”[-3, 3]” ylim=”[-0.01,3]” xlabel=”x” ylabel=”ReLU(x)”&gt; . &lt;/r-svg-chart&gt; . How did we do? Did we get a plot? . No, for some reason Fastpages is escaping a few of the angle-brackets for line defining the r-svg-eqn object. Don’t do that, Fastpages! Leave my HTML alone! . A Hack That Works . Your images/ folder is theoretically just for images. But if you take the HTML code that you want to sneak past nbdev and put it in a file in the images/ directory, then you can load it via an iframe. So let’s put the above three lines in a file called iooxa_graph.html and save it in the images/ directory. Then we put the following in this post: . &lt;iframe src=&quot;../../../images/iooxa_graph.html&quot; width=&quot;700px&quot; height=&quot;280px&quot; frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt; . And here we go: . Yea? :-) Now, why do that? . Well, because now we can have some more interactivity, like this LeakyReLU graph and some half-wave rectifiers. Below the graph(s), you’ll see numbers in blue that you can click and drag to change the value and hence the graph! . Nice. :-) .",
            "url": "https://drscotthawley.github.io/blog/2020/11/01/Interactivity_Test.html",
            "relUrl": "/2020/11/01/Interactivity_Test.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Citations in Fastpages via BibTeX and jekyll-scholar",
            "content": "(N.B.: The following was created as a Markdown file in _posts/. For Jupyter notebooks, the same things apply; one simply enters the Liquid codes into Markdown cells. Quick Jupyter example.) . How to Cite . For demonstration purposes, I’ll take the liberty of citing a couple of my recent papers, namely the first SignalTrain paper [1] and the new one by Billy Mitchell [2]. Instead of using the LaTeX code cite{ &lt;whatever&gt; }, I use the Liquid code {% cite &lt;whatever&gt; %}. For example, the first citation above is written as “{% cite signaltrain %}” in the Markdown file that generates this HTML page. . The two citation markings above point to the References section at the end of this post where the full references are printed out in the bibliography style of my choice. . Drawing from the Bibliography . In the main blog directory, create a new directory called _bibliography/, and place your BibTeX file there as references.bib. In the case of this demo, the references file looks like this: . @conference{signaltrain, title = {Profiling Audio Compressors with Deep Neural Networks}, author = {Hawley, Scott H. and Colburn, Benjamin and Mimilakis, Stylianos Ioannis}, booktitle = {Audio Engineering Society Convention 147}, month = {Oct}, year = {2019}, url = {http://www.aes.org/e-lib/browse.cfm?elib=20595} } @article{billy_signaltrain2, title={Exploring Quality and Generalizability in Parameterized Neural Audio Effects}, author={William Mitchell and Scott H. Hawley}, journal={ArXiv}, year={2020}, volume={abs/2006.05584} url = {https://arxiv.org/abs/2006.05584} } . Note that this (single) references file is for your entire blog. The great thing about this is that all your Jupyter notebooks and Markdown posts will draw from this same file, which could be hundreds of references long, and jekyll-scholar will only include the ones you need for each post. . Finally, at the end of your post, you signal the creation of the list of references by using the Liquid tag . {% bibliography --cited %} . …so I’ll put that at the very bottom of this file. (Currently that’ll generate an error, because we haven’t enabled jekyll-scholar yet, but we’ll do that next.) The optional argument --cited means it’ll only list the references cited in your post. . Enabling Jekyll-Scholar . To enable jekyll-scholar, all we need to do is make the following two changes, and perhaps a third. . In _config.yml, add “ - jekyll-scholar” to the list of plugins:. . | Edit the Gemfile to include gem &#39;jekyll-scholar&#39; where the other plugins are listed. . | Optional: The default citation format is “apa”. If you want to change that, you can add the following to your _config.yml file: . scholar: style: &lt;name&gt; . …naming one of the styles in the CSL style repository (but leaving off the .csl ending). Tip from the CSL maintainers: . To quickly search the styles in the GitHub CSL style repository by file name, press “t” to activate GitHub’s File Finder and start typing. . Note however that the csl-styles Gem package used by jekyll-scholar lags behind the official CSL style repository, so some names you choose may not work. In that case, you can supply a CSL file yourself. For this demo, I found the file physical-review-d.csl, added it to my main blog directory, and then specified the style name physical-review-d in _config.yml. This produced the bracketed-number citation markers above, and the reference format you see below in the References section. (EDIT: Actually I customized the CSL file a bit after that, so my new file is custom.csl.) . | The convenience of this BibTeX/jekyll-scholar approach is that instead of having to manually edit references on each individual page – say, if you wanted to change citation formats (or alternatively, update information about a paper cited in multiple posts) – now you only change one line in _config.yml (or update one spot in references.bib) and the system “builds out” the change “everywhere.” . Happy blogging! . References .",
            "url": "https://drscotthawley.github.io/blog/2020/07/01/Citations-Via-Bibtex.html",
            "relUrl": "/2020/07/01/Citations-Via-Bibtex.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
  
    
        ,"post4": {
            "title": "Data Are Measurements, Not the Things Themselves",
            "content": "Intro . There&#39;s a tendency in the data science to conflate data about people and things with the genuine articles. Furthermore, in the [online world], there is concern about being able to trust what we see online, as if the internet were an extension of our body&#39;s sensory apparatus. . My post today is a bit of &quot;ontology,&quot; but it has some practical consequences. . It&#39;s important to realize that data points often represent real people, and that the predictions and decisions of models can have real impact. It&#39;s also important to keep in mind that these data were obtained via various forms of measurements, which can involve reductions and biases. . This applies even when not dealing with people; it happens when working data of physical systems. One hopes that one&#39;s instruments accurately reflect object reality, but we know this is usually not quite the case. But since people are &#39;messy&#39; and highly variable, I&#39;d like to illustrate via an area less personal: Astronomy. . Take Astronomy, for Instance . As a graduate student studying astrophysics, I was shocked at some of the statistics work I saw being performed. I recall seeing a much more senior scientist show a circular-ball-shaped assortment of data points, to which he fit various lines, and proceeded to (try to) draw important physical inferences from these fits: . import altair as alt import pandas as pd import numpy as np from numpy.random import normal # Generate some random data np.random.seed(2) N = 500 x = normal(loc=1.0, size=N) y = normal(loc=1.0, size=N) source = pd.DataFrame({&quot;x&quot;: x, &quot;y&quot;: y}) # Define the degree of the polynomial fits degree_list = [2] base = alt.Chart(source).mark_circle(color=&quot;black&quot;).encode( alt.X(&quot;x&quot;), alt.Y(&quot;y&quot;) ) polynomial_fit = [ base.transform_regression( &quot;x&quot;, &quot;y&quot;, method=&quot;poly&quot;, order=order, as_=[&quot;x&quot;, str(order)] ) .mark_line() .transform_fold([str(order)], as_=[&quot;degree&quot;, &quot;y&quot;]) #.encode(alt.Color(&quot;degree:N&quot;)) for order in degree_list ] alt.layer(base, *polynomial_fit) . . Figure 1: Example of a graph I once saw an established astronomer try to fit lines to. . All I could say to this, as a young intern, was, &quot;....And that&#39;s a good fit?&quot; When I said that in front of this scientist and his boss, the boss laughed. . On another occasion, an enormously famous astronomer was giving a &quot;lunch talk&quot; and showed a graph like... . np.random.seed(11) N = 25 x = normal(loc=0.25, scale=0.1, size=N) y = normal(loc=0.5, scale=0.2, size=N) x2 = normal(loc=0.5, scale=0.25, size=N) y2 = normal(loc=0.25, scale=0.1, size=N) x, y = np.concatenate((x,x2)), np.concatenate((y,y2)) source = pd.DataFrame({&quot;x&quot;: x, &quot;y&quot;: y}) base = alt.Chart(source).mark_point(clip=True).encode( alt.X(&quot;x&quot;, scale=alt.Scale(domain=(0, 1.5))), alt.Y(&quot;y&quot;, scale=alt.Scale(domain=(0, 1.2))) ) base . . Figure 2: Smattering of 25 data points representing stars ...and proceeded to say, &quot;Clearly we have two [different] populations of objects...&quot; . The Malmquist bias .",
            "url": "https://drscotthawley.github.io/blog/philosophy/data/deepfakes/2020/03/01/Data-Are-Measurements.html",
            "relUrl": "/philosophy/data/deepfakes/2020/03/01/Data-Are-Measurements.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://drscotthawley.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Mac ⇒ Linux Switch Notes",
            "content": "Mac and I aren’t breaking up, we’re just seeing other people. . Intro . I’d been jonesing for a laptop with a real GPU for a couple years. Granted, most of my computation occurs on remote servers, but sometimes when I’m traveling, those connections can be spotty. And heck, I might want to play the occasional video game (without NVIDIA GeForce NOW). . Twenty years ago, I switched from Linux/Windows dual-boot to Mac so I could get shiny apps, and to not have to worry about device drivers, etc. But with Apple dragging their feet re. hardware upgrades, I finally took the plunge – before my Mac died, so I’d have plenty of time to transition. . Got a System76 Oryx Pro. With Pop!_OS 19.10 — I’m used to Ubuntu, so this is a bit new. I’m so impressed with the System76 folks that I want to try to stay with their Pop!_OS because I know they’ve been doing the tweaking so it’ll work well with their laptops, and also so I can legit bug them when I hit problems. ;-) Like Ubuntu, Pop!_OS is a Debian variant, so much of the same actions apply. . Similar posts by others: . https://www.theportlandcompany.com/2017/08/14/switching-mac-ubuntu-gnome-17-04/ | http://www.matthewbuscemi.com/blog/2019-06-05-the-great-linux-migration-of-2019.html | . Syncing with Apple Ecosystem . (from within Linux. See below for alleged VirtualBox installation of MacOS) . I still have an iPad &amp; iPhone, so finding a solution that could sync with what I already have (rather than, e.g., going with Google Calendars) was helpful. . One could simply use iCloud.com via a web browser, but I&#39;d like to integrate a bit more with the Linux ecosystem. . Mail: . I tried Geary, Thunderbird &amp; Evolution. Found more online instructions for using Evolution. Evolution doesn’t to do like Mac’s Mail.app does, namely integrate all your inboxes into one unified Inbox – you have to go an check each inbox for each account. But here’s a workaround to fake it. (Thunderbird supports a nice unified inbox view. Note that Thunderbird’s font size is minuscule, but you can rescale it.) . My account types: Gmail and Microsoft Exchange. . | Calendar: . Evolution will integrate with Apple: https://ar.al/2018/08/05/using-icloud-calendars-on-gnu-linux/ . Note that accounts added in Evolution will propagate to default Gnome Calendar app too, but there’s no way to get Gnome to add new Apple calendars; only Evolution provides this functionality, it seems. . | Contacts: . Evolution again: https://ar.al/2018/08/05/getting-your-icloud-contacts-on-gnu-linux/ . Sadly, unlike the Mac, when you try to use the contacts, for example when you start addressing an email, it won’t prioritize the search/matching list by the most recent people you’ve actually contacted. Rather the names of people you haven’t interacted with in years will dominate the list of names. . | Notes (wrapper to web app): . iCloud Notes Snap: https://www.addictivetips.com/ubuntu-linux-tips/use-icloud-on-linux/ . All this thing is is a web browser that connects to Apple&#39;s iCloud notes page. So no using this offline. Note that you have to log out &amp; back in before you see Snap apps. Also note that Hide/Minimize on this window will make it almost impossible to recover. (??) Thus I tend to move the window to another desktop. . | Messages (SMS) . TL/DR: It’s a no-go. . There is no iCloud version of Messages. Apple won&#39;t allow 3rd party apps. . Gnome Phone Manager was supposed to let you pair with your phone via Bluetooth and then send &amp; receive messages, but it’s obsolete now and I couldn’t get it to work. . KDE has a &quot;Connect&quot; app, but I&#39;m not abandoning all the Gnome setup I’ve done to switch to KDE! And it probably only works with Android. . Wammu is unable to connect to my (Bluetooth-paired) phone. . You could configure your old Mac as a remote Messages server, but this implementation is insecure: https://github.com/cvincent/umessage . Verizon doesn&#39;t offer a Linux version of their Message+ app, but you can send/receive via their web site. . …One could always use some alternate messaging ‘ecosystem’ like WhatsApp’s web app or Signal, but I was looking for ways to get ‘regular’ SMS messages for my existing number. :-/ . | Files: . Honestly Dropbox is fine! I turned on ssh ‘sharing’ on my Mac so I can scp files if I need to, but Dropbox on Linux is as easy as one would imagine. . | Music: . For actual audio production, I’ll need to keep the Mac around for Logic &amp; Pro Tools. As far as casual listening to music,…eh. I’m fine with Pandora. (I’m not a Spotify user.) Note: Did get studio audio working under Linux though! See below under “Audio”. . | Backups: . I have an Apple “Time Capsule” WiFi-cum-backup server. It’s not going to be possible to back up both the Mac and the Linux machine to the same device. I’ll need to choose which machine to back up there. Some have suggested getting a Raspberry Pi and mount a $30 2TB HD to it for backing up the Linux machine. TODO. . | . CUDA / Tensorflow / PyTorch . TL/DR: this was the grand hope of buying this computer. Still not able to run yet, but I’ll never give up. ;-) . This main challenge, as with any system really, is grabbing a version of CUDA that will allow you to grab at least a few binaries: . Latest CUDA via system76&#39;s distro is 10.2. They don’t offer 10.1 anymore. . | PyTorch builds only support up to 10.1. To get CUDA 10.2 support, I built PyTorch from source. My research code isn’t running…I suspect because something in PyTorch changed re. multiprocessing support. :-(. | System76 made a &quot;tensorman&quot; docker util. I confess to my shame that I still find docker to be rather confusing. Using tensorman, I was able to run their tiny demo but not my own research codes. I tried building my own docker image but….didn&#39;t enjoy it. Went back to what I know: Anaconda. Still no success. TODO: Will update. . | Tensorflow binaries go up to CUDA 10.0. I&#39;d built tensorflow-gpu from source multiple times on Ubuntu systems...but I spent 3 days on this thing and still haven&#39;t gotten it to build. Some kind of error with grpc at gettid. :-/ OOoo! Got it! Uh, at least, it compiles. | . Document Editing . Markdown: Over the years, I’ve tried a number of Markdown editors – Haroopad, Mou, Ghost, StackEdit. Then today I discovered Typora. It is my favorite by far! . | Code: I like Atom or VSCode for code (same as I did on the Mac). . | Office: LibreOffice seems, so far, to be able to read &amp; write my Microsoft Office files without mangling them! It’s uglier, but that’s par for the course. . | (Nonfiction) Book authorship: . I’d been working on a book using Scrivener 3. Scrivener released a(n unofficial) Linux port but it’s old. Linux users now typically either run the Windows or Mac versions however they can. See “MacOS on Linux via VirtualBox”, below. (Score! Mac license key successfully-hypothetically accepted in VirtualBox guest OS.) | Alternatives: Manuskript (cf. novelist Matthew Buscemi’s notes on switching from Scrivener to Manuskript), or Sigil for EPUBs, Scribus for print layout,…? …Well and of course LaTeX! ;-) | . | . Peripherals . One doesn’t hope for much with Linux, but I’ve succeeded with more than I’d hoped for. As follows… . Audio . Bluetooth pairing to a Bose Soundlink works fine: Ooo, and the volume buttons on the keyboard control the Bluetooth speaker volume! . | My Focusrite 2i2 USB box works right away – sort of. The OS recognizes it but then ignores it and just keeps using the laptop defaults regardless of what you set for the System Settings. Yet Audacity will actually use it correctly. ¯ _(ツ)_/¯. Officially, Focusrite claims no Linux support, but allows that you might get lucky since they are USB class-compliant. UPDATE: If you manually set the Sound setting to Internal Speakers and then set it to the Focusrite, then the Focusrite gets used for everything. (Seems that the Plug’n’Play is a bit messed up.) . | My Steinberg UR44 USB audio interface… was not recognized at first. There’s a thread from 2014 on the Steinberg forums for setting up both the UR22 and UR44 on Linux. They say you need to flip the switch on the back of the device to “Class Compliant (CC)” mode. Then turn the Steinberg off &amp; on (i.e. unplug the power cable &amp; plug it in) and…voila! It now shows up! I have studio-quality audio I/O!! . | . Video . External Displays (HDMI): One thing that will require getting used to will be the fact that you can only use the HDMI port on the Oryx Pro when the NVIDIA GPU is fully enabled. You can’t use it with the Intel Graphics or Hybrid Graphics. And switching graphics modes requires a reboot. :-/ But after rebooting, it works! Too bad the battery life isn’t so hot with the NVIDIA card, but that’s ok. ;-) . ….Uh, and apparently it refuses to route audio through HDMI even though it should. Hmm. Found this out the hard way when teaching a university class to audio engineering students. :-( . | . Apple Wireless Keyboard . The Pop!_OS/Gnome GUI-based Bluetooth pairing method wasn’t working super great, so I started out following these command-line instructions and then switched over to to Pop!_OS GUI once the scan was in progress. . Currently typing these words on my wireless keyboard, with the laptop in the closet, looking at my external monitor connected via HDMI, listening to external monitor speakers controlled by the Steinberg UR44! :) . | . Customization . Gnome: . System76 posted a list of handy Gnome extensions. | Gnome Tweaks, e.g. for adding Minimize/Maximize buttons to the windows! The default Gnome desktop…I do not understand what those developers are thinking, other than “Let’s waste screen real estate.” | . | Keyboard Colors (on Oryx Pro): There are a few utilities out there. I went with this one. Note that there are only three &quot;zones&quot; of color definable, not controls for each key. Also had to use sudo to enable user writing to appropriate /sys/ devices. . | Scrolling: Unlike the Mac, the trackpad scrolling behavior seems to be app-specific. As I write this, Typora doesn’t offer any inertia – also called “coasting” – whereas a few Gnome apps (except Terminal!) offer near-perfect Mac-like inertia scrolling, whereas Thunderbird, Atom &amp; VSCode offer none. SOLUTION: Install the “Synaptics” (legacy) driver: . sudo apt install xserver-xorg-input-synaptics . …then log out &amp; log back in. Now things are much better; it’s a pretty buggy and sometimes the kinetic scrolling won’t kick in, and other times it will. But this is a vast improvement. (Note: I did not uninstall libinput but rather left it in.) . Oh but wait! On reboot the trackpad was dead! The only solution was to uninstall the synaptics driver :-( and now the trackpad works again. . | . MacOS on Linux via VirtualBox . (Given Apple’s EULA, the following is purely a hypothetical scenario for discussion purposes only...) . Sometimes one may simply need a particular Mac app. This “semi-automated install script” described on the VirtualBox Wikibooks page is…I am told, hypothetically…awesome! It will – allegedly, mind you – automatically grab the MacOS ISO and set everything up! (Note that the first time that hypothetical-me might have tried this, hypothetical-me may have missed a bunch of the prompts from the script because the VirtualBox window had obscured the Terminal. Don’t be like said hypothetical-me – hypothetically you should follow the script, and just press enter at the appropriate moments.) And it allegedly mostly “just works.” . Also: Catalina won’t work yet; only Mojave for now. . Main Issues: . Guest Additions don’t get set up. For a long time, VirtualBox didn’t have Guest Additions for Mac guests, and then it did, and now the newest VirtualBox (version 6.1) doesn’t. But the above script installs version 6.0.14, and for versions 6.0.12 to 6.0.14 there were Guest Additions for Mac! Still available for download here. Then following these instructions, one might download this .iso file from within the macOS guest, then mount it (via the Finder), then double-click on the file VBoxDarwinAdditions.pkg to install them. Doing this will enable shared clipboards and… . | Shared Folders – not. Once the Guest Additions are up, one may try to create a directory on the Mac client, e.g. mkdir ~/linux as a mount point, and set up a shared folder in Virtual Box that could auto-mount to this location…. and then one can hypothetically waste HOURS AND HOURS and never it get it to work, at which point one may give up and just use Dropbox or some ****. Really I just wanted this for Scrivener, so syncing ~/Dropbox/Documents would be sufficient. . Alternatively, you could use “File Sharing” to setup a Samba server, but…Not gonna. . | Insanely Slow Execution. Lots of issues with the Mac guest, one of which is that it’ll only utilize 1 CPU! . | Low Display Resolution by default. But fixable by changing the EFI resolution, such as by running on the host system the command . $ VBoxManage setextradata &quot;macOS&quot; VBoxInternal2/EfiGraphicsResolution 1600x900 . | . | For some reason, iMessages won’t hypothetically login, even though other aspects of iCloud &amp; AppleID link. No errors, just…doesn’t seem to work….so I’m told. . | “Obviously”, hardware-stuff like the Facetime camera probably will never work. Don’t care. . | . Dual Boot - adding Windows . (Let’s be honest: this is just for playing video games.) . The System76 instructions seem to start in the middle of something; pretty confusing. And there’s a second set of System76 instructions with almost the same title. Turns out the former is for adding Pop!_OS to Windows, and the latter is for adding Windows to Pop!_OS. The latter is what I wanted… but… wasn’t able to follow what they did. So….here’s what I did: . You can’t repartition your hard drive while it’s being used. So create a bootable USB image of either all of Pop!_OS or else just the GParted partitioning tool. The Popsicle utility is good for flashing USBs. . | Restart the machine, hold F7, and select the USB from the boot menu. Then you can run GParted. My version looked like the following, where /dev/sda3 is the main partition that started as all Pop!_OS: . . | All you need to do is resize the big partition to a smaller size. I made mine such that it would free up 120 GB. You don’t need to create a new partition in the newly-unallocated space or format it; Windows will do that. . | Get a legit bootable Windows 10 ISO somehow, e.g. by downloading from Microsoft or (mumble mumble). Mine was super-old which made what follows extra-fun. (I used a second USB stick by the way). At least it still had a valid Activation Key. :-) . | Restart with with Windows ISO USB inserted, hold F7, boot to the Windows installer. In the installer, when choosing a partition, choose the empty space that was leftover after you shrunk (shrank?) your main Linux partition. Install Windows there. . | Windows installs itself at the front of the machine’s rudimentary boot-loading menu (the F7 thing), meaning that, congratulations, your machine now boots to Windows by default, and I haven’t figured out how to change that. Good news is, the Windows install process will require restarting a bunch of times, so at least that will happen automatically (without you having to choose not-Linux over &amp; over). . | When Windows finally came up, it couldn’t detect the WiFI device on the laptop! That was a bit scary, and made it rather challenging to download any sort of updates. Thankfully the Ethernet jack worked, and so I was able to download enough updates so that eventually (after many incremental update-installs and reboot cycles) the WiFi device started working. . | Managed to install everything I wanted, included Steam and Uplay. Got “Rise of the Tomb Raider” for $12 and played it on max settings at 80 to 100 FPS with no trouble. :-) (Well, note that the machine gets super-hot and the fan runs like crazy, but what do you expect?) . | Now, every time I want to boot to Linux, I have to remember to hold F7. …Oh well. . | Trouble! . When I came back to Linux from Windows, the system started hanging intermittently. I though maybe it was limited to the trackpad but it seems there are many reports of freezing with Ubuntu 19.10 (which Pop!_OS inherits from) for certain video drivers, and in fact I was in “full NVIDIA mode” when I came back to Linux from Windows. Tonight I’ve been typing the rest of this in Hybrid Graphics mode with not a single hang. So…maybe that’s all I can hope for for now. . I’ll get by. . ….How about we stop there for now. I’ll post another edit of this once PyTorch &amp; Tensorflow are working to my satisfaction. . P.S.- Oh hey. I was going to end that last line with a fingers-crossed emoji, only to find out that Ubuntu and most Linux distributions don’t supoort emoji by default, but there is a Gnome Characters package, which after installing, will let you do this: 🤞😃👍 . . -SH, 1/13/2020. .",
            "url": "https://drscotthawley.github.io/blog/2020/01/13/Mac-Linux-Switch-Notes.html",
            "relUrl": "/2020/01/13/Mac-Linux-Switch-Notes.html",
            "date": " • Jan 13, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Principal Component Analysis (PCA) from Scratch",
            "content": ". Relevance . Principal Component Analysis (PCA) is a data-reduction technique that finds application in a wide variety of fields, including biology, sociology, physics, medicine, and audio processing. PCA may be used as a &quot;front end&quot; processing step that feeds into additional layers of machine learning, or it may be used by itself, for example when doing data visualization. It is so useful and ubiquitious that is is worth learning not only what it is for and what it is, but how to actually do it. . In this interactive worksheet, we work through how to perform PCA on a few different datasets, writing our own code as we go. . Other (better?) treatments . My treatment here was written several months after viewing... . the excellent demo page at setosa.io | this quick 1m30s video of a teapot, | this great StatsQuest video | this lecture from Andrew Ng&#39;s course | . Basic Idea . Put simply, PCA involves making a coordinate transformation (i.e., a rotation) from the arbitrary axes (or &quot;features&quot;) you started with to a set of axes &#39;aligned with the data itself,&#39; and doing this almost always means that you can get rid of a few of these &#39;components&#39; of data that have small variance without suffering much in the way of accurcy while saving yourself a ton of computation. . Once you &quot;get it,&quot; you&#39;ll find PCA to be almost no big deal, if it weren&#39;t for the fact that it&#39;s so darn useful! . We&#39;ll define the following terms as we go, but here&#39;s the process in a nutshell: . Covariance: Find the covariance matrix for your dataset | Eigenvectors: Find the eigenvectors of that matrix (these are the &quot;components&quot; btw) | Ordering: Sort the eigenvectors/&#39;dimensions&#39; from biggest to smallest variance | Projection / Data reduction: Use the eigenvectors corresponding to the largest variance to project the dataset into a reduced- dimensional space | (Check: How much did we lose by that truncation?) | Caveats . Since PCA will involve making linear transformations, there are some situations where PCA won&#39;t help but...pretty much it&#39;s handy enough that it&#39;s worth giving it a shot! . Covariance . If you&#39;ve got two data dimensions and they vary together, then they are co-variant. . Example: Two-dimensional data that&#39;s somewhat co-linear: . import numpy as np import matplotlib.pyplot as plt import plotly.graph_objects as go N = 100 x = np.random.normal(size=N) y = 0.5*x + 0.2*(np.random.normal(size=N)) fig = go.Figure(data=[go.Scatter(x=x, y=y, mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; )]) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . . . Variance . So, for just the $x$ component of the above data, there&#39;s some mean value (which in this case is zero), and there&#39;s some variance about this mean: technically the variance is the average of the squared differences from the mean. If you&#39;re familiar with the standard deviation, usually denoted by $ sigma$, the variance is just the square of the standard deviation. If $x$ had units of meters, the variance $ sigma^2$ would have units of meters^2. . Think of the variance as the &quot;spread,&quot; or &quot;extent&quot; of the data, about some particular axis (or input, or &quot;feature&quot;). . Similarly we can look at the variance in just the $y$ component of the data. For the above data, the variances in $x$ and $y$ are . print(&quot;Variance in x =&quot;,np.var(x)) print(&quot;Variance in y =&quot;,np.var(y)) . Variance in x = 0.6470431671825421 Variance in y = 0.19318628312072175 . Covariance . You&#39;ll notice in the above graph that as $x$ varies, so does $y$ -- pretty much. So $y$ is &quot;covariant&quot; with $x$. &quot;Covariance indicates the level to which two variables vary together.&quot; To compute it, it&#39;s kind of like the regular variance, except that instead of squaring the deviation from the mean for one variable, we multiply the deviations for the two variables: . $${ rm Cov}(x,y) = {1 over N-1} sum_{j=1}^N (x_j- mu_x)(y_j- mu_j),$$ where $ mu_x$ and $ mu_y$ are the means for the x- and y- componenets of the data, respectively. Note that you can reverse $x$ and $y$ and get the same result, and the covariance of a variable with itself is just the regular variance -- but with one caveat! . The caveat is that we&#39;re dividing by $N-1$ instead of $N$, so unlike the regular variance we&#39;re not quite taking the mean. Why this? Well, for large datasets this makes essentially no difference, but for small numbers of data points, using $N$ can give values that tend to be a bit too small for most people&#39;s tastes, so the $N-1$ was introduced to &quot;reduce small sample bias.&quot; . In Python code, the covariance calculation looks like . def covariance(a,b): return ( (a - a.mean())*(b - b.mean()) ).sum() / (len(a)-1) print(&quot;Covariance of x &amp; y =&quot;,covariance(x,y)) print(&quot;Covariance of y &amp; x =&quot;,covariance(x,y)) print(&quot;Covariance of x with itself =&quot;,covariance(x,x),&quot;, variance of x =&quot;,np.var(x)) print(&quot;Covariance of y with itself =&quot;,covariance(y,y),&quot;, variance of x =&quot;,np.var(y)) . Covariance of x &amp; y = 0.3211758726837525 Covariance of y &amp; x = 0.3211758726837525 Covariance of x with itself = 0.6535789567500425 , variance of x = 0.6470431671825421 Covariance of y with itself = 0.19513765971790076 , variance of x = 0.19318628312072175 . Covariance matrix . So what we do is we take the covariance of every variable with every variable (including itself) and make a matrix out of it. Along the diagonal will be the variance of each variable (except for that $N-1$ in the denominator), and the rest of the matrix will be the covariances. Note that since the order of the variables doesn&#39;t matter when computing covariance, the matrix will be symmetric (i.e. it will equal its own transpose, i.e. will have a reflection symmetry across the diagonal) and thus will be a square matrix. . Numpy gives us a handy thing to call: . data = np.stack((x,y),axis=1) # pack the x &amp; y data together in one 2D array print(&quot;data.shape =&quot;,data.shape) cov = np.cov(data.T) # .T b/c numpy wants varibles along rows rather than down columns? print(&quot;covariance matrix = n&quot;,cov) . data.shape = (100, 2) covariance matrix = [[0.65357896 0.32117587] [0.32117587 0.19513766]] . Some 3D data to work with . Now that we know what a covariance matrix is, let&#39;s generate some 3D data that we can use for what&#39;s coming next. Since there are 3 variables or 3 dimensions, the covariance matrix will now be 3x3. . z = -.5*x + 2*np.random.uniform(size=N) data = np.stack((x,y,z)).T print(&quot;data.shape =&quot;,data.shape) cov = np.cov(data.T) print(&quot;covariance matrix = n&quot;,cov) # Plot our data import plotly.graph_objects as go fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; )]) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . data.shape = (100, 3) covariance matrix = [[ 0.65357896 0.32117587 -0.33982198] [ 0.32117587 0.19513766 -0.15839307] [-0.33982198 -0.15839307 0.5207214 ]] . . . (Note that even though our $z$ data didn&#39;t explicitly depend on $y$, the fact that $y$ is covariant with $x$ means that $y$ and $z$ &#39;coincidentally&#39; have a nonzero covariance. This sort of thing shows up in many datasets where two variables are correlated and may give rise to &#39;confounding&#39; factors.) . So now we have a covariance matrix. The next thing in PCA is find the &#39;principal components&#39;. This means the directions along which the data varies the most. You can kind of estimate these by rotating the 3D graph above. See also this great YouTube video of a teapot (1min 30s) that explains PCA in this manner. . To do Principal Component Analysis, we need to find the aforementioned &quot;components,&quot; and this requires finding eigenvectors for our dataset&#39;s covariance matrix. . What is an eigenvector, really? . First a definition. (Stay with me! We&#39;ll flesh this out in what comes after this.) . Given some matrix (or &#39;linear operator&#39;) ${ bf A}$ with dimensions $n times n$ (i.e., $n$ rows and $n$ columns), there exist a set of $n$ vectors $ vec{v}_i$ (each with dimension $n$, and $i = 1...n$ counts which vector we&#39;re talking about) such that multiplying one of these vectors by ${ bf A}$ results in a vector (anti)parallel to $ vec{v}_i$, with a length that&#39;s multiplied by some constant $ lambda_i$. In equation form: . $${ bf A} vec{v}_i = lambda_i vec{v}_i, (1)$$ . where the constants $ lambda_i$ are called eigenvalues and the vectors $ vec{v}_i$ are called eigenvectors. . (Note that I&#39;m departing a bit from common notation that uses $ vec{x}_i$ instead of $ vec{v}_i$; I don&#39;t want people to get confused when I want to use $x$&#39;s for coordinate variables.) . A graphical version of this is shown in Figure 1: . . Figure 1. &quot;An eigenvector is a vector that a linear operator sends to a multiple of itself&quot; -- Daniel McLaury . Brief Q &amp; A before we go on: . &quot;So what&#39;s with the &#39;eigen&#39; part?&quot; That&#39;s a German prefix, which in this context means &quot;inherent&quot; or &quot;own&quot;. | &quot;Can a non-square matrix have eigenvectors?&quot; Well,...no, and think of it this way: If $ bf{A}$ were an $n times m$ matrix (where $m &lt;&gt; n$), then it would be mapping from $n$ dimensions into $m$ dimensions, but on the &quot;other side&quot; of the equation with the $ lambda_i vec{v}_i$, that would still have $n$ dimensions, so... you&#39;d be saying an $n$-dimensional object equals an $m$-dimensional object, which is a no-go. | &quot;But my dataset has many more rows than columns, so what am I supposed to do about that?&quot; Just wait! It&#39;ll be ok. We&#39;re not actually going to take the eigenvectors of the dataset &#39;directly&#39;, we&#39;re going to take the eigenvectors of the covariance matrix of the dataset. | &quot;Are eigenvectors important?&quot; You bet! They get used in many areas of science. I first encountered them in quantum mechanics.* They describe the &quot;principal vectors&quot; of many objects, or &quot;normal modes&quot; of oscillating systems. They get used in computer vision, and... lots of places. You&#39;ll see them almost anywhere matrices &amp; tensors are employed, such as our topic for today: Data science! | *Ok that&#39;s not quite true: I first encountered them in an extremely boring Linear Algebra class taught by an extremely boring NASA engineer who thought he wanted to try teaching. But it wasn&#39;t until later that I learned anything about their relevance for...anything. Consquently I didn&#39;t &quot;learn them&quot; very well so writing this is a helpful review for me. . How to find the eigenvectors of a matrix . You call a library routine that does it for you, of course! ;-) . from numpy import linalg as LA lambdas, vs = LA.eig(cov) lambdas, vs . (array([1.07311435, 0.26724004, 0.02908363]), array([[-0.73933506, -0.47534042, -0.47690162], [-0.3717427 , -0.30238807, 0.87770657], [ 0.56141877, -0.82620393, -0.04686177]])) . Ok sort of kidding; we&#39;ll do it &quot;from scratch&quot;. But, one caveat before we start: Some matrices can be &quot;weird&quot; or &quot;problematic&quot; and have things like &quot;singular values.&quot; There are sophisticated numerical libraries for doing this, and joking aside, for real-world numerical applications you&#39;re better off calling a library routine that other very smart and very careful people have written for you. But for now, we&#39;ll do the straightforward way which works pretty well for many cases. We&#39;ll follow the basic two steps: . Find the eigenvalues | &#39;Plug in&#39; each eigenvalue to get a system of linear equations for the values of the components of the corresponding eigenvector | Solve this linear system. | 1. Find the eigenvalues . Ok I&#39;m hoping you at least can recall what a determinant of a matrix is. Many people, even if they don&#39;t know what a determinant is good for (e.g. tons of proofs &amp; properties all rest on the determinant), they still at least remember how to calculate one. . The way to get the eigenvalues is to take the determinant of the difference between a $ bf{A}$ and $ lambda$ times the identity matrix $ bf{I}$ (which is just ones along the diagonal and zeros otherwise) and set that difference equal to zero... . $$det( bf{A} - lambda I) = 0 $$ . Just another observation:Since ${ bf I}$ is a square matrix, that means $ bf{A}$ has to be a square matrix too. Then solving for $ lambda$ will give you a polynomial equation in $ lambda$, the solutions to (or roots of) which are the eigenvectors $ lambda_i$. . Let&#39;s do an example: . $$ { bf A} = begin{bmatrix} -2 &amp; 2 &amp; 1 -5 &amp; 5 &amp; 1 -4 &amp; 2 &amp; 3 end{bmatrix} $$To find the eigenvalues we set $$ det( bf{A} - lambda I) = begin{vmatrix} -2- lambda &amp; 2 &amp; 1 -5 &amp; 5- lambda &amp; 1 -4 &amp; 2 &amp; 3- lambda end{vmatrix} = 0.$$ . This gives us the equation... $$0 = lambda^3 - 6 lambda^2 + lambda - 6$$ . which has the 3 solutions (in descending order) $$ lambda = 3, 2, 1.$$ . *(Aside: to create an integer matrix with integer eigenvalues, I used [this handy web tool](https://ericthewry.github.io/integer_matrices/))*. . Just to check that against the numpy library: . A = np.array([[-2,2,1],[-5,5,1],[-4,2,3]]) def sorted_eig(A): # For now we sort &#39;by convention&#39;. For PCA the sorting is key. lambdas, vs = LA.eig(A) # Next line just sorts values &amp; vectors together in order of decreasing eigenvalues lambdas, vs = zip(*sorted(zip(list(lambdas), list(vs.T)),key=lambda x: x[0], reverse=True)) return lambdas, np.array(vs).T # un-doing the list-casting from the previous line lambdas, vs = sorted_eig(A) lambdas # hold off on printing out the eigenvectors until we do the next part! . (3.0000000000000027, 1.9999999999999991, 1.0000000000000013) . Close enough! . 2. Use the eigenvalues to get the eigenvectors . Although it was anncounced in mid 2019 that you can get eigenvectors directly from eigenvalues, the usual way people have done this for a very long time is to go back to the matrix $ bf{A}$ and solve the linear system of equation (1) above, for each of the eigenvalues. For example, for $ lambda_1=-1$, we have . $$ { bf}A vec{v}_1 = - vec{v}_1 $$i.e. . $$ begin{bmatrix} -2 &amp; 2 &amp; 1 -5 &amp; 5 &amp; 1 -4 &amp; 2 &amp; 3 end{bmatrix} begin{bmatrix} v_{1x} v_{1y} v_{1z} end{bmatrix} = - begin{bmatrix} v_{1x} v_{1y} v_{1z} end{bmatrix} $$This amounts to 3 equations for 3 unknowns,...which I&#39;m going to assume you can handle... For the other eigenvalues things proceed similarly. The solutions we get for the 3 eigenvalues are: . $$ lambda_1 = 3: vec{v}_1 = (1,2,1)^T$$ . $$ lambda_2 = 2: vec{v}_2 = (1,1,2)^T$$ . $$ lambda_3 = 1: vec{v}_3 = (1,1,1)^T$$ . Since our original equation (1) allows us to scale eigenvectors by any artibrary constant, often we&#39;ll express eigenvectors as unit vectors $ hat{v}_i$. This will amount to dividing by the length of each vector, i.e. in our example multiplying by $(1/ sqrt{6},1/ sqrt{6},1/ sqrt{3})$. . In this setting . $$ lambda_1 = 3: hat{v}_1 = (1/ sqrt{6},2/ sqrt{6},1/ sqrt{6})^T$$ . $$ lambda_2 = 2: hat{v}_2 = (1/ sqrt{6},1/ sqrt{6},2/ sqrt{6})^T$$ . $$ lambda_3 = 1: hat{v}_3 = (1,1,1)^T/ sqrt{3}$$ . Checking our answers (left) with numpy&#39;s answers (right): . print(&quot; &quot;*15,&quot;Ours&quot;,&quot; &quot;*28,&quot;Numpy&quot;) print(np.array([1,2,1])/np.sqrt(6), vs[:,0]) print(np.array([1,1,2])/np.sqrt(6), vs[:,1]) print(np.array([1,1,1])/np.sqrt(3), vs[:,2]) . Ours Numpy [0.40824829 0.81649658 0.40824829] [-0.40824829 -0.81649658 -0.40824829] [0.40824829 0.40824829 0.81649658] [0.40824829 0.40824829 0.81649658] [0.57735027 0.57735027 0.57735027] [0.57735027 0.57735027 0.57735027] . The fact that the first one differs by a multiplicative factor of -1 is not an issue. Remember: eigenvectors can be multiplied by an arbitrary constant. (Kind of odd that numpy doesn&#39;t choose the positive version though!) . One more check: let&#39;s multiply our eigenvectors times A to see what we get: . print(&quot;A*v_1 / 3 = &quot;,np.matmul(A, np.array([1,2,1]).T)/3 ) # Dividing by eigenvalue print(&quot;A*v_2 / 2 = &quot;,np.matmul(A, np.array([1,1,2]).T)/2 ) # to get vector back print(&quot;A*v_3 / 1 = &quot;,np.matmul(A, np.array([1,1,1]).T) ) . A*v_1 / 3 = [1. 2. 1.] A*v_2 / 2 = [1. 1. 2.] A*v_3 / 1 = [1 1 1] . Great! Let&#39;s move on. Back to our data! . Eigenvectors for our sample 3D dataset . Recall we named our 3x3 covariance matrix &#39;cov&#39;. So now we&#39;ll compute its eigenvectors, and then re-plot our 3D data and also plot the 3 eigenvectors with it... . lambdas, vs = sorted_eig(cov) # Compute e&#39;vals and e&#39;vectors of cov matrix print(&quot;lambdas, vs = n&quot;,lambdas,&quot; n&quot;,vs) # Re-plot our data fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; ) ]) # Draw some extra &#39;lines&#39; showing eigenvector directions n_ev_balls = 50 # the lines will be made of lots of balls in a line ev_size= 3 # size of balls t = np.linspace(0,1,num=n_ev_balls) # parameterizer for drawing along vec directions for i in range(3): # do this for each eigenvector # Uncomment the next line to scale (unit) vector by size of the eigenvalue # vs[:,i] *= lambdas[i] ex, ey, ez = t*vs[0,i], t*vs[1,i], t*vs[2,i] fig.add_trace(go.Scatter3d(x=ex, y=ey, z=ez,mode=&#39;markers&#39;, marker=dict(size=ev_size,opacity=0.8), name=&quot;v_&quot;+str(i+1))) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . lambdas, vs = (1.073114351318777, 0.26724003566904386, 0.0290836286576176) [[-0.73933506 -0.47534042 -0.47690162] [-0.3717427 -0.30238807 0.87770657] [ 0.56141877 -0.82620393 -0.04686177]] . . . Things to note from the above graph: . The first (red) eigenvector points along the direction of biggest variance | The second (greenish) eigenvector points along the direction of second-biggest variance | The third (purple) eigenvector points along the direction of smallest variance. | (If you edit the above code to rescale the vector length by the eigenvector, you&#39;ll really see these three point become apparent!) . &quot;Principal Component&quot; Analysis . Now we have our components (=eigenvectors), and we have them &quot;ranked&quot; by their &quot;significance.&quot; Next we will eliminate one or more of the less-significant directions of variance. In other words, we will project the data onto the various principal components by projecting along the less-significant components. Or even simpler: We will &quot;squish&quot; the data along the smallest-variance directions. . For the above 3D dataset, we&#39;re going to squish it into a 2D pancake -- by projecting along the direction of the 3rd (purple) eigenvector onto the plane defined by the 1st (red) and 2nd (greenish) eigenvectors. . Yea, but how to do this projection? . Projecting the data . It&#39;s actually not that big of a deal. All we have to do is multiply by the eigenvector (matrix)! . OH WAIT! Hey, you want to see a cool trick!? Check this out: . lambdas, vs = sorted_eig(cov) proj_cov = vs.T @ cov @ vs # project the covariance matrix, using eigenvectors proj_cov . array([[ 1.07311435e+00, 4.47193943e-18, -4.45219967e-17], [ 5.25950058e-17, 2.67240036e-01, 6.70099547e-17], [-7.05170575e-17, 4.02835192e-17, 2.90836287e-02]]) . What was THAT? Let me clean that up a bit for you... . proj_cov[np.abs(proj_cov) &lt; 1e-15] = 0 proj_cov . array([[1.07311435, 0. , 0. ], [0. , 0.26724004, 0. ], [0. , 0. , 0.02908363]]) . Important point: What you just saw is the whole reason eigenvectors get used for so many things, because they give you a &#39;coordinate system&#39; where different &#39;directions&#39; decouple from each other. See, the system has its own (German: eigen) inherent set of orientations which are different the &#39;arbitrary&#39; coordinates that we &#39;humans&#39; may have assigned initially. . The numbers in that matrix are the covariances in the directions of the eigenvectors, instead of in the directions of the original x, y, and z. . So really all we have to do is make a coordinate transformation using the matrix of eigenvectors, and then in order to project we&#39;ll literally just drop a whole index&#39;s-worth of data-dimension in this new coordinate system. :-) . So, instead of $x$, $y$ and $z$, let&#39;s have three coordinates which (following physicist-notation) we&#39;ll call $q_1$, $q_2$ and $q_3$, and these will run along the directions given by the three eigenvectors. . Let&#39;s write our data as a N-by-3 matrix, where N is the number of data points we have. . data = np.stack((x,y,z),axis=1) data.shape # we had a 100 data points, so expecting 100x3 matrix . (100, 3) . There are two ways of doing this, and I&#39;ll show you that they&#39;re numerically equivalent: . Use all the eigenvectors to &quot;rotate&quot; the full dataset into the new coordinate system. Then perform a projection by truncating the last column of the rotated data. | Truncate the last eigenvector, which will make a 3x2 projection matrix which will project the data onto the 2D plane defined by those two eigenvectors. | Let&#39;s show them both: . print(&quot; n 1. All data, rotated into new coordinate system&quot;) W = vs[:,0:3] # keep the all the eigenvectors new_data_all = data @ W # project all the data print(&quot;Checking: new_data_all.shape =&quot;,new_data_all.shape) print(&quot;New covariance matrix = n&quot;,np.cov(new_data_all.T) ) print(&quot; n 2. Truncated data projected onto principal axes of coordinate system&quot;) W = vs[:,0:2] # keep only the first and 2nd eigenvectors print(&quot;W.shape = &quot;,W.shape) new_data_proj = data @ W # project print(&quot;Checking: new_data_proj.shape =&quot;,new_data_proj.shape) print(&quot;New covariance matrix in projected space = n&quot;,np.cov(new_data_proj.T) ) # Difference between them diff = new_data_all[:,0:2] - new_data_proj print(&quot; n Absolute maximum difference between the two methods = &quot;,np.max(np.abs(diff))) . 1. All data, rotated into new coordinate system Checking: new_data_all.shape = (100, 3) New covariance matrix = [[1.07311435e+00 7.64444687e-17 3.77081428e-17] [7.64444687e-17 2.67240036e-01 1.21906748e-16] [3.77081428e-17 1.21906748e-16 2.90836287e-02]] 2. Truncated data projected onto principal axes of coordinate system W.shape = (3, 2) Checking: new_data_proj.shape = (100, 2) New covariance matrix in projected space = [[1.07311435e+00 7.64444687e-17] [7.64444687e-17 2.67240036e-01]] Absolute maximum difference between the two methods = 0.0 . ...Nada. The 2nd method will be faster computationally though, because it doesn&#39;t calculate stuff you&#39;re going to throw away. . One more comparison between the two methods. Let&#39;s take a look at the &quot;full&quot; dataset (in blue) vs. the projected dataset (in red): . fig = go.Figure(data=[(go.Scatter3d(x=new_data_all[:,0], y=new_data_all[:,1], z=new_data_all[:,2], mode=&#39;markers&#39;, marker=dict(size=4,opacity=0.5), name=&quot;full data&quot; ))]) fig.add_trace(go.Scatter3d(x=new_data_proj[:,0], y=new_data_proj[:,1], z=new_data_proj[:,0]*0, mode=&#39;markers&#39;, marker=dict(size=4,opacity=0.5), name=&quot;projected&quot; ) ) fig.update_layout(scene_aspectmode=&#39;data&#39;) fig.show() . . . (Darn it, [if only Plot.ly would support orthographic projections](https://community.plot.ly/t/orthographic-projection-for-3d-plots/3897) [[2](https://community.plot.ly/t/orthographic-projection-instead-of-perspective-for-3d-plots/10035)] it&#39;d be a lot easier to visually compare the two datasets!) . Beyond 3D . So typically we use PCA to throw out many more dimensions than just one. Often this is used for data visualization but it&#39;s also done for feature reduction, i.e. to send less data into your machine learning algorithm. (PCA can even be used just as a &quot;multidimensional linear regression&quot; algorithm, but you wouldn&#39;t want to!) . How do you know how many dimensions to throw out? . In other words, how many &#39;components&#39; should you choose to keep when doing PCA? There are a few ways to make this judgement call -- it will involve a trade-off between accuracy and computational speed. You can make a graph of the amount of variance you get as a function of how many components you keep, and often there will be a an &#39;elbow&#39; at some point on the graph indicating a good cut-off point to choose. Stay tuned as we do the next example; we&#39;ll make such a graph. For more on this topic, see this post by Arthur Gonsales. . Example: Handwritten Digits . The scikit-learn library uses this as an example and I like it. It goes as follows: . Take a dataset of tiny 8x8 pixel images of handwritten digits. | Run PCA to break it down from 8x8=64 dimensions to just two or 3 dimensions. | Show on a plot how the different digits cluster in different regions of the space. | (This part we&#39;ll save for the Appendix: Draw boundaries between the regions and use this as a classifier.) | To be clear: In what follows, each pixel of the image counts as a &quot;feature,&quot; i.e. as a dimension. Thus an entire image can be represented as a single point in a multidimensional space, in which distance from the origin along each dimension is given by the pixel intensity. In this example, the input space is not a 2D space that is 8 units wide and 8 units long, rather it consists of 8x8= 64 dimensions. . from sklearn.datasets import load_digits from sklearn.decomposition import PCA digits = load_digits() X = digits.data / 255.0 Y = digits.target print(X.shape, Y.shape,&#39; n&#39;) # Let&#39;s look a a few examples for i in range(8): # show 8 examples print(&quot;This is supposed to be a &#39;&quot;,Y[i],&quot;&#39;:&quot;,sep=&quot;&quot;) plt.imshow(X[i].reshape([8,8])) plt.show() . (1797, 64) (1797,) This is supposed to be a &#39;0&#39;: . This is supposed to be a &#39;1&#39;: . This is supposed to be a &#39;2&#39;: . This is supposed to be a &#39;3&#39;: . This is supposed to be a &#39;4&#39;: . This is supposed to be a &#39;5&#39;: . This is supposed to be a &#39;6&#39;: . This is supposed to be a &#39;7&#39;: . Now let&#39;s do the PCA thang... First we&#39;ll try going down to 2 dimensions. This isn&#39;t going to work super great but we&#39;ll try: . digits_cov = np.cov(X.T) print(&quot;digits_cov.shape = &quot;,digits_cov.shape) lambdas, vs = sorted_eig(np.array(digits_cov)) W = vs[:,0:2] # just keep two dimensions proj_digits = X @ W print(&quot;proj_digits.shape = &quot;, proj_digits.shape) # Make the plot fig = go.Figure(data=[go.Scatter(x=proj_digits[:,0], y=proj_digits[:,1],# z=Y, #z=proj_digits[:,2], mode=&#39;markers&#39;, marker=dict(size=6, opacity=0.7, color=Y), text=[&#39;digit=&#39;+str(j) for j in Y] )]) fig.update_layout( xaxis_title=&quot;q_1&quot;, yaxis_title=&quot;q_2&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.update_layout(scene_camera=dict(up=dict(x=0, y=0, z=1), center=dict(x=0, y=0, z=0), eye=dict(x=0, y=0, z=1.5))) fig.show() . digits_cov.shape = (64, 64) proj_digits.shape = (1797, 2) . . . This is &#39;sort of ok&#39;: There are some regions that are mostly one kind of digit. But you may say there&#39;s there&#39;s too much intermingling between classes for a lot of this plot. So let&#39;s try it again with 3 dimensions for PCA: . W = vs[:,0:3] # just three dimensions proj_digits = X @ W print(&quot;proj_digits.shape = &quot;, proj_digits.shape) # Make the plot, separate them by &quot;z&quot; which is the digit of interest. fig = go.Figure(data=[go.Scatter3d(x=proj_digits[:,0], y=proj_digits[:,1], z=proj_digits[:,2], mode=&#39;markers&#39;, marker=dict(size=4, opacity=0.8, color=Y, showscale=True), text=[&#39;digit=&#39;+str(j) for j in Y] )]) fig.update_layout(title=&quot;8x8 Handwritten Digits&quot;, xaxis_title=&quot;q_1&quot;, yaxis_title=&quot;q_2&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . proj_digits.shape = (1797, 3) . . . Now we can start to see some definition! The 6&#39;s are pretty much in one area, the 2&#39;s are in another area, and the 0&#39;s are in still another, and so on. There is some intermingling to be sure (particularly between the 5&#39;s and 8&#39;s), but you can see that this &#39;kind of&#39; gets the job done, and instead of dealing with 64 dimensions, we&#39;re down to 3! . Graphing Variance vs. Components . Earlier we asked the question of how many components one should keep. To answer this quantitatively, we note that the eigenvalues of the covariance matrix are themselves measures of the variance in the datset. So these eigenvalues encode the &#39;significance&#39; that each feature-dimension has in the overall dataset. We can plot these eigenvalues in order and then look for a &#39;kink&#39; or &#39;elbow&#39; in the graph as a place to truncate our representation... . plt.plot( np.abs(lambdas)/np.sum(lambdas) ) plt.xlabel(&#39;Number of components&#39;) plt.ylabel(&#39;Significance&#39;) plt.show() . ...So, if we were wanting to represent this data in more than 3 dimensions but less than the full 64, we might choose around 10 principal compnents, as this looks like roughly where the &#39;elbow&#39; in the graph lies. . Interpretability . What is the meaning of the new coordinate axes or &#39;features&#39; $q_1$, $q_2$, etc? Sometimes there exists a compelling physical intepretation of these features (e.g., as in the case of eigenmodes of coupled oscillators), but often...there may not be any. And yet we haven&#39;t even done any &#39;real machine learning&#39; at this point! ;-) . This is an important topic. Modern data regulations such as the European Union&#39;s GDPR require that models used in algorithmic decision-making be &#39;explainable&#39;. If the data being fed into such algorithmics is already abstracted via methods such as PCA, this could be an issue. Thankfully, the linearity of the components mean that one can describe each principal component as a linear combination of inputs. . Further reading . There are many books devoted entirely to the intricacies of PCA and its applications. Hopefully this post has helped you get a better feel for how to construct a PCA transformation and what it might be good for. To expand on this see the following... . Examples &amp; links . &quot;Eigenstyle: Principal Component Analysis and Fashion&quot; by Grace Avery. Uses PCA on Fashion-MNIST. It&#39;s good! | Neat paper by my friend Dr. Ryan Bebej from when he was a student and used PCA to classify locomotion types of prehistoric acquatic mammals based on skeletal measurements alone. | Andrew Ng&#39;s Machine Learning Course, Lecture on PCA. How I first learned about this stuff. | PCA using Python by Michael Galarnyk. Does similar things to what I&#39;ve done here, although maybe better! | Plot.ly PCA notebook examples | . Appendix A: Overkill: Bigger Handwritten Digits . Sure, 8x8 digit images are boring. What about 28x28 images, as in the MNIST dataset? Let&#39;s roll... . from sklearn.datasets import fetch_openml from random import sample #mnist = fetch_mldata(&#39;MNIST original&#39;) mnist = fetch_openml(&#39;mnist_784&#39;, version=1, cache=True) X2 = mnist.data / 255 Y2 = np.array(mnist.target,dtype=np.int) #Let&#39;s grab some indices for random suffling indices = list(range(X2.shape[0])) # Let&#39;s look a a few examples for i in range(8): # 8 is good i = sample(indices,1) print(&quot;This is supposed to be a &quot;,Y2[i][0],&quot;:&quot;,sep=&quot;&quot;) plt.imshow(X2[i].reshape([28,28])) plt.show() . This is supposed to be a 1: . This is supposed to be a 9: . This is supposed to be a 8: . This is supposed to be a 7: . This is supposed to be a 3: . This is supposed to be a 5: . This is supposed to be a 2: . This is supposed to be a 6: . mnist_cov = np.cov(X2.T) lambdas, vs = sorted_eig(np.array(mnist_cov)) W = vs[:,0:3] # Grab the 3 most significant dimensions # Plotting all 70,000 data points is going to be too dense too look at. # Instead let&#39;s grab a random sample of 5,000 points n_plot = 5000 indices = sample(list(range(X2.shape[0])), n_plot) proj_mnist = np.array(X2[indices] @ W, dtype=np.float32) # Last step of PCA: project fig = go.Figure(data=[go.Scatter3d(x=proj_mnist[:,0], y=proj_mnist[:,1], z=proj_mnist[:,2], mode=&#39;markers&#39;, marker=dict(size=4, opacity=0.7, color=Y2[indices], showscale=True), text=[&#39;digit=&#39;+str(j) for j in Y2[indices]] )]) fig.show() . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: ComplexWarning: Casting complex values to real discards the imaginary part . . . ...bit of a mess. Not as cleanly separated as the 8x8 image examples. You can see that the 0&#39;s are well separated from the 1&#39;s and the 3&#39;s, but everything else is pretty mixed together. (I suspect the 1&#39;s are clustered strongly because they involve the most dark pixels.) . If you wanted to push this further then either keeping more dimensions (thereby making it un-visualizable) or just using a different method entirely (e.g. t-SNE or even better: UMAP) would be the way to go. Still, it&#39;s neat to see that you can get somewhat intelligible results in 3D even on this &#39;much harder&#39; problem. . Appendix B: Because We Can: Turning it into a Classifier . ...But let&#39;s not do a neural network because all I ever do are neural networks, and because I don&#39;t want to have take the time &amp; space to explain how they work or load in external libraries. Let&#39;s do k-nearest-neighbors instead, because it&#39;s intuitively easy to grasp and it&#39;s not hard to code up: . For any new point we want to evaluate, we take a &#39;vote&#39; of whatever some number (called $k$) of its nearest neighbor points are already assigned as, and we set the class of the new point according to that vote. . Making a efficient classifier is all about finding the *boundaries* between regions (and usually subject to some user-adjustable parameter like $k$ or some numerical threshold). Finding these boundaries can be about finding the &#39;edge cases&#39; that cause the system to &#39;flip&#39; (discontinuously) from one result to another. However, we are *not* going to make an efficient classifier today. ;-) . Let&#39;s go back to the 8x8 digits example, and split it into a training set and a testing set (so we can check ourselves): . indices = sample(list(range(X.shape[0])), X.shape[0]) X_shuf, Y_shuf = X[indices,:], Y[indices] # 80-20 train-test split max_train_ind = int(0.8*X.shape[0]) X_train, Y_train = X_shuf[0:max_train_ind,:], Y_shuf[0:max_train_ind] X_test, Y_test = X_shuf[max_train_ind:-1,:], Y_shuf[max_train_ind:-1] # Do PCA on training set train_cov = np.cov(X_train.T) ell, v = sorted_eig(np.array(train_cov)) pca_dims = 3 # number of top &#39;dimensions&#39; to take W_train = v[:,0:pca_dims] proj_train = X_train @ W_train # also project the testing set while we&#39;re at it proj_test = X_test @ W_train # yes, same W_train . Now let&#39;s make a little k-nearest neighbors routine... . from collections import Counter def knn_predict(xnew, proj_train, Y_train, k=3): &quot;&quot;&quot; xnew is a new data point that has the same shape as one row of proj_train. Given xnew, calculate the (squared) distance to all the points in X_train to find out which ones are nearest. &quot;&quot;&quot; distances = ((proj_train - xnew)**2).sum(axis=1) # stick on an extra column of indexing &#39;hash&#39; for later use after we sort dists_i = np.stack( (distances, np.array(range(Y_train.shape[0]) )),axis=1 ) dists_i = dists_i[dists_i[:,0].argsort()] # sort in ascending order of distance knn_inds = (dists_i[0:k,-1]).astype(np.int) # Grab the indexes for k nearest neighbors # take &#39;votes&#39;: knn_targets = list(Y_train[knn_inds]) # which classes the nn&#39;s belong to votes = Counter(knn_targets) # count up how many of each class are represented return votes.most_common(1)[0][0] # pick the winner, or the first member of a tie # Let&#39;s test it on the first element of the testing set x, y_true = proj_test[0], Y_test[0] guess = knn_predict(x, proj_train, Y_train) print(&quot;guess = &quot;,guess,&quot;, true = &quot;,y_true) . guess = 6 , true = 6 . Now let&#39;s try it for the &#39;unseen&#39; data in the testing set, and see how we do... . mistakes, n_test = 0, Y_test.shape[0] for i in range(n_test): x = proj_test[i] y_pred = knn_predict(x, proj_train, Y_train, k=3) y_true = Y_test[i] if y_true != y_pred: mistakes += 1 if i &lt; 20: # show some examples print(&quot;x, y_pred, y_true =&quot;,x, y_pred, y_true, &quot;YAY!&quot; if y_pred==y_true else &quot; BOO. :-(&quot;) print(&quot;...skipping a lot...&quot;) print(&quot;Total Accuracy =&quot;, (n_test-mistakes)/n_test*100,&quot;%&quot;) . x, y_pred, y_true = [ 0.06075339 0.00153272 -0.0477644 ] 6 6 YAY! x, y_pred, y_true = [ 0.04083212 0.09757529 -0.05361896] 1 1 YAY! x, y_pred, y_true = [-0.0199586 -0.00778773 0.00972962] 8 5 BOO. :-( x, y_pred, y_true = [ 0.02400112 -0.07267613 0.02774141] 0 0 YAY! x, y_pred, y_true = [0.01180771 0.03483923 0.07526469] 1 7 BOO. :-( x, y_pred, y_true = [ 0.00379226 -0.06269449 -0.00195609] 0 0 YAY! x, y_pred, y_true = [-0.06832135 -0.05396545 0.02980845] 9 9 YAY! x, y_pred, y_true = [-0.02397417 -0.04914796 0.0109273 ] 5 5 YAY! x, y_pred, y_true = [ 0.08213707 -0.01608953 -0.08072889] 6 6 YAY! x, y_pred, y_true = [ 0.03056858 -0.04852946 0.02204423] 0 0 YAY! x, y_pred, y_true = [-0.02124777 0.03623541 -0.01773196] 8 8 YAY! x, y_pred, y_true = [0.03035896 0.01398381 0.01415554] 8 8 YAY! x, y_pred, y_true = [ 0.0214849 0.02114674 -0.08951798] 1 1 YAY! x, y_pred, y_true = [ 0.07878152 0.03312015 -0.06488347] 6 6 YAY! x, y_pred, y_true = [-0.01294308 0.00158962 -0.01255491] 8 5 BOO. :-( x, y_pred, y_true = [ 0.01351581 0.11000321 -0.03895516] 1 1 YAY! x, y_pred, y_true = [0.0081306 0.01683952 0.05911389] 7 1 BOO. :-( x, y_pred, y_true = [0.06497268 0.02817075 0.07118004] 4 4 YAY! x, y_pred, y_true = [-0.03879657 -0.04460611 0.02833793] 9 5 BOO. :-( x, y_pred, y_true = [-0.05975051 0.03713843 -0.07174727] 2 2 YAY! ...skipping a lot... Total Accuracy = 76.88022284122563 % . ...eh! Not bad, not amazing. You can improve the accuracy if you go back up and increase the number of PCA dimensions beyond 3, and/or increase the value of $k$. Go ahead and try it! . (For 10 dimensions and $k=7$, I got 97.7% accuracy. The highest I ever got it was 99%, but that was really working overtime, computationally speaking; the point of PCA is to let you dramatically reduce your workload while retaining reasonably high accuracy.) . Just to reiterate: This is NOT supposed to be a state of the art classifier! It&#39;s just a toy that does pretty well and helps illustrate PCA without being hard to understand or code. . The End . Thanks for sticking around! Hope this was interesting. PCA is pretty simple, and yet really useful! ...and writing this really helped me to better understand it. ;-) .",
            "url": "https://drscotthawley.github.io/blog/2019/12/21/PCA-From-Scratch.html",
            "relUrl": "/2019/12/21/PCA-From-Scratch.html",
            "date": " • Dec 21, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Getting Started in ML+Audio",
            "content": "Recently, after co-chairing the session on Machine Learning for Musical Acoustics along with the wonderful Bozena Kostek at the Acoustical Society of America Meeting in San Diego, a student came up to me to ask for advice or resources on how to start learning machine learning for audio applications. . And I didn’t have a good answer. Because, while there’s plenty of tips out there for learning image classification or NLP, not a ton of it relates to audio. Students have asked me this before…and so I was frustrated by my own lack of an answer, apart from “read whatever you find, try out demos, take whatever courses you can, etc.” . SO! Now, with graduate student Ryan Miller (EDIT 6/1/2020: who is now gainfully employed at Creative Labs!), we compiled a list and posted it to GitHub: . ml-audio-start Page . … and soliciting contributions from the community. . EDIT 02/01/2020: We now have contributions from accomplished researchers at Google, Adobe, and others! .",
            "url": "https://drscotthawley.github.io/blog/2019/12/06/Getting-Started-In-ML-Audio.html",
            "relUrl": "/2019/12/06/Getting-Started-In-ML-Audio.html",
            "date": " • Dec 6, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
            "content": "Links to lessons: Part 0, Part 1, Part 2, Part 3 . What is Backpropagation? . First watch this 5-minute video on backprop by Siraj Raval. . EDIT 2/20/2020: ^Ravel was later revealed to be plagiarizing content. I will look for an alternative link. The format in my (&quot;flipped&quot;) ML course last year involved reading things, watching brief videos, and modifying code. Siraj&#39;s video fit the bill last year as being brief &amp; good. . A Multi-Layer Network . Between the input $X$ and output $ tilde{Y}$ of the network we encountered earlier, we now interpose a &quot;hidden layer,&quot; connected by two sets of weights $w^{(0)}$ and $w^{(1)}$ as shown in the figure below. This image is a bit more complicated than diagrams one might typically encounter; I wanted to be able to show and label all the different &quot;parts.&quot; We will explain what the various symbols mean as we continue after the figure. . As before, the combined operation of weighted sum and nonlinear activation is referred to as a(n artificial) neuron; in the diagram above there are 4 &quot;hidden neurons&quot; or equivalently &quot;4 neurons in the hidden layer&quot; as well as one neuron for output. (The activations $f^{(0)}$ and $f^{(1)}$ may be the same, or they may be different.) . Semantics: What is a Layer? . The term &quot;layer&quot; in regards to neural network is not always used consistently. You may find it used in different senses by different authors. . Some users of the term will only use it with repect to weight matrices, (since these are the parts of the network which are adjusted in learning). | Others will refer to the input and (predicted) output as layers, and may or may not include the weights as layers.. | Others will only count additional &quot;hidden layers&quot; between the inputs and outputs, and these &quot;layers&quot; are connected by multiple weight matrices. | Some will speak of &quot;activation layers.&quot; In software libraries like Keras, many different types of operations and storage are referred to as layers. | For the work we&#39;ve done so far, we&#39;ve had inputs and outputs connected by one weight matrix, subject to a nonlinear activation function. Is this a two-layer network made of input and output &quot;layers,&quot;&quot; or is it a single-layer network, because there is only one weight matrix? What about the activation layer? This is to some degree a semantic issue which one does not need to get hung up on. . For our purposes it is convenient to refer to the inputs $X$, the &#39;activated&#39; hidden states $H$, and the output $ tilde{Y}$ as &quot;layers&quot;, numbering them 0, 1, and 2 respectively, and using the script notation $ mathcal{L}^l$ to denote each layer, where the layer index $l=0..2$, so that . $$ mathcal{L}^{(0)} = X, mathcal{L}^{(1)} = H, mathcal{L}^{(2)} = tilde{Y} $$This makes it easy to write the value of higher-numbered layers in terms of lower-numbed layers, i.e., $$ mathcal{L}^{(l+1)} = f^{(l)} left( { mathcal{L}^{(l)}}^T cdot w^{(l)} right), $$ where the dot $ cdot$ denotes a matrix product. This is often referred to as a &quot;feed foward&quot; operation because values are fed from left to right in the above diagram, &quot;forward&quot; through the network. (Backpropagation will involve feeding values from right to left.) . Response to student question(s): &quot;What are neurons? Like, what does this mean in terms of matrices?&quot; . This will serve as a review of the Part 1 lesson. Using the above notation, the operations from the input to the hidden layer look like this in matrix form: . ...where the lines in dark red and cyan are simply to indicate sample calculations which are part of the matrix multiplication. . Figuring out dimensions of the weights . When we learned about matrix multipliation, we remarked that most of the time in machine learning, &quot;the trick is to get the inner dimensions to match.&quot; . Let&#39;s say there are $N$ different input data &quot;points&quot; consisting of $M$ values each. So the input $X$ is an $N times M$ matrix. And let the output $ tilde{Y}$ be a $NxP$ matrix (in our example, $P=1$). If we were just connecting $X$ and $ tilde{Y}$ with no hidden layer, the single weights matrix would be a $M times P$ matrix: . $$ ( color{blue}N times color{red}M) cdot( color{red}M times color{green}P) = ( color{blue}N times color{green}P) $$(The nonlinear activation doesn&#39;t change the dimensions of the matrices.) . Adding a hidden layer with $Q$ number of neurons means we will still have $N$ different activations for each neuron (i.e. for each datapoint), so that $H$ is a $N times Q$ matrix. Thus the dimensions of $w^0$ must &quot;match&quot; between these two matrices, and so $w^0$ must be a $M times Q$ matrix: . $$ ( color{blue}N times color{red}M) cdot( color{red}M times color{purple}Q) = ( color{blue}N times color{purple}Q) $$ Similarly $w^1$ must be a $Q times P$ matrix, and the full operation in terms of matrix dimensions is . $$ ( color{blue}N times color{red}M) cdot( color{red}M times color{purple}Q) cdot( color{purple}Q times color{green}P) = ( color{blue}N times color{green}P). $$Compare this to the diagram above for $P=1$, $Q=4$. . Note: If you add bias terms to your model, you may need to remember that the number of columns in both the input $X$ and hidden layer $H$ are greater by one, i.e. $ color{red}{M} rightarrow color{red}{M+1}$, etc. . ..a bit of code . The layers $ mathcal{L}^l$ can be represented in Python a list called layers which has a of length 3. Similarly, our weights can be items in a list called weights. Returning to our first sample problem from Part 1: . import numpy as np %matplotlib inline import matplotlib.pyplot as plt # Training data: input and target X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) Y = np.array([[0,0,1,1]]).T # define auxiliary variables N, M, P = X.shape[0], X.shape[1], Y.shape[1] # infer matrix shape variables from training data Y_tilde = np.zeros((N,P)) # setup storage for network output # Hidden layers Q = 4 # number of hidden neurons, i.e. &quot;size of hidden layer&quot; H = np.zeros((N,Q)) # weight matrices w0 = 2*np.random.random((M,Q)) - 1 w1 = 2*np.random.random((Q,P)) - 1 # Make lists for layers and weights layers = [X, H, Y_tilde] weights = [w0, w1] # Just try a sample calculation with random intialization to see how this works # Feed-forward (with linear activation): for l in range(2): layers[l+1] = np.dot(layers[l], weights[l]) print(&quot;layers [&quot;,l+1,&quot;] = n&quot;, layers[l+1], sep=&quot;&quot;) # sep=&quot;&quot; just omits spaces . layers [1] = [[-0.71619863 -0.93579012 0.4707394 -0.27924993] [-0.7101555 -0.1050841 0.91379718 0.29545498] [-1.16689961 -0.7912951 0.85311015 -0.32876382] [-1.16085648 0.03941092 1.29616794 0.24594109]] layers [2] = [[ 0.37453611] [-0.30449833] [-0.07951087] [-0.75854532]] . Generalizing this so it will do the full feed-forward will take a bit more code. We&#39;ll leave a placeholder routine for backpropagation for now. . def sigmoid(x, deriv=False): f = 1/(1+np.exp(-x)) return f*(1-f) if deriv else f def relu(x, deriv=False): return 1*(x&gt;0) if deriv else x*(x&gt;0) # Placeholder routine to perform backprop. Will fill in later def update_weights(weights, layers, Y, alpha, activ): return weights # for now, it&#39;s a no-op def fit(layers, Y, activ=[sigmoid]*2, use_bias=True, alpha=1.0, maxiter=10000): &quot;&quot;&quot; Routine for training using a multi-layer network layers: list of layer values, i.e. layers = [X, H, Y_tilde] Y: target output activ: list of activation functions. default = list of 2 sigmoids use_bias: Whether to include a constant offset in weighted sums alpha: learning rate maxiter: number of iterations to run &quot;&quot;&quot; lmax = len(layers)-1 # max index of layers, also = # of weights if use_bias: # add a column of 1&#39;s to every layer except the last for el in range(lmax): new_col = np.ones((layers[el].shape[0],1)) layers[el] = np.hstack((new_col, layers[el])) # Define weights np.random.seed(1) # for reproducibility weights = [None]*lmax # allocate slots in a blank list for el in range(lmax): # &quot;el&quot; because &quot;l&quot; and &quot;1&quot; may look similar weights[el] = 2*np.random.random((layers[el].shape[1], layers[el+1].shape[1])) - 1 loss_hist = [] # start with an empty list for iter in range(maxiter): # Feed-forward pass for el in range(lmax): layers[el+1] = activ[el](np.dot(layers[el], weights[el])) # Loss monitoring diff = layers[lmax] - Y loss_hist.append( (diff**2).mean() ) # use MSE loss for monitoring # Backprop code will go here weights = update_weights(weights, layers, Y, alpha, activ) return weights, layers[lmax], loss_hist # Test this just to make sure it runs layers = [X,H,Y_tilde] weights, Y_tilde, loss_hist = fit(layers, Y, maxiter=1) for el in range(len(weights)): print(&quot;weights[&quot;,el,&quot;] = n&quot;,weights[el], sep=&quot;&quot;) . weights[0] = [[-0.16595599 0.44064899 -0.99977125 -0.39533485 -0.70648822] [-0.81532281 -0.62747958 -0.30887855 -0.20646505 0.07763347] [-0.16161097 0.370439 -0.5910955 0.75623487 -0.94522481] [ 0.34093502 -0.1653904 0.11737966 -0.71922612 -0.60379702]] weights[1] = [[ 0.60148914] [ 0.93652315] [-0.37315164] [ 0.38464523] [ 0.7527783 ]] . ...Now that we&#39;ve achieved feed-foward operation of the network, in order to make it &#39;learn&#39; or &#39;train&#39;, we need to compare the output value $ tilde{Y}$ (which is the same as layers[2] by the way) to the target value, compute the gradients of the loss function, and then backpropagate in order to update all the weights! . Backpropagating: Theory . TL/DR: You can skip down to the last boxed equation of this section if math scares you. You will not be required to reproduce this. I do want to show you &quot;where this stuff comes from&quot;, but if you find the derivation too intimidating, you can still progress in the course just fine. . Let&#39;s review how we got the gradients for $w^{(1)}$ in Part 2, denoting weighted sums by &quot;$S$&quot;, e.g. $S^l = mathcal{L}^l cdot w^l$, we just used the Chain Rule: $$ { partial L over partial w^{(1)}} = color{blue} { partial L over partial mathcal{L}^{(2)}} color{green} { partial mathcal{L}^{(2)} over partial S^{(1)}} color{red} { partial S^{(1)} over partial w^{(1)}} $$ We&#39;ll define the first partial derivative to be $ delta^{(2)}$, which works out (given our choice of $L$ from Part 2) to be $$ color{blue}{ { partial L over partial mathcal{L}^{(2)}} = delta^{(2)} = tilde{Y}-Y},$$ i.e., it is the error in the final ouput. The next partial derivative (in green) is just the derivative of the activation function $f$, and the last partial derivative is just $ color{red}{ mathcal{L}^{(1)}}$, so as we saw in the previous lesson, we can write this &#39;schematically&#39; (i.e. not quite as a properly-set-up matix equation yet) as $$ { partial L over partial w^{(1)}} = color{blue}{ delta^{(2)}} color{green} {f^{(1) prime} } color{red}{ mathcal{L}^{(1)}} $$ whereas in proper form it will take on this ordering as a matrix equation: $$ boxed{ { partial L over partial w^{(1)}} = { mathcal{L}^{(1)}}^T cdot { delta^{(2)}} {f^{(1) prime}} }. $$ . To get the gradients for $w^{(0)}$, we can make use of a similar &quot;$ delta$&quot; notation if we&#39;re careful in how we define a new $ delta^{(1)}$. Let&#39;s write out the chain rule, and put parentheses around a particular group of terms for later: . $$ { partial L over partial w^{(0)}} = color{blue}{ left( { partial L over partial mathcal{L}^{(2)}} { partial mathcal{L}^{(2)} over partial S^{(1)}} { partial S^{(1)} over partial mathcal{L}^{(1)}} right)} color{green} { partial mathcal{L}^{(1)} over partial S^{(0)}} color{red} { partial S^{(0)} over partial w^{(0)}} $$In a manner similar to what we did above, this can be written as $$ { partial L over partial w^{(0)}} = color{blue}{ left( delta^{(2)}f^{(1) prime}w^{(1)} right)} color{green}{f^{(0) prime}} color{red}{ mathcal{L^{(0)}}} $$ We now define the terms in parentheses as $ delta^{(1)}$ . $$ color{blue}{ delta^{(1)} = delta^{(2)}f^{(1) prime}w^{(1)} }, $$...which is kind of like &quot;the error in the hidden layer,&quot; or like the final solution error projected backward into the hidden layers via our (momentarily fixed) weights $w^{(1)}$. . Then our gradients for $w^{(0)}$ take on a similar form as the gradients for $w^{(1)}$. &#39;Schematically&#39; this looks like $$ { partial L over partial w^{(0)}} = color{blue}{ delta^{(1)}} color{green}{f^{(0) prime}} color{red}{ mathcal{L^{(0)}}} $$ and in proper matrix form this is $$ boxed{ { partial L over partial w^{(0)}} = { mathcal{L}^{(0)}}^T cdot { delta^{(1)}} {f^{(0) prime}} }, $$ i.e., the same form as the preceding layer, just &quot;back&quot; one layer. We are backpropagating the errors $ delta^{(l)}$ from one layer to another in order to update the weights. . The weights are then updated as before, except now we will write this &#39;generically&#39; for all weights and layers using the index $l$: $$ boxed{ w^{(l)} := w^{(l)} - alpha { mathcal{L}^{(l)}}^T cdot { delta^{(l+1)}} {f^{(l) prime}} }, $$ where $$ delta^{(l+1)} = left { begin{array}{l} tilde{Y}-Y, &amp; &amp;l+1=l_{max} ({ rm e.g.} l_{max}=2) delta^{(l+2)}f^{(l+1) prime} cdot {w^{(l+1)}}^T, &amp; &amp;l+1 &lt; l_{max} end{array} right. $$ . Writing the Backprop Code . Now we&#39;ll use the above analysis to replace the update_weights() function from earlier. . def update_weights(weights, layers, Y, alpha, activ): &quot;&quot;&quot; Backprop routine, for arbitrary numbers of layers, assuming weights &amp; activations are defined Inputs: weights: list of arrays of weights between each layer layers: list of arrays of layer values (post-activation function) Y: target output alpha: learning rate activ: list of activation functions for each (non-input) layer Outputs: weights (updated) &quot;&quot;&quot; lmax = len(layers) - 1 # a useful variable assert len(weights)==lmax # make sure number of weights match up assert len(activ) &gt;= lmax # make sure we defined enough activations for the layers delta = layers[lmax] - Y # error between output and target for el in range(lmax-1, -1, -1): # Count backwards to layer zero fprime = activ[el](np.dot(layers[el], weights[el]), deriv=True) # deriv of activation weights[el] -= alpha * np.dot( layers[el].T, delta*fprime ) # gradient descent step delta = np.dot(delta*fprime, weights[el].T) if (el&gt;0) else None # setup delta for next pass in loop return weights # Let&#39;s run it! layers = [X, H, Y_tilde] alpha = 0.1 maxiter=5000 weights, Y_tilde, loss_hist_2weights = fit(layers, Y, alpha=alpha, maxiter=maxiter) # compare against a 1-weight (no hidden layer) network: layers = [X, Y_tilde] weights, Y_tilde, loss_hist_1weight = fit(layers, Y, alpha=alpha, maxiter=maxiter) # Plot the loss history plt.semilogy(loss_hist_1weight, label=&quot;No hidden layers&quot;) plt.semilogy(loss_hist_2weights, label=&quot;Hidden layer&quot;) plt.legend() plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.show() . Q = 50 # number of hidden neurons, i.e. &quot;size of hidden layer&quot; H = np.zeros((N,Q)) layers = [X, H, Y_tilde] weights, Y_tilde, loss_hist_many = fit(layers, Y, alpha=alpha, maxiter=maxiter) # try a relu activation for the hidden layer (leave output activ as sigmoid!) weights, Y_tilde, lhm_relu = fit(layers, Y, alpha=alpha, activ=[relu,sigmoid], maxiter=maxiter) plt.semilogy(loss_hist_1weight, label=&quot;0 hidden neurons&quot;) plt.semilogy(loss_hist_2weights, label=&quot;4 hidden neurons&quot;) plt.semilogy(loss_hist_many, label=&quot;many hidden neurons&quot;) plt.semilogy(lhm_relu, label=&quot;many, relu on hidden&quot;) plt.legend() plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) . Text(0, 0.5, &#39;Loss&#39;) . Solving XOR . Now let&#39;s revisit the &quot;XOR&quot; problem that a single neuron couldn&#39;t handle. . $$ overbrace{ left[ { begin{array}{cc} 0 &amp; 0 0 &amp; 1 1 &amp; 0 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 1 1 0 end{array} } right] }^Y. $$With our multi-layer network, we can solve this. Note that while an exact solution to the XOR problem exists using only 2 hidden neurons and linear activations, a program can still have a hard time finding a good approximation via gradient descent, and we use 20 hidden neurons to assist, as follows: . X = np.array([[0,0],[0,1],[1,0],[1,1]]) Y = np.array([[0,1,1,0]]).T Y_tilde = 0*Y # Just allocate some storage H = np.zeros((N,20)) weights, Y_tilde, loss_hist_xor = fit([X,H,Y_tilde], Y, activ=[relu,sigmoid], alpha=0.5) print(&quot;Prediction Y_tilde =&quot;,Y_tilde.T) print(&quot;Target Y (correct answer) =&quot;,Y.T) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.loglog(loss_hist_xor) #print(&quot;weights = &quot;,weights) . Prediction Y_tilde = [[0.00480128 0.9962425 0.9963109 0.00374649]] Target Y (correct answer) = [[0 1 1 0]] . [&lt;matplotlib.lines.Line2D at 0x7f9283c17550&gt;] . Same thing using neural network libraries Keras &amp; PyTorch. . Since most of the time we won&#39;t be writing neural network systems &quot;from scratch, by hand&quot; in numpy, let&#39;s take a look at similar operations using libraries such as Keras or PyTorch. . Keras version . Keras is so simple to set up, it&#39;s easy to get started. This is what the previous example for XOR looks like &quot;in Keras&quot;: . import keras from keras.models import Sequential from keras.layers import Dense, Activation from keras.optimizers import Adam # training data X = np.array([[0,0],[0,1],[1,0],[1,1]]) Y = np.array([[0,1,1,0]]).T # specify model n_hidden = 20 model = Sequential([ Dense(n_hidden, input_shape=(X.shape[1],), activation=&#39;relu&#39;), Dense(1, activation=&#39;sigmoid&#39;)]) # choices for loss and optimization method opt = Adam(lr=alpha) # We&#39;ll talk about optimizer choices later model.compile(optimizer=opt, loss=&#39;binary_crossentropy&#39;,metrics=[&#39;binary_accuracy&#39;]) # training iterations model.fit(X, Y, epochs=maxiter, batch_size=1, verbose=0) print(&quot; nY_tilde = n&quot;, model.predict(X) ) . Y_tilde = [[0.00] [1.00] [1.00] [0.00]] . Keras can get a better appoximation than we did because of the choice of optimizer algorithm. We&#39;ll talk about optimization algorithms (refinements to gradient descent) another time. . PyTorch version . Unlike Keras, PyTorch does not have any &quot;training wheels.&quot; You have to specify a number of the operations yourself. It&#39;s helpful to have a template to start from, such as the following example for our XOR problem. . import torch # it&#39;s &#39;PyTorch&#39; but the package is &#39;torch&#39; device = torch.device(&#39;cpu&#39;) # handy for changing to &#39;cuda&#39; in GPU runtimes later! torch.manual_seed(1) # for reproducibility # training data X = np.array([[0,0],[0,1],[1,0],[1,1]],dtype=np.float32) Y = np.array([[0,1,1,0]],dtype=np.float32).T # re-cast data as PyTorch variables, on the device (CPU or GPU) were calc&#39;s are performed x, y = torch.tensor(X).to(device), torch.tensor(Y).to(device) # specify model (similar to Keras but not quite) n_hidden = 20 # number of hidden neurons model = torch.nn.Sequential( torch.nn.Linear(X.shape[1], n_hidden), torch.nn.ReLU(), torch.nn.Linear(n_hidden, 1), torch.nn.Sigmoid() ).to(device) # choices for loss and optimization method loss_fn = torch.nn.BCELoss() # binary cross-entropy loss optimizer = torch.optim.Adam([{&#39;params&#39;: model.parameters()}], lr=alpha) # training iterations loss_hist_pytorch = [] for iter in range(maxiter): optimizer.zero_grad() # set gradients=0 before calculating more y_tilde = model(x) # feed-forward step loss = loss_fn(y_tilde, y) # compute the loss loss_hist_pytorch.append(loss.item()) # save loss for plotting later loss.backward() # compute gradients via backprop optimizer.step() # actually update the weights # print and plot our results print(&quot; nY_tilde = n&quot;, y_tilde.cpu().data.numpy() ) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.loglog(loss_hist_pytorch) . Y_tilde = [[2.9910339e-07] [9.9999976e-01] [9.9999964e-01] [3.4460226e-07]] . [&lt;matplotlib.lines.Line2D at 0x7f38a9fcfe80&gt;] . Exercise: Exploring Hidden Layers. . More with the 7-segment display . Using the $X$ and $Y$ arrays for from previous exercices with the 7-segment display, we&#39;ll explore the effects of adding hidden neurons and different activation functions. Using the code template that follows below,... . A. Explore hidden layer sizes &amp; activations . Write code for a new activation function: $ tanh(x)$ and its derivative. Note: there is already placeholder code for this in the template below | Set training data to be that of the 7-segment display. | Choose (for yourself) a single learning rate (e.g. $ alpha=$0.5), and a standard number of iterations (e.g. 10000). | Then compare results for multiple networks (all with softmaxsigmoid activation on the end): . A single hidden layer with 20 neurons and (for the hidden layer)... . sigmoid activation | relu activation | tanh activation | . | A single hidden layer with 100 neurons and (for the hidden layer)... . sigmoid activation | relu activation | tanh activation | . | B. Explore multiple hidden layers . Now use two hidden layers, H and H2 with 10 neurons each, and experiment to find the best combination of activations, and best choice of learning rate that gives you the lowest loss at the end of your chosen number of iterations. Check that your predicted output is as you expect. | Assignment: . Upload a text file of the code for your &quot;winning&quot; entry for #3 to Blackboard. Use the code below as a template. . import numpy as np import matplotlib.pyplot as plt ### LEAVE THIS UNCHANGED # First, let&#39;s repeat the sigmoid(), relu(), update_weights() and fit() routines # already defined, so we have a&#39;standalone&#39; code and can easily make changes # Activation choices def sigmoid(x, deriv=False): f = 1/(1+np.exp(-x)) return f*(1-f) if deriv else f def relu(x, deriv=False): return 1*(x&gt;0) if deriv else x*(x&gt;0) # Backpropagation routine def update_weights(weights, layers, Y, alpha, activ): lmax = len(layers) - 1 # a useful variable assert len(weights)==lmax # make sure number of weights match up assert len(activ) &gt;= lmax # make sure we defined enough activations for the layers delta = layers[lmax] - Y # error between output and target for el in range(lmax-1, -1, -1): # Count backwards to layer zero fprime = activ[el](np.dot(layers[el], weights[el]), deriv=True) # deriv of activation weights[el] -= alpha * np.dot( layers[el].T, delta*fprime ) # gradient descent step delta = np.dot(delta*fprime, weights[el].T) if (el&gt;0) else None # setup delta for next pass in loop return weights # Routine for training via gradient descent def fit(layers, Y, activ=[sigmoid]*2, use_bias=True, alpha=1.0, maxiter=10000): lmax = len(layers)-1 # max index of layers, also = # of weights if use_bias: # add a column of 1&#39;s to every layer except the last for el in range(lmax): new_col = np.ones((layers[el].shape[0],1)) layers[el] = np.hstack((new_col, layers[el])) # Define weights np.random.seed(1) # for reproducibility weights = [None]*lmax # allocate slots in a blank list for el in range(lmax): # &quot;el&quot; because &quot;l&quot; and &quot;1&quot; may look similar weights[el] = 2*np.random.random((layers[el].shape[1], layers[el+1].shape[1]))-1 loss_hist = [] # start with an empty list for iter in range(maxiter): # Feed-forward pass for el in range(lmax): layers[el+1] = activ[el](np.dot(layers[el], weights[el])) Y_tilde = layers[lmax] # Loss monitoring diff = Y_tilde - Y loss_hist.append( (diff**2).mean() ) # use MSE loss for monitoring # Backprop code will go here weights = update_weights(weights, layers, Y, alpha, activ) return weights, Y_tilde, loss_hist ##### END OF PART TO LEAVE UNCHANGED ##### MAKE YOUR CHANGES BELOW ############## # define the tanh activation function def tanh(x, deriv=False): if deriv: pass # *** Students: replace &#39;pass&#39; with what the derivative should be return np.tanh(x) ## Students: replace X, Y with 7-segment data instead X = np.array([[0,0],[0,1],[1,0],[1,1]],dtype=np.float32) Y = np.array([[0,1,1,0]],dtype=np.float32).T Y_tilde = np.copy(Y) # Just allocates some storage for Y_tilde ## Hidden layers: Students: Change Q, the number of hidden neurons, as needed Q = 10 N = X.shape[0] # this just grabs the number of rows in X H = np.zeros((N,Q)) H2 = np.zeros((N,Q)) # extra hidden layer, might not be used ## Students: change this as instructed layers = [X, H, Y_tilde] # later, add another layer H2 when instructed activ = [sigmoid, sigmoid, sigmoid] # change the first (2) activation(s) as instructed alpha = 0.5 # play around with this ## LEAVE THIS PART UNCHANGED weights, Y_tilde, loss_hist = fit(layers, Y, activ=activ, alpha=alpha) np.set_printoptions(formatter={&#39;float&#39;: lambda x: &quot;{0:0.2f}&quot;.format(x)}) # 2 sig figs print(&quot;Prediction Y_tilde = n&quot;,Y_tilde.T) print(&quot;Target Y (correct answer) = n&quot;,Y.T) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.loglog(loss_hist) . Prediction Y_tilde = [[0.01 0.99 0.99 0.01]] Target Y (correct answer) = [[0.00 1.00 1.00 0.00]] . [&lt;matplotlib.lines.Line2D at 0x7f928397cf98&gt;] . Preview of next lesson: MNIST . Now that you&#39;ve built up some experience with reading digits, let&#39;s move to handwritten digits! This is a problem usually solved with an architecture called a Convolutional Neural Network, but our ordinary feed-forward network can do it too. . The MNIST database of handwritten digits is a classic dataset that every ML student works on. It consists of a large number images of handwritten digits only 28x28 pixels in size. We will &quot;flatten&quot; these into a row of 784 columns, and output a $ tilde{Y}$ of one-hot-encoded vectore just like we did for the output of the 7-segment display (same digits, 0 to 9!). .",
            "url": "https://drscotthawley.github.io/blog/2019/02/08/My-1st-NN-Part-3-Multi-Layer-and-Backprop.html",
            "relUrl": "/2019/02/08/My-1st-NN-Part-3-Multi-Layer-and-Backprop.html",
            "date": " • Feb 8, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "My First Neural Network, Part 2. Bias and CE Loss",
            "content": "Links to lessons: Part 0, Part 1, Part 2, Part 3 . Moving on from our our previous notebook, we will investigate three things we could do to improve the models developed previously: . Add a bias term | Use a different loss function | Add more layers to the network (postponed to next lesson) | 1. Add a bias term . Our weighted sums did not include any constant offset or &quot;bias&quot; term. This may be fine for some data, but not for many others. For example, in a simple linear model $y = mx+b$, the choice of $b=0$ limits the model&#39;s ability to accurately fit some data. . . That is effectively what we did with our weighted sum $ sum_j X_{ij}w_j$: there was no constant offset. To correct this lack, we could add a new variable $b$ and make our weighted sum $b + sum_j X_{ij}w_j$. Equivalently, and more conveniently for the purposes of coding, we could put an additional column of 1&#39;s in the input $X$, and a new row to our weight matrix $w$. By convention, this is usually done with the zeroth element, so that $X_{i0}=1$ and the columns of $X$ are moved to the right, and $w_0 = b$ will be the new constant offset (because $1*w_0 = w_0$.) . For the first problem (Trask&#39;s first problem), this change makes our new matrix equation look like (with new bias terms in red) . $$ f left( overbrace{ left[ { begin{array}{ccc} color{red}1 &amp; 0 &amp; 0 &amp; 1 color{red}1 &amp; 0 &amp; 1 &amp; 1 color{red}1 &amp; 1 &amp; 0 &amp; 1 color{red}1 &amp; 1 &amp; 1 &amp; 1 end{array} } right] }^ text{X} overbrace{ left[ { begin{array}{c} color{red}{w_0} w_1 w_2 w_3 end{array} } right] }^{w} right) = overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^{ tilde{Y}} $$Foreshadowing: Note that in this problem, the rightmost column of $X$ already was a column of all 1&#39;s, and so already has something akin to a bias. Thus, adding a new column of all 1&#39;s will not add any information, and so for this problem we expect that adding the bias won&#39;t improve the model performance.) . With this change, we can still write our weighted sum as $ sum_j X_{ij}w_j$, it&#39;s just that $j$ now runs over 0..3 instead of 0..2. To emphasize: We can leave the rest of our code the same as before, provided we change $X$ by adding a column of 1&#39;s. . In terms of coding the change to $X$, we can either rewrite it by hand, or pull a numpy trick: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline # old data X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) # add a column of 1&#39;s new_col = np.ones((X.shape[0],1)) # array of 1s, w/ same # of rows as X, 1 col wide X_bias = np.hstack((new_col,X)) # stack them horizontally print(X_bias) . [[1. 0. 0. 1.] [1. 0. 1. 1.] [1. 1. 0. 1.] [1. 1. 1. 1.]] . Let&#39;s compare our the use of the bias term without. We&#39;ll define functions for the gradient descent and for the plotting of the loss history, so we can call these again later in this lesson. . Y = np.array([[0,0,1,1]]).T # target output dataset def sigmoid(x,deriv=False): if(deriv==True): return x*(1-x) return 1/(1+np.exp(-x)) def calc_loss(Y_pred, Y, X, w, activ, loss_type=&quot;mse&quot;): # MSE loss diff = Y_pred - Y loss = (diff**2).mean() gradient = np.dot( X.T, diff*activ(Y_pred, deriv=True)) # for weight update return loss, gradient def fit(X, Y, activ, use_bias=True, alpha=3.0, maxiter=10000, loss_type=&#39;mse&#39;): &quot;&quot;&quot; fit: Generic routine for doing our gradient decent Required arguments: X: input matrix Y: target output activ: reference to an activation function Keywork arguments (optional): use_bias: Flag for whether to use bias in the model alpha: learning rate. Tip: Use the largest alpha &#39;you can get away with&#39; maxiter: maximum number of iterations loss_type: Set to MSE for now but we&#39;ll extend this later. &quot;&quot;&quot; if use_bias: # add a column of 1&#39;s to X new_col = np.ones((X.shape[0],1)) X = np.hstack((new_col,X)) # Define weights np.random.seed(1) # for reproducibility if activ == sigmoid: w = 2*np.random.random((X.shape[1],Y.shape[1])) - 1 # -1..1 else: w = np.random.random((X.shape[1],Y.shape[1]))/10 # only positive weights (for later) loss_hist = [] # start with an empty list for iter in range(maxiter): Y_pred = activ(np.dot(X,w)) # compute prediction, i.e. tilde{Y} loss, gradient = calc_loss(Y_pred, Y, X, w, activ, loss_type) loss_hist.append(loss) # add to the history of the loss w -= alpha * gradient # weight update return w, Y_pred, loss_hist # send these values back # Now call the fit function twice, to compare with and without bias: w_old, Y_pred_old, loss_hist_old = fit(X, Y, sigmoid, use_bias=False) w_new, Y_pred_new, loss_hist_new = fit(X, Y, sigmoid) # Plot the results. Make a function so we can call this again later def plot_new_old(loss_hist_old, loss_hist_new, labels=[&quot;no bias&quot;, &quot;with bias&quot;]): plt.loglog(loss_hist_old, label=labels[0]) plt.loglog(loss_hist_new, label=labels[1]) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;MSE Loss (monitoring)&quot;) plt.legend() plt.show() plot_new_old(loss_hist_old, loss_hist_new) # And print the final answers: print(&quot;No bias: Y_pred =&quot;,Y_pred_old) print(&quot;With bias: Y_pred =&quot;,Y_pred_new) . No bias: Y_pred = [[0.00553557] [0.00451069] [0.99632065] [0.9954838 ]] With bias: Y_pred = [[0.00485703] [0.00433876] [0.99612094] [0.99565733]] . As expected, for this problem, the inclusion of bias didn&#39;t make any significant difference. Let&#39;s try the same thing for the 7-segment display problem from Part 1. And let&#39;s try two different runs, one with sigmoid activation, and another with ReLU: . X_7seg = np.array([ [1,1,1,1,1,1,0,1], # 0 [0,1,1,0,0,0,0,1], # 1 [1,1,0,1,1,0,1,1], # 2 [1,1,1,1,0,0,1,1], # 3 [0,1,1,0,0,1,1,1], # 4 [1,0,1,1,0,1,1,1], # 5 [1,0,1,1,1,1,1,1], # 6 [1,1,1,0,0,0,0,1], # 7 [1,1,1,1,1,1,1,1], # 8 [1,1,1,1,0,1,1,1] # 9 ]) Y_7seg = np.eye(10) X, Y = X_7seg, Y_7seg def relu(x,deriv=False): # relu activation if(deriv==True): return 1*(x&gt;0) return x*(x&gt;0) # Call the fit routine twice, once for sigmoid activation, once for relu for activ in [sigmoid, relu]: print(&quot; n n activ = &quot;,activ) alpha = 0.5 if activ == sigmoid else 0.005 # assign learning rate w_old, Y_pred_old, loss_hist_old = fit(X, Y, activ, alpha=alpha, use_bias=False) w_new, Y_pred_new, loss_hist_new = fit(X, Y, activ, alpha=alpha) # Report results plot_new_old(loss_hist_old, loss_hist_new) np.set_printoptions(formatter={&#39;float&#39;: lambda x: &quot;{0:0.2f}&quot;.format(x)}) # 2 sig figs print(&quot;No bias: Y_pred = n&quot;,Y_pred_old) print(&quot;With bias: Y_pred = n&quot;,Y_pred_new) . activ = &lt;function sigmoid at 0x7fa465235d08&gt; . No bias: Y_pred = [[0.98 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.03 0.02] [0.01 0.98 0.00 0.01 0.01 0.00 0.00 0.02 0.00 0.00] [0.00 0.00 0.99 0.01 0.00 0.00 0.01 0.00 0.03 0.00] [0.00 0.00 0.01 0.98 0.00 0.00 0.00 0.01 0.00 0.02] [0.00 0.01 0.00 0.00 0.99 0.00 0.00 0.00 0.03 0.02] [0.00 0.00 0.00 0.01 0.01 0.98 0.02 0.00 0.00 0.02] [0.01 0.00 0.00 0.00 0.00 0.01 0.98 0.00 0.03 0.00] [0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.98 0.00 0.00] [0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.94 0.00] [0.00 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.02 0.96]] With bias: Y_pred = [[0.98 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.03 0.01] [0.01 0.98 0.00 0.01 0.01 0.00 0.00 0.02 0.00 0.00] [0.00 0.00 0.99 0.01 0.00 0.00 0.01 0.00 0.02 0.00] [0.00 0.00 0.01 0.98 0.00 0.00 0.00 0.01 0.00 0.02] [0.00 0.01 0.00 0.00 0.99 0.00 0.00 0.00 0.02 0.02] [0.00 0.00 0.00 0.01 0.01 0.98 0.02 0.00 0.00 0.02] [0.01 0.00 0.00 0.00 0.00 0.01 0.98 0.00 0.03 0.00] [0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.98 0.00 0.00] [0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.95 0.01] [0.00 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.01 0.97]] activ = &lt;function relu at 0x7fa4651f2400&gt; . No bias: Y_pred = [[1.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.00 0.01 0.00] [-0.00 1.00 -0.00 -0.00 0.00 -0.00 -0.00 0.00 -0.00 -0.00] [-0.00 -0.00 1.00 0.00 -0.00 -0.00 -0.00 -0.00 0.01 -0.00] [-0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.00 -0.00 0.00] [-0.00 0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.01 0.00] [-0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.00 -0.00 -0.00 0.00] [-0.00 -0.00 -0.00 -0.00 -0.00 0.00 1.00 -0.00 0.01 -0.00] [0.00 0.00 -0.00 0.00 -0.00 -0.00 -0.00 1.00 -0.00 -0.00] [0.00 -0.00 0.00 -0.00 -0.00 -0.00 0.00 -0.00 0.96 -0.00] [-0.00 -0.00 -0.00 0.00 0.00 0.00 -0.00 -0.00 0.00 1.00]] With bias: Y_pred = [[1.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.00 0.00 0.00] [-0.00 1.00 -0.00 -0.00 0.00 -0.00 -0.00 0.00 -0.00 -0.00] [-0.00 -0.00 1.00 0.00 -0.00 -0.00 -0.00 -0.00 0.00 -0.00] [-0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.00 -0.00 0.00] [-0.00 0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.00 0.00] [-0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.00 -0.00 -0.00 0.00] [-0.00 -0.00 -0.00 -0.00 -0.00 0.00 1.00 -0.00 0.00 -0.00] [0.00 0.00 -0.00 0.00 -0.00 -0.00 -0.00 1.00 -0.00 -0.00] [0.00 -0.00 0.00 -0.00 -0.00 -0.00 0.00 -0.00 0.99 -0.00] [-0.00 -0.00 -0.00 0.00 0.00 0.00 -0.00 -0.00 0.00 1.00]] . ...So for this problem, it seems that adding the bias gave us a bit more accuracy, both for the sigmoid and relu activations. Note: in this example, the learning rates were chosen by experiment; you should get in the habit of going back and experimenting with different learning rates. . Video Interlude: Logistic Regression . What we&#39;ve been doing up until now has been a &quot;classification&quot; problem, with &quot;yes&quot;/&quot;no&quot; answers represented by 1&#39;s and 0&#39;s. This sort of operation is closely associated with the statistical method of Logistic Regression. It is akin to linear regression but with a sigmoid activation function. When doing Logistic Regression, one optimizes to fit by finding the maximum &quot;likelihood&quot; of a given model being correct. . To gain some insight on Logistic Regression, watch the following StatQuest video. (You can ignore his remarks about his preceding video, &quot;R squared&quot; and &quot;p-value&quot;, etc.) . In what follows, we will be minimizing the negative of the logarithm of the likelihood, a quantity typically known as the Cross-Entropy loss. (This same quantity is also the non-constant part of the &quot;Kullback-Leibler Divergence&quot; or &quot;K-L divergence,&quot; so you may hear it called that sometimes.) . 2. Use a different loss function: Cross-Entropy loss . Let&#39;s return to Trask&#39;s first problem for which there is only one target per data point (row) of input, namely a target of 0 or 1. In this case, using the sigmoid function for this classification problem is one of logistic regression, even though we hadn&#39;t it identified it as such. . We&#39;ve been using mean squared error (MSE) loss, but other loss functions exist. In particular, for outputs which are either &quot;yes&quot; or &quot;no&quot; such as the classification problem we are solving, a function called &quot;cross entropy&quot; is typically preferred. The cross-entropy loss is written like this: . $$L_{CE} = - sum_i left[ Y_i log( tilde{Y}_i) + (1-Y_i) log(1- tilde{Y}_i) right]$$ . Note that since the function $ log(x)$ is undefined for $x le0$, we need to make sure $0&lt; tilde{Y}_i&lt;1$ for all $i$. One way to ensure this is to use sigmoid activation! Thus, for classification problems, it is very common to see sigmoid activation (or its multi-class relative &quot;softmax&quot;) immediately before the output, even for many-layer neural networks with all kinds of other activations in other places. . To use the CE loss with gradient descent, we need its derivative with respect to the weights. First let&#39;s write the CE loss in terms of the inputs $X$, weights $w$ and activation function $f$: . ...wait, for compactness, let&#39;s write the weighted sum as $S_i = sum_j X_{ij}w_j$. Ok, now going forward.... . $$L_{CE} = - sum_i left[ Y_i log left(f left(S_i right) right) + (1-Y_i) log left(1- f left(S_{i} right) right) right]$$ . For any function $g(x)$, the derivative of $ log(g(x))$ with respect to x is just $1/g*(du/dx)$, so our partial derivatives with respect to weights look like . $${ partial L_{CE} over partial w_j} = - sum_i left[ {Y_i over tilde{Y_i}}{ partial f(S_i) over partial w_j} - {1-Y_i over 1- tilde{Y}_i} { partial f(S_i) over partial w_j} right] = - sum_i { partial f(S_i) over partial S_i}{ partial S_i over partial w_j} left[ {Y_i over tilde{Y_i}} - {1-Y_i over 1- tilde{Y}_i} right] $$And if we multiply by $2/N$, we can write this as $$ { partial L_{CE} over partial w_j} = {2 over N} sum_{i=0}^{N-1} { partial f(S_i) over partial S_i}X_{ij} left[ { tilde{Y_i}-Y_i over tilde{Y_i}(1- tilde{Y_i}) } right]$$ This is similar to the partial derivatives for our MSE loss, except the term in the denominator is new. To see this more clearly, recall that the weight update for MSE (from Part 1) was $$ w := w - alpha X^T cdot left( [ tilde{Y}-Y]* tilde{Y}*(1- tilde{Y}) right) $$ whereas for CE we actually get a bit of a simplification because the term in the denominator cancels with a similar term in the numerator: $$ w := w - alpha X^T cdot left( [ tilde{Y}-Y]* tilde{Y}*(1- tilde{Y}) right) / ( tilde{Y}*(1- tilde{Y})) w := w - alpha X^T cdot [ tilde{Y}-Y]. $$ Thus despite all this seeming complication, our CE weight update is actually simpler than what it was before as MSE! . Let&#39;s try this out with code now: . def calc_loss(Y_pred, Y, X, w, activ, loss_type=&#39;ce&#39;): diff = Y_pred - Y loss = (diff**2).mean() # MSE loss if &#39;ce&#39; == loss_type: diff = diff / (Y_pred*(1-Y_pred)) # use this for gradient #loss = -(Y*np.log(Y_tilde) + (1-Y)*np.log1p(-Y_tilde)).mean() # CE Loss # Actually we don&#39;t care what the loss itself is. # Let&#39;s use MSE loss for &#39;monitoring&#39; regardless, so we can compare the # effects of using different gradients-of-loss functions gradient = np.dot( X.T, diff*activ(Y_pred, deriv=True)) # same as before return loss, gradient #- X = X_bias Y = np.array([[0,0,1,1]]).T # target output dataset # Compare old and new w_mse, Y_pred_mse, loss_hist_mse = fit(X, Y, sigmoid, alpha=0.5, loss_type=&#39;mse&#39;) w_ce, Y_pred_ce, loss_hist_ce = fit(X, Y, sigmoid, alpha=0.5, loss_type=&#39;ce&#39;) # fit plot_new_old(loss_hist_mse, loss_hist_ce, [&quot;MSE, with bias&quot;, &quot;CE, with bias&quot;]) # And print the final answers: print(&quot;MSE _loss: Y_pred = n&quot;,Y_pred_mse) print(&quot;CE loss: Y_pred = n&quot;,Y_pred_ce) . MSE _loss: Y_pred = [[0.01] [0.01] [0.99] [0.99]] CE loss: Y_pred = [[0.00] [0.00] [1.00] [1.00]] . This works a lot better! To understand why, note that the gradients for MSE loss scale like $$[ tilde{Y}-Y]* tilde{Y}*(1- tilde{Y})$$ and thus these gradients go to zero as $ tilde{Y} rightarrow 0$, and/or $ tilde{Y} rightarrow 1$, which makes training very slow! In contrast, the extra denominator in the CE gradients effectively cancels out this behavior, leaving the remaining term of $$[ tilde{Y}-Y]$$ which varies linearly with the difference from the target value. This makes training much more efficient. . # Recall that ReLU maps negative numbers to 0, and isn&#39;t bounded from above. # Thus the &quot;denominator&quot; in the &#39;diff term&#39; in our earlier code will tend to &#39;explode&#39;. # Put differently, note that log(x) is undefined for x=0, as is log(1-x) for x=1. w_relu_ce, Y_pred_relu_ce, loss_hist_relu_ce = fit(X, Y, relu, alpha=0.001, loss_type=&#39;ce&#39;) plot_new_old(loss_hist_ce, loss_hist_relu_ce, [&quot;CE, sigmoid&quot;, &quot;CE, ReLU&quot;]) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in greater /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in greater . Excercise: . Do the same comparison for the 7-segment display problem: Make a plot showing a comparison of the loss history use MSE loss vs. using CE loss. And print out the final values of Y_pred for each. Use a learning rate of 0.5 and sigmoid activation, with bias. . Take a screenshot of the output and upload it to your instructor. . (Note: for the 7-segment display, since the target $Y$ has multiple columns, we should &quot;normalize&quot; the output in order to be able to properly interpret the output values $ tilde{Y}$ as probabilities. To do so, we would use a softmax activation. For now, we haven&#39;t bothered with this because it would add a bit more math, and is not actually necessary to solve this problem. ) . Next time: Part 3: Multi-Layer Networks and Backpropagation .",
            "url": "https://drscotthawley.github.io/blog/2019/02/04/My-First-NN-Part-2.html",
            "relUrl": "/2019/02/04/My-First-NN-Part-2.html",
            "date": " • Feb 4, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "My First Neural Network, Part 1",
            "content": "Links to lessons: Part 0, Part 1, Part 2, Part 3 . We will be reproducing work from Andrew Trask&#39;s excellent tutorial &quot;A Neural Network in 11 lines of Python&quot;, albeit with a different emphasis, and in a different way. You may regard this treatment and his original treatment as complimentary, and feel free to refer to both. This lesson is written with the intent of building on the lesson about linear regression -- which we might call &quot;Part 0&quot; -- at the link &quot;Following Gravity - ML Foundations Part Ia.&quot; . The Sample Problem . Consider a system that tries to map groups of 3 inputs to some corresponding output which is a single number. In the following picture, we&#39;ll show each set of 3 inputs as a row of a matrix $X$, and each output as the corresponding row of $Y$: . $$ overbrace{ left[ { begin{array}{ccc} 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^Y. $$Even though this system has an exact solution (namely, $Y$ equals the first column of $X$), usually we&#39;ll need to be satisfied with a system that maps our inputs $X$ to some approximate &quot;prediction&quot; $ tilde{Y}$, which we hope to bring closer to the &quot;target&quot; $Y$ by means of successive improvements. . The way we&#39;ll get our prediction $ tilde{Y}$ is by means of a weighted sum of each set of 3 inputs, and some nonlinear function $f$ which we call the &quot;activation function&quot; (or just &quot;activation&quot;). Pictorially, the process looks like the following, for each row $i$ of $X$ and $Y$, (where the columns of $X$ are shown arranged vertically instead of horizonally): . . In terms of matrix multiplication, since X is a 4x3 matrix, and Y is a 4x1 matrix, that implies that our weights should be a 3x1 matrix consisting of (unknown) values $w_0$, $w_1$ and $w_2$. The calculation can be written as: . $$ f left( overbrace{ left[ { begin{array}{ccc} 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 end{array} } right] }^ text{X} overbrace{ left[ { begin{array}{c} w_0 w_1 w_2 end{array} } right] }^{w} right) = overbrace{ left[ { begin{array}{c} 0 0 1 1 end{array} } right] }^{ tilde{Y}} $$Our nonlinear activation function $f$ is taken to operate on each row element one at a time, and we&#39;ll let $f_i$ denote the $i$th row of this completed activation, i.e.: . $$ f_i = f left( sum_j X_{ij}w_j right) = tilde{Y}_i . $$The particular activation function we will use is the &quot;sigmoid&quot;, . $$ f(x) = {1 over{1+e^{-x}}}, $$-- click here to see a plot of this function -- which has the derivative . $$ {df over dx} = {e^{-x} over(1 + e^{-x})^2} $$which can be shown (Hint: exercise for &quot;mathy&quot; students!) to simplify to $$ {df over dx}= f(1-f). $$ . The overall problem then amounts to finding the values of the &quot;weights&quot; $w_0, w_1,$ and $w_2$ so that the $ tilde{Y}$ we calculate is as close to the target $Y$ as possible. . To do this, we will seek to minimize a loss function defined as a sum across all data points we have, i.e. all 4 rows. The loss function $L$ we will choose is the mean square error loss, or MSE (note: later in Part 2 we will use a &#39;better&#39; loss function for this problem): . $$ L = {1 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]^2, $$or in terms of the activation function $$ L = {1 over N} sum_{i=0}^{N-1} left[ f_i - Y_i right]^2. $$ . Each of the weights $w_j$ ($j=0..2$) will start with random values, and then be updated via gradient descent, i.e. . $$ w_j^{new} = w_j^{old} - alpha{ partial L over partial w_j} $$where $ alpha$ is the learning rate, chosen to be some small parameter. For the MSE loss shown above, the partial derivatives with respect to each of the weights is . $$ { partial L over partial w_j} = {2 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]{ partial f_i over partial w_j} = {2 over N} sum_{i=0}^{N-1} left[ tilde{Y}_i - Y_i right]f_i(1-f_i)X_{ij}. $$Absorbing the factor of 2/N into our choice of $ alpha$, and writing the summation as a dot product, and noting that $f_i = tilde{Y}_i$, we can write the update for all the weights together as . $$ w = w - alpha X^T cdot left( [ tilde{Y}-Y]* tilde{Y}*(1- tilde{Y}) right) $$where the $ cdot$ denotes a matrix-matrix product (i.e. a dot product for successive rows of $X^T$) and $*$ denotes elementwise multiplication. . To clarify the above expression in terms of matrix dimensions, we can see that $w$, a 3x1 matrix, can be made by multipyting $X^T$ (a 3 x4 matrix) with the term in parentheses, i.e. the product of elementwise terms involving $ tilde{Y}$, which is a 4x1 matrix. In other words, a 3x4 matrix, times a 4x1 matrix, yields a 3x1 matrix. . Actual Code . The full code for all of this is then... . # https://iamtrask.github.io/2015/07/12/basic-python-network/ import numpy as np # sigmoid activation def sigmoid(x,deriv=False): if(deriv==True): return x*(1-x) return 1/(1+np.exp(-x)) # input dataset X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) # target output dataset Y = np.array([[0,0,1,1]]).T # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly with mean 0 w = 2*np.random.random((3,1)) - 1 alpha = 1.0 # learning rate loss_history = [] # keep a record of how the loss proceeded, blank for now for iter in range(1000): # forward propagation Y_pred = sigmoid(np.dot(X,w)) # prediction, i.e. tilde{Y} # how much did we miss? diff = Y_pred - Y loss_history.append((diff**2).mean()) # add to the history of the loss # update weights w -= alpha * np.dot( X.T, diff*sigmoid(Y_pred, deriv=True)) print(&quot;Output After Training:&quot;) print(&quot;Y_pred = (should be two 0&#39;s followed by two 1&#39;s) n&quot;,Y_pred) print(&quot;weights = n&quot;,w) . Output After Training: Y_pred = (should be two 0&#39;s followed by two 1&#39;s) [[0.03178421] [0.02576499] [0.97906682] [0.97414645]] weights = [[ 7.26283009] [-0.21614618] [-3.41703015]] . Note that, because of our nonlinear activation, we don&#39;t get the solution $w_0=1, w_1=0, w_2=0$. . Plotting the loss vs. iteration number, we see... . %matplotlib inline import matplotlib.pyplot as plt plt.loglog(loss_history) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.show() . Change the activation function . Another popular choice of activation function is the rectified linear unit or ReLU. The function ReLU(x) is zero for x &lt;= 0, and equal to x (i.e. a straight line at 45 degrees for) x &gt;0. It can be written as max(x,0) or x * (x&gt;0), and its derivative is 1 for positive x, and zero otherwise. . Click here to see a graph of ReLU . Modifying our earlier code to use ReLU activation instead of sigmoid looks like this: . def relu(x,deriv=False): # relu activation if(deriv==True): return 1*(x&gt;0) return x*(x&gt;0) # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly (but only &gt;0 because ReLU clips otherwise) w = np.random.random((3,1)) alpha = 0.3 # learning rate new_loss_history = [] # keep a record of how the error proceeded for iter in range(1000): # forward propagation Y_pred = relu(np.dot(X,w)) # how much did we miss? diff = Y_pred - Y new_loss_history.append((diff**2).mean()) # add to the record of the loss # update weights w -= alpha * np.dot( X.T, diff*relu(Y_pred, deriv=True)) print(&quot;Output After Training:&quot;) print(&quot;Y_pred = (should be two 0&#39;s followed by two 1&#39;s) n&quot;,Y_pred) print(&quot;weights = n&quot;,w) . Output After Training: Y_pred = (should be two 0&#39;s followed by two 1&#39;s) [[-0.] [-0.] [ 1.] [ 1.]] weights = [[ 1.01784368e+00] [ 8.53961786e-17] [-1.78436793e-02]] . print( w[2] - (1-w[0]) ) . [-3.46944695e-17] . Plot old results with new results: . %matplotlib inline import matplotlib.pyplot as plt plt.loglog(loss_history,label=&quot;sigmoid&quot;) plt.loglog(new_loss_history,label=&quot;relu&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend() plt.show() . Looks like ReLU may be a better choice than sigmoid for this problem! . Exercise: Read a 7-segment display . A 7-segment display is used for displaying numerical digits 0 through 9, usually by lighting up LEDs or parts of a liquid crystal display (LCD). The segments are labelled $a$ through $g$ according to the following diagram: . . Diagram of the network . The 7 inputs &quot;a&quot; through &quot;g&quot; will be mapped to 10 outputs for the individual digits, and each output can range from 0 (&quot;false&quot; or &quot;no&quot;) to 1 (&quot;true&quot; or &quot;yes&quot;) for that digit. The input and outputs will be connected by a matrix of weights. Pictorially, this looks like the following (Not shown: activation function $f$): . . ...where again, this network operates on a single data point at a time, datapoints which are rows of X and Y. What is shown in the above diagram are the columns of $X$ and $Y$ for a single row (/ single data point). . Create the dataset . Let the input X be the segments $a$ through $g$ are the columns of the input $X$, and are either 1 for on or 0 for off. Let the columns of the target $Y$ be the digits 0-9 themselves arranged in a &quot;one hot&quot; encoding scheme, as follows: . Digit | One-Hot Encoding for $Y$ | . 0 | 1,0,0,0,0,0,0,0,0,0 | . 1 | 0,1,0,0,0,0,0,0,0,0 | . 2 | 0,0,1,0,0,0,0,0,0,0 | . ... | ... | . 9 | 0,0,0,0,0,0,0,0,0,1 | . The values in the columns for $Y$ are essentially true/false &quot;bits&quot; for each digit, answering the question &quot;Is this digit the appropriate output?&quot; with a &quot;yes&quot;(=1) or &quot;no&quot; (=0) response. . The input $X$ will be a 10x7 matrix, and the target $Y$ will be a 10x10 matrix. Each row of $X$ will be the segments to produce the digit for that row. For example, the zeroth row of $X$ should show segments on which make an image of the digit zero, namely segments a, b, c, d, e, and f but not g, so that the zeroth row of X should be [1,1,1,1,1,1,0]. . Define numpy arrays for both $X$ and $Y$ (Hint: for $Y$, check out np.eye()): . # for the 7-segment display. The following is just a &quot;stub&quot; to get you started. X = np.array([ [1,1,1,1,1,1,0], [], [], [] ]) Y = np.array([ [1,0,0,0,0,0,0,0,0,0], [], [] ]) . Initialize the weights . Previously the dimensions of the weight matrix $w$ were 3x1 because we were mapping each row of 3 elements in $X$ to each row of 1 element of $Y$. For this new problem, each row of $X$ has 7 elements, and we want to map those to the 10 elements in each 1-hot-encoded row of $Y$, so what should the dimensions of the weights matrix $w$ be? . Write some numpy code to randomly initialize the weights matrix: . np.random.seed(1) # initial RNG so everybody gets similar results w = np.random.random(( , )) # Students, fill in the array dimensions here . File &#34;&lt;ipython-input-7-20f53a51cded&gt;&#34;, line 1 w = np.random.random(( , )) ^ SyntaxError: invalid syntax . Train the network . Having created an $X$ and its matching $Y$, and initalized the weights $w$ randomly, train a neural network such as the ones above to learn to map a row of X to a row of Y, i.e. train it to recognize digits on 7-segment displays. Do this below. . # Use sigmoid activation, and 1000 iterations, and learning rate of 0.9 # Question: What happens if you use ReLU instead? Try it later. Is ReLU always the best choice? # And then print out your Y_pred &amp; weights matrix, and limit it to 3 significant digits print(&quot;Output After Training:&quot;) np.set_printoptions(formatter={&#39;float&#39;: lambda x: &quot;{0:0.3f}&quot;.format(x)}) # 3 sig figs print(&quot;Y_pred= n&quot;,Y_pred) print(&quot;weights = n&quot;,repr(w)) # the repr() makes it so it can be copied &amp; pasted back into Python code . Final Check: Keras version . Keras is a neural network library that lets us write NN applications very compactly. Try running the following using the X and Y from your 7-segment dataset: . import keras from keras.models import Sequential from keras.layers import Dense, Activation model = Sequential([ Dense(10, input_shape=(7,)), Activation(&#39;sigmoid&#39;) ]) model.compile(optimizer=&#39;adam&#39;, # We&#39;ll talk about optimizer choices and loss choices later loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(X, Y, epochs=200, batch_size=1) print(&quot; nY_tilde = n&quot;, model.predict(X) ) . Follow-up: Remarks . Re-stating what we just did . The original problem (posed at the top of this notebook) involves mapping some points from a 3-dimensional space into points in a 1-dimensional space, i.e. to points on the number line. The mapping is done by the combination of a weighted sum (a linear operation) and a nonlinear &quot;activation function&quot; applied to that sum. The use of an activation function like a sigmoid was originally intended to serve as an analogy of activation of biological neurons. Nonlinear activation functions are source of the &quot;power&quot; of neural networks (essentially we approximate some other function by means of a sum of basis functions in some function space, but don&#39;t worry about that if you&#39;re not math-inclined). The algorithm &#39;learns&#39; to approximate this operation via supervised learning and gradient descent according to some loss function. We used the mean squared error (MSE) for our loss, but lots and lots of different loss functions could be used, a few of which we&#39;ll look at another time. . Question for reflection: Unlike fitting a line $y = mx+b$, the weighted sum in our models in this notebook had no constant &quot;bias&quot; term like $b$. How might we include such a term? . One thing we glossed over: &quot;batch size&quot; . Question: Should we apply the gradient descent &quot;update&quot; to the weights each time we process a single row of $X$ &amp; $Y$, or should we compute the combined loss of all the rows together at the same time, and then do the update? This is essentially asking the same question as &quot;When fitting a line $mx+b$ to a bunch of data points, should we use all the points together to update $m$ and $b,$ or should we do this one point at a time -- compute the gradients of the loss at one point, update the weights, compute gradients at another point, etc.?&quot; . The number of points you use is called the batch size and it is what&#39;s known as a &quot;hyperparameter&quot; -- it is not part of the model per se, but it is a(n important) choice you make when training the model. The batch size affects the learning as follows: Averaging the gradints for many data points (i..e. a large batch size) will produce a smoother loss function and will also usually make the code execute more quickly through the dataset, but updating the weights for every point will cause the algorithm to learn with fewer iterations. . One quick way to observe this is to go up to the Keras code above and change batch_size from 1 to 10, and re-execute the cell. How is the accuracy after 200 iteractions, compared to when batch_size=1? . Terminology: Technically, it&#39;s called &quot;batch training&quot; when you sum the gradients for all the data points before updating the weights, whereas using fewer points is &quot;minibatch training&quot;, and updating for each point (i.e. each row, for us) is Stochastic Gradient Descent* (SGD -- more on these terms here). In practice, there is a tradeoff between smaller vs. larger (mini)batches, which has been the subject of intense scrutiny by researchers over the years. We will have more to say on this later. . For discussion later: In our presentation above, were we using batch training, minibatch training or SGD? . . . *Note: many people will regard SGD as an optimization algorithm per se, and refer to doing SGD even for (mini)batch sizes larger than 1. . Optional: If you want to go really crazy . How about training on this dataset: $$ overbrace{ left[ { begin{array}{cc} 0 &amp; 0 0 &amp; 1 1 &amp; 0 1 &amp; 1 end{array} } right] }^{X} rightarrow overbrace{ left[ { begin{array}{c} 0 1 1 0 end{array} } right] }^Y. $$ Good luck! ;-) (Hint 1: This problem features prominently in the history of Neural Networks, involving Marvin Minsky and &quot;AI Winter.&quot; Hint 2: This whole lesson could instead be entitled &quot;My First Artificial Neuron.&quot;) . Next time, we will go on to Part 2: Bias and CE Loss. . Additional Optional Exercise: Binary Math vs. One-Hot Encoding . For the 7-segment display, we used a one-hot encoding for our output, namely a set of true/false &quot;bits&quot; for each digit. One may wonder how effective this ouput-encoding method is, compared to a different bit-setting encoding method, namely binary representations. . Construct the target output matrix $Y$ for binary representations of the numbers 0 through 9. Your target matrix should have 10 rows and 4 columns (i.e, output bits for 1s, 2s, 4s, and 8s). | Using this $Y$ array, train the network as before, and plot the loss as a function of iteration. | Question: Which method works &#39;better&#39;? One-hot encoding or binary encoding? .",
            "url": "https://drscotthawley.github.io/blog/2019/01/30/My-First-Neural-Network.html",
            "relUrl": "/2019/01/30/My-First-Neural-Network.html",
            "date": " • Jan 30, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "Parallelizing Python, Simplified",
            "content": ". So you have some serial task that takes forever, and you’re thinking it should be parallelizable, but you find the documentation on this to be obtuse? Yea. . Usually I’m interested in either creating lots of data in parallel, or inputting lots of data in parallel, and it’s often something that I first implemented as a loop but got tired of how slow it runs. These involve embarrassingly parallel tasks in that they don’t depend on one another. . There’s a simple prescription for parallelizing most of these kinds of tasks in Python. It goes as follows: . Have some kind of task performed in a for loop. | Write a function that does what you want for one “instance.” For example, take what’s inside one of your for loops, put all that in a separate function. | As a check, keep your loop but use only the function call. Make sure it produces the same results as the original version of your code. | Use functools.partial to create a wrapper for your function. | Replace the loop with a call to Pool.map(). | In the following, we’ll cover 3 examples for parallel tasks: . Generate a bunch of files | Read a bunch of files into a list | Filling a numpy array | Example 1: Generate a bunch of files . Let’s say you have some important synthetic data that you want to generate lots of instances of. For now, for simplicity, we’re just going to generate images of, let’s say, random noise. And to make it interesting we’ll generate 2000 of them. . Here’s the serial for-loop version: . import numpy as np import cv2 n_images = 2000 size_x, size_y = 100, 100 for i in range(n_images): arr = 255*np.random.rand(size_x,size_y) filename = &#39;image_&#39;+str(i)+&#39;.png&#39; print(&quot;writing file &quot;,filename) cv2.imwrite(filename,arr) . Now we write a dedicated function, put it in a partial wrapper, and call it as follows: . import numpy as np import cv2 from functools import partial def write_one_file(size_x, size_y, name_prefix, i): arr = 255*np.random.rand(size_x,size_y) filename = name_prefix + str(i) + &#39;.png&#39; print(&quot;writing file &quot;,filename) cv2.imwrite(filename,arr) n_images = 2000 size_x, size_y = 100, 100 wrapper = partial(write_one_file, size_x, size_y, &#39;image_&#39;) for i in range(n_images): wrapper(i) . Finally we replace the loop with a multiprocessing pool. We can either use all the cpus on the machine (which is the default) or specify how many to use, by giving an argument to Pool(): . import numpy as np import cv2 from functools import partial import multiprocessing as mp def write_one_file(size_x, size_y, name_prefix, i): arr = 255*np.random.rand(size_x,size_y) filename = name_prefix + str(i) + &#39;.png&#39; print(&quot;writing file &quot;,filename) cv2.imwrite(filename,arr) n_images = 2000 size_x, size_y = 100, 100 wrapper = partial(write_one_file, size_x, size_y, &#39;image_&#39;) num_procs = mp.cpu_count() # or can replace with some number of processes to use pool = mp.Pool(num_procs) indices = range(n_images) results = pool.map(wrapper, indices) pool.close() pool.join() . There are other ways you can do this to get more control, e.g. to have each process in the pool receive a particular range of indices, but this basic setup will get the job done. And if you turn off the printing to screen and time the execution, you’ll see the speedup. . Example 2: Read a bunch of files into a list . This example is actually of limited utility and you may want to just skip down to “Example 3: Filling a numpy array,” but it’s still an illustrative example that motivates Example 3, and offers a bit of variety in how one might do things. In this case we’re not going to use Pool.map; instead we’re going to use a context manager for the particular datatype of list. . Let’s try to load in all the image files we just generated, into a list. Here’s the serial version: . import glob import cv2 name_prefix = &#39;image_&#39; # we&#39;ll use glob to get the list of available files # note that glob order isn&#39;t...easily discernible, so we&#39;ll sort. img_file_list = sorted(glob.glob(name_prefix+&#39;*.png&#39;)) n_files = len(img_file_list) print(n_files,&quot;files available.&quot;) img_data_list = [] for i in range(n_files): filename = name_prefix + str(i) + &#39;.png&#39; print(&quot;Reading file&quot;,filename) img = cv2.imread(filename) img_data_list.append(img) print(len(img_data_list),&quot;images in list.&quot;) . (If we wanted to, we could easily convert this list of images to a numpy array. But let’s hold off on that.) . This time, we’ll split up the tasks manually into equal numbers for each process. Parallelizing this can take the following form: . from multiprocessing import Process, Manager, cpu_count import glob import cv2 def load_one_proc(img_data_list, img_file_list, iproc, per_proc): istart, iend = iproc * per_proc, (iproc+1) * per_proc for i in range(istart,iend): # each process will read a range of files filename = img_file_list[i] print(&quot;Reading file&quot;,filename) img = cv2.imread(filename) img_data_list.append(img) return name_prefix = &#39;image_&#39; # we&#39;ll use glob to get the list of available files # note that glob order isn&#39;t...easily discernible, so we&#39;ll sort. img_file_list = sorted(glob.glob(name_prefix+&#39;*.png&#39;)) n_files = len(img_file_list) print(n_files,&quot;files available.&quot;) # We&#39;ll split up the list manually num_procs = cpu_count() print(&quot;Parallelizing across&quot;,num_procs,&quot;processes.&quot;) per_proc = n_files // num_procs # Number of files per processor to load assert n_files == per_proc * num_procs # Make sure tasks divide evenly. Obvously one can do something more sophisticated than this! with Manager() as manager: img_data_list = manager.list() processes = [] for iproc in range(num_procs): p = Process(target=load_one_proc, args=(img_data_list, img_file_list, iproc, per_proc)) p.start() processes.append(p) for p in processes: p.join() outside_list = list(img_data_list) # Copy out of the Manager context (there may be a better way to do this) print(len(outside_list),&quot;images in list.&quot;) . Okay, great. The thing is, that set of processes operates asynchronously, so there’s no telling what order the final list is going to be in. Maybe you don’t care, but sometimes I care. One way of dealing with this is to add an index item within the list for each item, and then sort on that index. . But most of the time what I really want in the end is a numpy array. So let’s just look at how to fill one of those, directly. . Example 3: Filling a NumPy array . Data scientist Jonas Teuwen made a great post which got me started on how to do this, but then it seems I uncovered a bug in numpy’s garbage collection for which there’s now a patch. Even without the patch, there are a couple workarounds one can use, and I’ll choose the simpler of the two workarounds. . Let’s load all those images into a numpy array instead of a list. First the serial version: . import numpy as np import glob import cv2 name_prefix = &#39;image_&#39; img_file_list = sorted(glob.glob(name_prefix+&#39;*.png&#39;)) n_files = len(img_file_list) first_image = cv2.imread(img_file_list[0]) print(n_files,&quot;files available. Shape of first image is&quot;,first_image.shape) print(&quot;Assuming all images are that size.&quot;) img_data_arr = np.zeros([n_files]+list(first_image.shape)) # allocate storage for i in range(n_files): filename = img_file_list[i] print(&quot;Reading file&quot;,filename) img_data_arr[i] = cv2.imread(filename) print(&quot;Finished.&quot;) . For the parallel version, we’re going to have to use a global variable. Sorry, there’s no away around it, because of Python’s Global Interpreter Lock (GIL). . Without further ado, here’s the parallel, numpy version of the ‘loading a list of images’ shown earlier in Example 2. (One other change: rather than specifying ranges of images for each processor – which I did just for the sake of variety – this time we’ll let Pool.map decide how to – evenly – distribute the tasks.) . import numpy as np import glob import cv2 from multiprocessing import Pool, sharedctypes, cpu_count from functools import partial import gc mp_shared_array = None # global variable for array def load_one_proc(img_file_list, i): global mp_shared_array # tmp will end up pointing to the memory address of the shared array we want to populate tmp = np.ctypeslib.as_array(mp_shared_array) filename = img_file_list[i] print(&quot;Reading file&quot;,filename) tmp[i] = cv2.imread(filename) # assign the values into the memory of the shared array return name_prefix = &#39;image_&#39; img_file_list = sorted(glob.glob(name_prefix+&#39;*.png&#39;)) n_files = len(img_file_list) first_image = cv2.imread(img_file_list[0]) print(n_files,&quot;files available. Shape of first image is&quot;,first_image.shape) print(&quot;Assuming all images are that size.&quot;) img_data_arr = np.zeros([n_files]+list(first_image.shape)) # allocate storage tmp = np.ctypeslib.as_ctypes(img_data_arr) # tmp variable avoids numpy garbage-collection bug print(&quot;Allocating shared storage for multiprocessing (this can take a while)&quot;) mp_shared_array = sharedctypes.RawArray(tmp._type_, tmp) num_procs = cpu_count() print(&quot;Parallelizing across&quot;,num_procs,&quot;processes.&quot;) p = Pool(num_procs) wrapper = partial(load_one_proc, img_file_list) indices = range(n_files) result = p.map(wrapper, indices) # here&#39;s where we farm out the op img_data_arr = np.ctypeslib.as_array(mp_shared_array, shape=img_data_arr.shape) # this actually happens pretty fast p.close() p.join() # Next couple lines are here just in case you want to move on to other things # and force garbage collection mp_shared_array = None gc.collect() print(&quot;Finished.&quot;) . So that’s the basic implementation. Let me know in the comments if you have suggestions for improvements, or other ideas! . P.S.- Final Remarks . Added in a couple thoughts, post-facto: . 1. Performance. Notice that what gets passed to p.map() is an iterator, which typically you’ll use as either the indices of the members of an array (as we did just now), or it as a range over the number of processors (kinda like we did in Example 2). In the former case, the system is likely to spawn lots and lots of processes (not all at the same time, but as one ‘job’ finishes the system will spawn a new one, and will keep the “pool” going), which will have a bit of overhead – i.e. latency – each time these start and stop. It’s not much though, and so you probably won’t notice if your goal is merely, “I’m doing this so that I only have to wait 5 minutes instead of an hour to get something done.” If instead you make the indices in the map over the range of processors on your machine and manually break up the array indices into chunks (sort of like we did in Example 2), then you won’t be spawning as many processes and so your latency will be considerably lower. But the gain may be small enough (depending on your system) that you may not notice the difference in performance. Still, if you want to go all-out for performance, then make the pool.map go over the number of procs you want. Otherwise, feel free to trust the system to do its thing for you and just use the array (or list) indices for the iterator. . 2. When Processes Die. Debugging multiprocessing runs is a pain. If one process dies (crashes, seg faults, generates any kind of “Error”), it will hang the entire pool and you won’t know why because the error messages won’t come to stdout or stderr. Look elsewhere for tutorials on tools for debugging multiprocessing runs. Good news is that regular print statements still come to stdout for all processes, so one way of debugging is the age-old method of just loading your code with print statements. .",
            "url": "https://drscotthawley.github.io/blog/2018/12/16/Parallelizing-Python-Simplified.html",
            "relUrl": "/2018/12/16/Parallelizing-Python-Simplified.html",
            "date": " • Dec 16, 2018"
        }
        
    
  
    
        ,"post13": {
            "title": "Resolving Mac OS X Aliases in Python",
            "content": "Mac OSX aliases are not symbolic links. Trying to read one may crash your code. . In an app I’m developing, I want users to be able to easily create a “library” of symbolic links to other places on their machine, and this is most easily achieved for many of them by Cmd-Option-Dragging and dropping the files. This creates an “alias”, which is a special file that Apple dreamed up. UNIX users are accustomed to symbolic links, and codes written in UNIX will not follow or “resolve” Mac aliases. Instead, they will cause an exception to be thrown. . There used to be some libraries to handle this, but they relied on Apple’s old Carbon framework which is no longer supported. There is a mac_alias package but the documentation is lacking. So, I found an old post on MacWorld where one solution is given, and I ported that for what I need. . Happy to share with you, so that you won’t have to worry about this. As an added bonus, you can tell it to convert aliases to symbolic links, so that “next time” you won’t have to deal with this. Enjoy. . #!/usr/bin/env python3 # # Resolve Mac OS X &#39;aliases&#39; by finding where they point to # Author: Scott H. Hawley # # Description: # Mac OSX aliases are not symbolic links. Trying to read one will probably crash your code. # Here a few routines to help. Run these to change the filename before trying to read a file. # Intended to be called from within other python code # # Python port modified from https://hints.macworld.com/article.php?story=20021024064107356 # # Requirements: osascript (AppleScript), platform, subprocess, shlex # # TODO: - could make it work in parallel when mutliple filenames are given # # NOTE: By default, this only returns the names of the original source files, # but if you set convert=True, it will also convert aliases to symbolic links. # import subprocess import platform import os # returns true if a file is an OSX alias, false otherwise def isAlias(path, already_checked_os=False): if (not already_checked_os) and (&#39;Darwin&#39; != platform.system()): # already_checked just saves a few microseconds ;-) return False checkpath = os.path.abspath(path) # osascript needs absolute paths # Next several lines are AppleScript line_1=&#39;tell application &quot;Finder&quot;&#39; line_2=&#39;set theItem to (POSIX file &quot;&#39;+checkpath+&#39;&quot;) as alias&#39; line_3=&#39;if the kind of theItem is &quot;alias&quot; then&#39; line_4=&#39; return true&#39; line_5=&#39;else&#39; line_6=&#39; return false&#39; line_7=&#39;end if&#39; line_8=&#39;end tell&#39; cmd = &quot;osascript -e &#39;&quot;+line_1+&quot;&#39; -e &#39;&quot;+line_2+&quot;&#39; -e &#39;&quot;+line_3+&quot;&#39; -e &#39;&quot;+line_4+&quot;&#39; -e &#39;&quot;+line_5+&quot;&#39; -e &#39;&quot;+line_6+&quot;&#39; -e &#39;&quot;+line_7+&quot;&#39; -e &#39;&quot;+line_8+&quot;&#39;&quot; args = shlex.split(cmd) # shlex splits cmd up appropriately so we can call subprocess.Popen with shell=False (better security) p = subprocess.Popen(args, shell=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) retval = p.wait() if (0 == retval): line = p.stdout.readlines()[0] line2 = line.decode(&#39;UTF-8&#39;).replace(&#39; n&#39;,&#39;&#39;) if (&#39;true&#39; == line2): return True else: return False else: print(&#39;resolve_osx_alias: Error: subprocess returned non-zero exit code &#39;+str(retval)) return None # returns the full path of the file &quot;pointed to&quot; by the alias def resolve_osx_alias(path, already_checked_os=False, convert=False): # single file/path name if (not already_checked_os) and (&#39;Darwin&#39; != platform.system()): # already_checked just saves a few microseconds ;-) return path checkpath = os.path.abspath(path) # osascript needs absolute paths # Next several lines are AppleScript line_1=&#39;tell application &quot;Finder&quot;&#39; line_2=&#39;set theItem to (POSIX file &quot;&#39;+checkpath+&#39;&quot;) as alias&#39; line_3=&#39;if the kind of theItem is &quot;alias&quot; then&#39; line_4=&#39; get the posix path of (original item of theItem as text)&#39; line_5=&#39;else&#39; line_6=&#39;return &quot;&#39;+checkpath+&#39;&quot;&#39; line_7 =&#39;end if&#39; line_8 =&#39;end tell&#39; cmd = &quot;osascript -e &#39;&quot;+line_1+&quot;&#39; -e &#39;&quot;+line_2+&quot;&#39; -e &#39;&quot;+line_3+&quot;&#39; -e &#39;&quot;+line_4+&quot;&#39; -e &#39;&quot;+line_5+&quot;&#39; -e &#39;&quot;+line_6+&quot;&#39; -e &#39;&quot;+line_7+&quot;&#39; -e &#39;&quot;+line_8+&quot;&#39;&quot; args = shlex.split(cmd) # shlex splits cmd up appropriately so we can call subprocess.Popen with shell=False (better security) p = subprocess.Popen(args, shell=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) retval = p.wait() if (0 == retval): line = p.stdout.readlines()[0] source = line.decode(&#39;UTF-8&#39;).replace(&#39; n&#39;,&#39;&#39;) if (convert): os.remove(checkpath) os.symlink(source, checkpath) else: print(&#39;resolve_osx_aliases: Error: subprocess returned non-zero exit code &#39;+str(retval)) source = &#39;&#39; return source # used for multiple files at a time, just a looped call to resolve_osx_alias def resolve_osx_aliases(filelist, convert=False): # multiple files #print(&quot;filelist = &quot;,filelist) if (&#39;Darwin&#39; != platform.system()): return filelist outlist = [] for infile in filelist: source = resolve_osx_alias(infile, already_checked_os=True, convert=convert) if (&#39;&#39; != source): outlist.append(source) #print(&quot;outlist = &quot;,outlist) return outlist if __name__ == &quot;__main__&quot;: import argparse parser = argparse.ArgumentParser(description=&#39;Resolve OSX aliases&#39;) parser.add_argument(&#39;file&#39;, help=&quot;alias files to resolve&quot;, nargs=&#39;+&#39;) args = parser.parse_args() outlist = resolve_osx_aliases(args.file) print(&quot;outlist = &quot;,outlist) . The above code is part of the utils/ directory in my Panotti package. The way it’s called is in the context of trying to read an audio file, called in the file panotti/datautils.py: . def load_audio(audio_path, mono=None, sr=None, convertOSXaliases=True): # wrapper for librosa.load try: signal, sr = librosa.load(audio_path, mono=mono, sr=sr) # try to read the file &#39;normally&#39; except NoBackendError as e: if (&#39;Darwin&#39; == platform.system()): # if an exception is thrown, check: Am I on a Mac? If so try to resolve an alias source = resolve_osx_alias(audio_path, convert=convertOSXaliases, already_checked_os=True) # ...and convert to symlinks for next time try: signal, sr = librosa.load(source, mono=mono, sr=sr) # Now try to read again except NoBackendError as e: # Ok, even that didn&#39;t work, giving up (for now). print(&quot; n*** ERROR: Could not open audio file {}&quot;.format(audio_path),&quot; n&quot;,flush=True) raise e else: # Failure for some other reason. print(&quot; n*** ERROR: Could not open audio file {}&quot;.format(audio_path),&quot; n&quot;,flush=True) raise e return signal, sr . Happy coding! . NOTE: Currently this code only follows _one_ alias. If there’s an alias pointing to an alias to a file, it won’t resolve to that file. Full generality would involve adding an iterative or recursive way of traversing multiple aliases which…I may do later. ;-) .",
            "url": "https://drscotthawley.github.io/blog/2018/02/21/Resolving-OSX-Aliases.html",
            "relUrl": "/2018/02/21/Resolving-OSX-Aliases.html",
            "date": " • Feb 21, 2018"
        }
        
    
  
    
        ,"post14": {
            "title": "Notes on Convexity of Loss Functions for Classification",
            "content": ". Edit 7/18/2020: . I wrote this when I was a bit new to the nitty-gritty of ML. If I had this to do all over again, I would simplify the following by just saying to myself: &quot;It is the combination of cross-entropy (or log-likelihood) loss with the sigmoid activation function that gives you the nice property: whereby the gradient becomes monotonic (in the case of either class) -- i.e. the system is convex.&quot; And that would be sufficient. . . This is a post where I&#39;m investigating convexity a bit, as it relates to neural networks. . Andrew Ng, in Week 3 of his Coursera course on Machine Learning, shows the following image with respect to the &quot;cost function&quot; to be optimized: . Andrew Ng, in Week 3 of his Coursera course on Machine Learning, shows the following image with respect to the &quot;cost function&quot; to be optimized (as slide 14 of Lecture 6): . (image is giving problems, sorry) . I wanted to investigate this:Could I reproduce the two graphs he sketched? The two different loss functions are the mean squared error (MSE or sometimes just SE) and cross entropy (CE): $$ MSE = {1 over m} sum_{i=1}^m left( y_i - h_i right)^2 $$ . $$ CE = - {1 over m} sum_{i=1}^m left[ y_i log(h_i) + (1-y_i) log(1-h_i) right] $$ . where $y_i$ are the true values (0 or 1) and $h_i = h(x_i)$ are the predictions. . TL/DR: No I can&#39;t reproduce his sketches. The graph I get for sum of the squared error (SE) doesn&#39;t have the wiggles that his drawing on the left does. (Perhaps he was just doodling an example of an arbitrary non-convex function, rather than the squared loss in particular?) Takeways at the bottom of this, re. the difference between a convex loss function (by itself) vs. a convex loss for a problem -- i.e. the individual terms are convex for either function, but the sum of these terms is actually not strictly convex for either function (for this problem). . I read a few posts about this first... . Math StackExchange: Show that logistic regression with squared loss function is non-convex, which includes a link to this nice demo on Desmos | https://math.stackexchange.com/questions/2193478/loss-function-for-logistic-regression | https://en.wikipedia.org/wiki/Loss_functions_for_classification seems to say that squared loss is convex. ?? | . ...but then wanted to try for myself. As follows: . #%matplotlib ipympl %matplotlib inline from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt from matplotlib import cm import numpy as np . def h(x,a,b): # h = logistic function. a is &#39;weight&#39; and b is &#39;bias&#39; return 1/(1 + np.exp(-(a*x + b))) # For code below, a &amp; b should be scalars, x can be anything def classify_x(x, threshold): out = 0*x out[x &gt; threshold] = 1.0 return out . num_x = 25 x_arr = np.linspace(-5,5,num_x) # _arr denotes&quot;array&quot; threshold = 0.7314 # threshold value chosen arbitrarily y_arr = classify_x(x_arr, threshold) # make a prediction a_guess = 10/(x_arr[1]-x_arr[0]) b_guess = -a_guess * threshold print(&quot;Prediction guess: a =&quot;,a_guess,&quot;, b =&quot;,b_guess) h_arr = np.array(h(x_arr, a_guess, b_guess)) # plot the data fig = plt.figure() plt.plot(x_arr, y_arr,&#39;o&#39;,color=&#39;red&#39;,label=&quot;Truth y&quot;) plt.plot(x_arr, h_arr,&#39;x-&#39;,color=&#39;green&#39;,label=&quot;Prediction h&quot;) plt.legend() plt.show() . Prediction guess: a = 23.999999999999982 , b = -17.55359999999999 . def calc_se_loss(y_arr, h_arr): # squared error loss return np.mean( (y_arr - h_arr)**2) def calc_ce_loss(y_arr, h_arr): # cross-entropy loss, related to Kullback-Liebler divergence eps = 1.0e-16 # added to avoid log(0) errors return -np.mean( y_arr*np.log(h_arr) + (1-y_arr)*np.log(1-h_arr+eps) ) # elementwise multiplication . # define parameter space over which to plot num_a = 100 a_arr = np.linspace(-a_guess,4*a_guess,num_a) b_arr = b_guess + 0*a_arr # make at the b&#39;s all the same value for this first plot. se_loss = [] ce_loss = [] for i in range(a_arr.shape[0]): # cycle through all the values of a and b, getting a different loss for each h_arr = h(x_arr, a_arr[i], b_arr[i]) se_loss.append( calc_se_loss(y_arr, h_arr) ) ce_loss.append( calc_ce_loss(y_arr, h_arr) ) # plot 1-d version fig = plt.figure() ax = fig.gca() ax.set_xlabel(&#39;a&#39;) #ax.set_ylim(2.38, 2.41) # zoom in on flat part on the right plt.plot(a_arr, np.log(se_loss),&#39;o-&#39;,color=&#39;red&#39;,label=&quot;SE Loss&quot;) plt.plot(a_arr, np.log(ce_loss),&#39;o-&#39;,color=&#39;blue&#39;,label=&quot;CE Loss&quot;) ax.set_ylabel(&quot;log of loss&quot;) plt.legend() plt.show() . . EDIT (later): A reader pointed out that the use of the log function distorts the shape of the function and thereby obscures the visual inspection of the convexity. Ooops. See. I was new to this! . . In the above figure, it looks like the SE loss goes &#39;flat&#39; for a bit on the left, but never turns upward until after the global minimum. The CE loss...I can see a few places where we could connect two points with a straight line and not have all of the line lie with in the epigraph). Still the lack of flat regions for the blue line would make it preferable for gradient-based optimization. . Plot the full error surface . num_a = 100 # try experimenting: play around with the max &amp; min of the a &amp; b values to see the surface a_arr = np.linspace(-a_guess/2,a_guess*2,num_a) b_arr = np.linspace(b_guess*2,-b_guess/2,num_a) A, B = np.meshgrid(a_arr, b_arr) def plot_loss_surf(A, B, x_arr, loss=&#39;SE&#39;): Z = np.zeros([len(A), len(B)]) if (&#39;SE&#39; == loss): for i in range(len(A)): for j in range(len(B)): h_arr = h(x_arr, a_arr[i], b_arr[j]) Z[j, i] = calc_se_loss(y_arr, h_arr) else: for i in range(len(A)): for j in range(len(B)): h_arr = h(x_arr, a_arr[i], b_arr[j]) Z[j, i] = calc_ce_loss(y_arr, h_arr) fig = plt.figure() ax = fig.gca(projection=&#39;3d&#39;) surf = ax.plot_surface(A, B, np.log(Z), cmap=cm.coolwarm, linewidth=0, antialiased=False) ax.set_xlabel(&#39;a&#39;) ax.set_ylabel(&#39;b&#39;) ax.set_zlabel(&#39;log(&#39;+loss+&#39;)&#39;) #ax.set_zlim(2, 2.5) ax.set_title(loss+&#39; loss&#39;) #fig.colorbar(surf, shrink=0.5, aspect=5) # Add a color bar which maps values to colors. plt.show() plot_loss_surf(A,B,x_arr,loss=&#39;SE&#39;) plot_loss_surf(A,B,x_arr,loss=&#39;CE&#39;) . The Verdict . So, although the individual terms $(y_i - h_i)^2$ and/or $[y_i log(h_i)+(1-y_i) log(1-h_i)]$ are individually convex, the sum for either type of loss terms is actually non-convex for this problem. Although neither give rise to unwanted local minima for this problem. . The SE loss, while at least not having any non-global minima, still has multiple significant flat regions that would prove tedious for gradient descent optimiazation, whereas in contrast, the CE loss is smoother and is strictly monotonic on either side of the global minimum. The the CE loss (and/or KL divergence) would be preferable for this problem -- you could do it with SE loss assuming you had momentum or some fancy optimization algorithm, but it would take longer and why bother? . Aside: Solve the logistic regression problem using scikit-learn . from sklearn import linear_model clf = linear_model.LogisticRegression(C=1e5, solver=&#39;lbfgs&#39;) clf.fit(x_arr[:, np.newaxis], y_arr) a_fit, b_fit = clf.coef_[0][0], clf.intercept_[0] print(&quot;Fit paramters: a =&quot;,a_fit,&quot;, b =&quot;,b_fit) print(&quot;Predicted threshold: x = &quot;,-b_fit/a_fit) . Fit paramters: a = 32.449347939033615 , b = -20.23838504197379 Predicted threshold: x = 0.6236915786412106 . Afterward: &quot;But you still haven&#39;t found the global minimum!&quot; . In the surface plots above, we see the minimum of the surface going lower and lower -- even lower than the supposedly &#39;optimum&#39; parameters we just found via scikit-learn. The reason is that there is no optimal paramter combination: The steepness parameter $a$ of the sigmoid function $h(x)$ is only bounded from below by the data in this problem. Thus there is no upper bound. The data will constrain the center of the sigmoid $x_0 = -b/a$ to some extent (i.e. it needs to lie between two values of $x_i$), but other than that... . So how then does the loss function seem to get lower and lower? The steeper the sigmoid function, the more closely its values will approach 0 and 1 on either side. Thus for this problem, the optimal solution is $a rightarrow infty$, with $b = -({ rm threshold})/a$. .",
            "url": "https://drscotthawley.github.io/blog/2017/11/22/Convexity-Loss-Classification.html",
            "relUrl": "/2017/11/22/Convexity-Loss-Classification.html",
            "date": " • Nov 22, 2017"
        }
        
    
  
    
        ,"post15": {
            "title": "Crash Course On GANs",
            "content": ". Image credit: Dev Nag . This post is not necessarily a crash course on GANs. It is at least a record of me giving myself a crash course on GANs. Adding to this as I go. . Intro/Motivation . I’ve been wanting to grasp the seeming-magic of Generative Adversarial Networks (GANs) since I started seeing handbags turned into shoes and brunettes turned to blondes… …and seeing Magic Pony’s image super-resolution results and hearing that Yann Lecun had called GANs the most important innovation in machine learning in recent years. . Finally, seeing Google’s Cat-Pig Sketch-Drawing Math… . …broke me, and so…I need to ‘get’ this. . I’ve noticed that, although people use GANs with great success for images, not many have tried using them for audio yet (Note: see SEGAN paper, below). Maybe with already-successful generative audio systems like WaveNet, SampleRNN (listen to those piano sounds!!) and TacoTron there’s less of a push for trying GANs. Or maybe GANs just suck for audio. Guess I’ll find out… . Steps I Took . Day 1: . Gathered list of some prominent papers (below). | Watched video of Ian Goodfellow’s Berkeley lecture (notes below). | Started reading the EBGAN paper (notes below)… | …but soon switched to BEGAN paper – because wow! Look at these generated images: | Googled for Keras-based BEGAN implementations and other code repositories (below)…Noticed SEGAN… | …Kept reading BEGAN, making notes as I went (below). | Finished paper, started looking through BEGAN codes from GitHub (below) &amp; began trying them out… a. Cloned @mokemokechicken’s Keras repo, grabbed suggested LFW database, converted images via script, ran training… Takes 140 seconds per epoch on Titan X Pascal. . Main part of code is in models.py | . b. Cloned @carpedm’s Tensorflow repo, looked through it, got CelebA dataset, started running code. . | Leaving codes to train overnight. Next time, I’d like to try to better understand the use of an autoencoder as the discriminator. | Day 2: . My office is hot. Two Titan X GPUs pulling ~230W for 10 hours straight has put the cards up towards annoyingly high temperatures, as in ~ 85 Celsius! My previous nightly runs wouldn’t even go above 60 C. But the results – espically from the straight-Tensorflow code trained on the CelebA dataset – are as incredible as advertised! (Not that I understand them yet. LOL.) The Keras version, despite claiming to be a BEGAN implementation, seems to suffer from “mode collapse,” i.e. that too many very similar images get generated. | Fished around a little more on the web for audio GAN applications. Found an RNN-GAN application to MIDI, and found actual audio examples of what not to do: don’t try to just produce spectrograms with DCGAN and convert them to audio. The latter authors seem to have decided to switch to a SampleRNN approach. Perhaps it would be wise to heed their example? ;-) | Since EBGAN implemented autoencoders as discriminators before BEGAN did, I went back to read that part of the EBGAN paper. Indeed, section “2.3 - Using AutoEncoders” (page 4). (see notes below) | Ok, I basically get the autoencoder-discriminator thing now. :-) | Day 3: . “Life” intervened. :-/ Hope to pick this up later. . Papers . Haven’t read hardly any of these yet, just gathering them here for reference: . Original GAN Paper: ” Generative Adversarial Networks” by GoodFellow (2014) | DCGAN: “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” by Radford, Metz &amp; Chintala (2015) | “Image-to-Image Translation with Conditional Adversarial Networks” by Isola et al (2016) | “Improved Techniques for Training GANs” by Salimans et al (2016). | DiscoGAN: “Learning to Discover Cross-Domain Relations with Generative Adversarial Networks” by Kim et al. (2017) | EBGAN: “Energy-based Generative Adversarial Network by Zhao, Matheiu &amp; Lecun (2016/2017). Remarks/Notes: | “This variant [EBGAN] converges more stably [than previous GANs] and is both easy to train and robust to hyper-parameter variations” (quoting from BEGAN paper, below). | If it’s energy-based, does that mean we get a Lagrangian, and Euler-Lagrange Equations, and Lagrange Multipliers? And thus can physics students (&amp; professors!) grasp these networks in a straightforward way? Should perhaps take a look at Lecun’s Tutorial on Energy-Based Learning. | “The energy is the resconstruction error [of the autoencoder]” (Section 1.3, bullet points) | | . Image credit: Roozbeh Farhoodi + EBGAN authors . “…256×256 pixel resolution, without a multi-scale approach.” (ibid) | Section 2.3 covers on the use of the autoencoder as a discriminator. Wow, truly, the discriminator’s “energy”/ “loss” criterion is literally just the reconstruction error of the autoencoder. How does that get you a discriminator?? | It gets you a discriminator because the outputs of the generator are likely to have high energies whereas the real data (supposedly) will produce low energies: “We argue that the energy function (the discriminator) in the EBGAN framework is also seen as being regularized by having a generator producing the contrastive samples, to which the discrim- inator ought to give high reconstruction energies” (bottom of page 4). | . | “Wasserstein GAN (WGAN) by Arjovsky, Chintala, &amp; Bottou (2017) . | “BEGAN: Boundary Equilibrium Generative Adversarial Networks” by Berthelot, Schumm &amp; Metz (April 2017). . Remarks/Notes: | “Our model is easier to train and simpler than other GANs architectures: no batch normalization, no dropout, no transpose convolutions and no exponential growth for convolution filters.” (end of section 3.5, page 5) | This is probably not the kind of paper that anyone off the street can just pick up &amp; read. There will be math. | Uses an autoencoder for the discriminator. | I notice that Table 1, page 8 shows “DFM” (from “Improving Generative Adversarial Networks with Denoising Feature Matching” by Warde-Farley &amp; Bengio, 2017) as scoring higher than BEGAN. | page 2: “Given two normal distributions…with covariances C1,C2C_1, C_2C1​,C2​,…”: see “Multivariate Normal Distribution”. | Section 3.3, Equilibrium: The “E[ ] mathbb{E}[ ]E[ ]” notation – as in E[L(x)] mathbb{E} left[ mathcal{L}(x) right]E[L(x)] – means “expected value.” See https://en.wikipedia.org/wiki/Expected_value | Introduces the diversity ratio: γ=E[L(G(z))]E[L(x)] gamma= frac{ mathbb{E} left[ mathcal{L}(G(z)) right]}{ mathbb{E} left[ mathcal{L}(x) right]}γ=E[L(x)]E[L(G(z))]​. “Lower values of γ gammaγ lead to lower image diversity because the discriminator focuses more heavily on auto-encoding real images.” | “3.5 Model architecture”: Did not actually get the bit about the autoencoder as the discriminator: “How does an autoencoder output a 1 or a zero?” | Ok, done. Will come back later if needed; maybe looking at code will make things clearer… | . | “SEGAN: Speech Enhancement Generative Adversarial Network” by Pascual, Bonafonte &amp; Serra (April 2017). Actual audio GAN! They only used it to remove noise. | . Videos . Ian Goodfellow (original GAN author), Guest lecture on GANs for UC Berkeley CS295 (Oct 2016). 1 hour 27 minutes. NOTE: actually starts at 4:33. Watch at 1.25 speed. Remarks/Notes: | This is on a fairly “high” level, which may be too much for some viewers; if hearing the words “probability distribution” over &amp; over again makes you tune out, and e.g. if you don’t know what a Jacobian is, then you may not want to watch this. | His “Taxonomy of Generative Models” is GREAT! | The discriminator is just an ordinary classifier. | So, the generator’s cost function can be just the negative of the discriminator’s cost function, (i.e. it tries to “mess up” the discriminator), however that can saturate (i.e. produce small gradients) so instead they try to “maximize the probability that the discriminator will make a mistake” (44:12). | “KL divergence” is a measure of the ‘difference’ between two PD’s. | “Logit” is the inverse of the sigmoid/logistical function. (logit&lt;–&gt;sigmoid :: tan&lt;–&gt;arctan) | Jensen-Shannon divergence is a measure of the ‘similarity’ between two PD’s. Jensen-Shannon produces better results for GANs than KL/maximum likelihood. | . | . Web Posts/Tutorials . “Machine Learning is Fun Part 7: Abusing Generative Adversarial Networks to Make 8-bit Pixel Art” by Adam Geitgey, skip down to “How DCGANs Work” (2017) | Post on BEGAN: https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/ | An introduction to Generative Adversarial Networks (with code in TensorFlow) | “Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)” by Dev Nag (2017) | “Stability of Generative Adversarial Networks” by Nicholas Guttenberg (2016) | “End to End Neural Art with Generative Models” by Bing Xu (2016) . | Kording Lab GAN Tutorial by Roozbeh Farhoodi :-). Nicely done, has code too. . | . Code . Keras: . ‘Basic’ GAN with MNIST example: https://www.kdnuggets.com/2016/07/mnist-generative-adversarial-model-keras.html | GAN, BiGAN &amp; Adversarial AutoEncoder: https://github.com/bstriner/keras-adversarial | Kording Lab’s GAN tutorial, Jupyter Notebook https://github.com/KordingLab/lab_teaching_2016/blob/master/session_4/Generative%20Adversarial%20Networks.ipynb. (Code is short and understandable.) | Keras BEGAN: https://github.com/mokemokechicken/keras_BEGAN: Only works on 64x64 images; BEGAN paper shows some 128x128 | https://github.com/pbontrager/BEGAN-keras: No documentation, and I don’t see how it could run. I notice local variables being referenced in models.py as if they’re global. | . | Keras DCGAN (MNIST): https://github.com/jacobgil/keras-dcgan | Auxiliary Classifier GAN: https://github.com/lukedeo/keras-acgan | . Tensorflow: . BEGAN-Tensorflow: https://github.com/carpedm20/BEGAN-tensorflow | EBGAN.Tensorflow: https://github.com/shekkizh/EBGAN.tensorflow | SEGAN: https://github.com/santi-pdp/segan | DCGAN-Tensorflow: https://github.com/carpedm20/DCGAN-tensorflow | . PyTorch: . Tutorial &amp; simple implementation: https://github.com/devnag/pytorch-generative-adversarial-networks | . Datasets . CelebA: https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html | MNIST: https://yann.lecun.com/exdb/mnist/ | Speech enhancement: https://datashare.is.ed.ac.uk/handle/10283/1942 | “Labelled Faces in the Wild” https://vis-www.cs.umass.edu/lfw/ | . More References (Lists) . “Delving deep into Generative Adversarial Networks (GANs): A curated list of state-of-the-art publications and resources about Generative Adversarial Networks (GANs) and their applications.” | .",
            "url": "https://drscotthawley.github.io/blog/2017/05/05/Crash-Course-On-GANs.html",
            "relUrl": "/2017/05/05/Crash-Course-On-GANs.html",
            "date": " • May 5, 2017"
        }
        
    
  
    
        ,"post16": {
            "title": "Learning Room Shapes (with Neural Networks)",
            "content": "‘Deep’ neural network learns (boxy) room shape given mode frequencies, or vice versa. . This is just a quick note, sharing a new “toy” result, that relates to function spaces, acoustics, and machine learning. I never bothered to publish it, because I was just “playing around”. The real trick would be to extend this to “non-boxy” room shapes, which…turns out to be hard! haha . Main Python code for this is here. . Motivation . During Karlheinz Brandenburg’s visit, he remarked that learning room shapes from the sound of the room is still an open question. So yesterday, “for fun” I decided to try the easiest possible thing I could think of, the most “ideal” if you will: wrote a NN system that learns to use a set of room mode frequencies to predict (boxy) room dimensions, or vice versa, i.e. it learns the “Rayleigh Equation” for 3D standing waves… . . …both ‘forwards’ or ‘backwards’: [L,W,H] &lt;–&gt; [list of mode frequencies] (given speed of sound v_s). . Results . Seems to learn within +/- 5%. I don’t have a graphical representation of going from mode frequencies to room dimensions (any ideas?). For mapping dimensions to frequencies, here’s a sample room shown in the picture (red=target, green=predicted), used a training set of 200,000 random rooms. It even learns to sort the freqs in ascending order. . . Interestingly, trying to ‘help’ the network by using squared frequencies, inverse-(squared)-dimensions, etc. actually gave worse performance than letting the network learn the nonlinearities itself. (???) . Of what possible practical utility is this? Not really sure. Although, it does a fairly decent job learning the room shape even using a smaller random subsample of the available mode frequencies. :-) If anything, it highlights a weakness of data-driven modeling: there’s no way you’ll measure 200,000 rooms in real life! . Future Work . Taking this further, would require analyzing the weights of the network in detail, to see how it represents the function approximation. Which would be worth doing! . Moving on to non-boxy rooms would be relevant and challenging. Some means of encoding the room geometry would be needed, as well some way of simulating (or measuring?) resonant frequencies or impulse responses. .",
            "url": "https://drscotthawley.github.io/blog/2017/05/04/Learning-Room-Shapes.html",
            "relUrl": "/2017/05/04/Learning-Room-Shapes.html",
            "date": " • May 4, 2017"
        }
        
    
  
    
        ,"post17": {
            "title": "Following Gravity - ML Foundations Part I",
            "content": "*Image credit: NASA* . Preface: I&#39;m writing this for myself, current students &amp; ASPIRE collaborators, and to &#39;give back&#39; to the internet community. I recently had insight into my &#39;main&#39; research problem, but started to hit a snag so decided to return to foundations. Going back to basics can be a good way to move forward... . By the end of this session, we will -- as an example problem -- have used the 1-dimensional path of an object in the presesece of gravity, to &#39;train&#39; a system to correctly infer (i.e. to &#39;learn&#39;) the constants of the motion -- initial position and velocity, and the acceleration due to gravity. Hopefully we learn a few other things along the way. ;-) . In the next installment, &quot;Part Ib,&quot; we&#39;ll derive the differential equation of motion, and in then in &quot;Part II&quot; we&#39;ll adapt the techniques we&#39;ve learned here to do signal processing. . Optimization Basics: Gradient Descent . Let&#39;s put the &quot;sample problem&quot; aside for now, and talk about the general problem of optimization. Often we may wish to minimize some function $f(x)$. In science, doing so may enable us to fit a curve to our data, as we&#39;ll do below. Similarly,&#39;machine learning&#39; systems often operate on the basis of minimizing a &#39;cost&#39; function to discern patterns in complex datasets. . Thus we want to find the value of $x$ for which $f(x)$ is the smallest. A graph of such a function might look like this... . (Python code follows, to make the graph) . import numpy as np, matplotlib.pyplot as plt fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;f&#39;}) x = np.arange(-5,7,0.1) ax.plot(x,(x-1)**2+1) plt.show() . If $f(x)$ is differentiable and the derivative (i.e., slope) $df/dx$ can be evaluated easily, then we can perform a so-called &quot;gradient descent&quot;. . We do so as follows: . Start with some initial guess for $x$ | &quot;Go in the direction of $-df/dx$&quot;: $$x_{new} = x_{old} - alpha {df over dx},$$ where $ alpha$ is some parameter often called the &quot;learning rate&quot;. All this equation is saying is, &quot;If the function is increasing, then move to the left; and if the function is decreasing then move to the right.&quot; The actual change to $x$ is given by $ Delta x equiv - alpha (df/dx)$. | Repeat step 2 until some approximation criterion is met. | A nice feature of this method is that as $df/dx rightarrow 0$, so too $ Delta x rightarrow0$. So an &quot;adaptive stepsize&quot; is built-in. . Now let&#39;s try this out with some Python code... . from __future__ import print_function # for backwards-compatibility w/ Python2 import numpy as np, matplotlib.pyplot as plt def f(x): return (x-1)**2+1 def dfdx(x): return 2*(x-1) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;f&#39;}) x = np.arange(-5,7,0.1) ax.plot(x,f(x),ls=&#39;dashed&#39;) for alpha in ([0.002,0.1,0.25,0.8]): print(&quot;alpha = &quot;,alpha) x = -5 # starting point x_arr = [x] y_arr = [f(x)] maxiter = 50 for iter in range(maxiter): # do the descent # these two lines are just for plotting later x_arr.append(x) y_arr.append( f(x) ) # Here&#39;s the important part: update via gradient descent x = x - alpha * dfdx(x) # report and make the plot print(&quot; final x = &quot;,x) ax.plot(x_arr,y_arr,&#39;o-&#39;,label=&quot;alpha = &quot;+str(alpha)) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels) plt.show() . alpha = 0.002 final x = -3.910414704056598 alpha = 0.1 final x = 0.9999143651384377 alpha = 0.25 final x = 0.9999999999999947 alpha = 0.8 final x = 0.999999999951503 . Notice how the larger learning rate ($ alpha$=0.8) meant that the steps taken were so large that they &quot;overshot&quot; the minimum, whereas the too-small learning rate ($ alpha=0.002$) still hadn&#39;t come anywhere close to the minimum before the maximum iteration was reached. . Exercise: Experiment by editing the above code: Try different learning rates and observe the behavior. . Challenge: Instability . You may have noticed, if you made the learning rate too large, that the algorithm does not converge to the solution but instead &#39;blows up&#39;. This is the &#39;flip side&#39; of the &#39;adaptive step size&#39; feature of this algorithm: If you jump &quot;across&quot; the minimum to the other side and end up a greater distance from the minimum that where you started, you will encounter an even larger gradient, which will lead to an even larger $ Delta x$, and so on. . We can see this with the same code from before, let&#39;s just use a different starting point and a step size that&#39;s clearly too large... . from __future__ import print_function # for backwards-compatibility w/ Python2 import numpy as np, matplotlib.pyplot as plt def f(x): return (x-1)**2+1 def dfdx(x): return 2*(x-1) alpha = 1.1 # &quot;too big&quot; learning rate print(&quot;alpha = &quot;,alpha) x = -1 # starting point x_arr = [] y_arr = [] maxiter = 12 for iter in range(maxiter): # do the descent x_arr.append(x) y_arr.append( f(x) ) x = x - alpha * dfdx(x) # report and make the plot print(&quot; final x = &quot;,x) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;f&#39;}) plt.plot(x_arr,y_arr,&#39;r&#39;,zorder=2,) plt.scatter(x_arr,y_arr,zorder=3,c=range(len(x_arr)),cmap=plt.cm.viridis) xlim = ax.get_xlim() # find out axis limits x = np.arange(xlim[0],xlim[1],1) # dashed line plt.plot(x,f(x),zorder=1,ls=&#39;dashed&#39;) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels) plt.show() . alpha = 1.1 final x = -16.83220089651204 . In the above plot, we colored the points by iteration number, starting with the dark purple at the initial location of x=-1, and bouncing around ever-farther from the solution as the color changes to yellow. As this happens, the error is growing exponentially; this is one example of a numerical instability. Thus, this algorithm is not entirely stable. . . One way to guard against this to check: is our value of $f(x)$ at the current iteration larger than the value it was at the previous iteration? If so, that&#39;s a sign that our learning rate is too large, and we can use this criterion to dynamically adjust the learning rate. . Let&#39;s add some &#39;control&#39; code to that effect, to the previous script, and also print out the values of the relevant variables so we can track the progress: . from __future__ import print_function # for backwards-compatibility w/ Python2 import numpy as np, matplotlib.pyplot as plt def f(x): return (x-1)**2+1 def dfdx(x): return 2*(x-1) alpha = 13.0 # &quot;too big&quot; learning rate print(&quot;alpha = &quot;,alpha) x = -1 # starting point x_arr = [] y_arr = [] maxiter = 20 f_old = 1e99 # some big number for iter in range(maxiter): # do the descent # these two lines are just for plotting later x_arr.append(x) f_cur = f(x) y_arr.append( f_cur ) print(&quot;iter = &quot;,iter,&quot;x = &quot;,x,&quot;f(x) =&quot;,f(x),&quot;alpha = &quot;,alpha) if (f_cur &gt; f_old): # check for runaway behavior alpha = alpha * 0.5 print(&quot; decreasing alpha. new alpha = &quot;,alpha) f_old = f_cur # update via gradient descent x = x - alpha * dfdx(x) # report and make the plot print(&quot; final x = &quot;,x) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;f&#39;}) plt.plot(x_arr,y_arr,&#39;r&#39;,zorder=2,) plt.scatter(x_arr,y_arr,zorder=3,c=range(len(x_arr)),cmap=plt.cm.viridis) xlim = ax.get_xlim() x = np.arange(xlim[0],xlim[1],1) # x for dashed line plt.plot(x,f(x),zorder=1,ls=&#39;dashed&#39;) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels) plt.show() . alpha = 13.0 iter = 0 x = -1 f(x) = 5 alpha = 13.0 iter = 1 x = 51.0 f(x) = 2501.0 alpha = 13.0 decreasing alpha. new alpha = 6.5 iter = 2 x = -599.0 f(x) = 360001.0 alpha = 6.5 decreasing alpha. new alpha = 3.25 iter = 3 x = 3301.0 f(x) = 10890001.0 alpha = 3.25 decreasing alpha. new alpha = 1.625 iter = 4 x = -7424.0 f(x) = 55130626.0 alpha = 1.625 decreasing alpha. new alpha = 0.8125 iter = 5 x = 4641.625 f(x) = 21535401.390625 alpha = 0.8125 iter = 6 x = -2899.390625 f(x) = 8412266.77758789 alpha = 0.8125 iter = 7 x = 1813.744140625 f(x) = 3286042.31937027 alpha = 0.8125 iter = 8 x = -1131.965087890625 f(x) = 1283610.8903790116 alpha = 0.8125 iter = 9 x = 709.1031799316406 f(x) = 501411.1134293014 alpha = 0.8125 iter = 10 x = -441.5644874572754 f(x) = 195864.32555832085 alpha = 0.8125 iter = 11 x = 277.6028046607971 f(x) = 76510.1115462191 alpha = 0.8125 iter = 12 x = -171.8767529129982 f(x) = 29887.37169774183 alpha = 0.8125 iter = 13 x = 109.04797057062387 f(x) = 11675.363944430403 alpha = 0.8125 iter = 14 x = -66.52998160663992 f(x) = 4561.298415793126 alpha = 0.8125 iter = 15 x = 43.20623850414995 f(x) = 1782.36656866919 alpha = 0.8125 iter = 16 x = -25.37889906509372 f(x) = 696.8463158864023 alpha = 0.8125 iter = 17 x = 17.486811915683575 f(x) = 272.8149671431259 alpha = 0.8125 iter = 18 x = -9.304257447302234 f(x) = 107.17772154028356 alpha = 0.8125 iter = 19 x = 7.440160904563896 f(x) = 42.47567247667326 alpha = 0.8125 final x = -3.025100565352435 . So in the preceding example, we start at $x=-1$, then the unstable behavior starts and we begin diverging from the minimum, so we decrease $ alpha$ as often as our criterion tells us to. Finally $ alpha$ becomes low enought to get the system &#39;under control&#39; and the algorithm enters the convergent regime. . Exercise: In the example above, we only decrease $ alpha$ by a factor of 2 each time, but it would be more efficient to decrease by a factor of 10. Try that and observe the behavior of the system. . You may say, &quot;Why do I need to worry about this instability stuff? As long as $ alpha&lt;1$ the system will converge, right?&quot; Well, for this simple system it seems obvious what needs to happen, but with multidimensional optimization problems (see below), it&#39;s not always obvious what to do. (Sometimes different &#39;dimensions&#39; need different learning rates.) This simple example serves as an introduction to phenomena which arise in more complex situations. . Challenge: Non-global minima . To explore more complicated functions, we&#39;re going to take advantage of the SymPy package, to let it take derivatives for us. Try executing the import in the next cell, and if nothing happens it means you have SymPy installed. If you get an error, you may need to go into a Terminal and run &quot;pip install sympy&quot;. . import sympy . You&#39;re good? No errors? Ok, moving on... . from __future__ import print_function # for backwards-compatibility w/ Python2 import numpy as np, matplotlib.pyplot as plt from sympy import Symbol, diff x = Symbol(&#39;x&#39;) # our function, more complicated (SymPy handles it!) f = (x-1)**4 - 20*(x-1)**2 + 10*x + 1 dfdx = diff(f,x) # setup fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;f&#39;}) x_arr = np.arange(-5,7,0.1) y_arr = np.copy(x_arr) for i, val in enumerate(x_arr): y_arr[i] = f.evalf(subs={x:val}) ax.plot(x_arr,y_arr,ls=&#39;dashed&#39;) # space of &#39;error function&#39; # for a variety of learning rates... for alpha in ([0.002,0.01,0.03]): print(&quot;alpha = &quot;,alpha) xval = 6 # starting point x_arr = [xval] y_arr = [f.evalf(subs={x:xval})] maxiter = 50 # do the descent for iter in range(maxiter): # these two lines are just for plotting later x_arr.append(xval) y_arr.append( f.evalf(subs={x:xval}) ) # update via gradient descent xval = xval - alpha * dfdx.evalf(subs={x:xval}) print(&quot; final xval = &quot;,xval) ax.plot(x_arr,y_arr,&#39;o-&#39;,label=&quot;alpha = &quot;+str(alpha)) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels) plt.show() . alpha = 0.002 final xval = 4.02939564594151 alpha = 0.01 final xval = 4.02896613891181 alpha = 0.03 final xval = -2.00328879556504 . All the runs start at $x=6$. Notice how the runs marked in organge and green go on to find a &quot;local&quot; minimum, but they don&#39;t find the &quot;global&quot; minimum (the overall lowest point) like the run marked in red does. The problem of ending up at non-global local minima is a generic problem for all kinds of optimization tasks. It tends to get even worse when you add more parameters... . Multidimensional Gradient Descent . (A descent into darkness...) . Let&#39;s define a function of two variables, that&#39;s got at least one minimum in it. We&#39;ll choose $$f(x,y) = - left( cos x + 3 cos y right) /2,$$ which actually has infinitely many minima, but we&#39;ll try to &#39;zoom in&#39; on just one. . We can vizualize this function via the graph produced by the code below; in the graph, darker areas show lower values than ligher areas, and there is a minimum at the point $x=0,y=0$ where $f(0,0)=-2$. . import numpy as np, matplotlib.pyplot as plt def f(x,y): return -( np.cos(x) + 3*np.cos(y) )/2 x = y = np.linspace(-4, 4, 100) z = np.zeros([len(x), len(y)]) for i in range(len(x)): for j in range(len(y)): z[j, i] = f(x[i], y[j]) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;y&#39;}) cs = ax.pcolor(x, y, z, cmap=plt.cm.afmhot) plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;) cbar = fig.colorbar(cs, orientation=&#39;vertical&#39;) plt.show() . The way we find a minimum is similar to what we did before, except we use partial derivatives in the x- and y-directions: . $$x_{new} = x_{old} + Delta x, Delta x = - alpha { partial f over partial x} $$ $$y_{new} = y_{old} + Delta y, Delta y = - alpha { partial f over partial y},$$ . from __future__ import print_function # for backwards-compatibility w/ Python2 import numpy as np, matplotlib.pyplot as plt # our function def f(x,y): return -( np.cos(x) + 3*np.cos(y) )/2 def dfdx(x,y): return np.sin(x)/2 def dfdy(x,y): return 3*np.sin(y)/2 # variables for this run alpha = 0.5 xval, yval = 2.5, 1.5 # starting guess(es) x_arr = [] y_arr = [] maxiter = 20 for iter in range(maxiter): # gradient descent loop x_arr.append(xval) y_arr.append(yval) xval = xval - alpha * dfdx(xval,yval) yval = yval - alpha * dfdy(xval,yval) print(&quot;Final xval, yval = &quot;,xval,yval,&quot;. Target is (0,0)&quot;) # background image: plot the color background x = y = np.linspace(-4, 4, 100) z = np.zeros([len(x), len(y)]) for i in range(len(x)): for j in range(len(y)): z[j, i] = f(x[i], y[j]) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;y&#39;}) cs = ax.pcolor(x, y, z, cmap=plt.cm.afmhot) plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;) cbar = fig.colorbar(cs, orientation=&#39;vertical&#39;) # plot the progress of our optimization plt.plot(x_arr,y_arr,zorder=1) plt.scatter(x_arr,y_arr,zorder=2,c=range(len(x_arr)),cmap=plt.cm.viridis) handles, labels = ax.get_legend_handles_labels() plt.show() . Final xval, yval = 0.0272555602238 3.59400699273e-12 . Target is (0,0) . In the above figure, we&#39;ve shown the &#39;path&#39; the algorithm takes in $x$-$y$ space, coloring the dots according to iteration number, so that the first points are dark purple, and later points tend to yellow. . Note that due to the asymmetry in the function (between $x$ and $y$), the path descends rapidly in $y$, and then travels along the &quot;valley&quot; in $x$ to reach the minimum. This &quot;long narrow valley&quot; behavior is common in multidimensional optimization problems: the system may &#39;solve&#39; one parameter quickly, but require thousands of operations to find the other one. . Many sophisticated schemes have arisen to handle this challenge, and we won&#39;t cover them here. For now, suffice it to say that, yes, this sort of thing happens. You may have &#39;found&#39; highly accurate values for certain parameters, but others are bogging down the process of convergence. . Next time, we&#39;ll cover a common application of optimization: Least Squares Regression... . Least Squares Regression . This is such a common thing to do in science and statistics, that everyone should learn how it works. We&#39;ll do it for linear relationships, but it generalizes to nonlinear situations as well. . How to Fit a Line . Let&#39;s say we&#39;re trying to fit a line to a bunch of data. We&#39;ve been given $n$ data points with coordinates $(x_i,y_i)$ where $i=1..n$. The problem becomes, given a line $f(x) = mx+b$, find the values of the parameters $m$ and $b$ which minimize the overall &quot;error&quot;. . add some kinda picture here? . The error can take many forms; one is the squared error $SE$, which is just the sum of the squares of the &quot;distances&quot; between each data point&#39;s $y$-value and the &quot;guess&quot; from the line fit $f$ at each value of $x$: . $$ SE = (f(x_1) - y_1)^2 + (f(x_2) - y_2)^2 + ... (f(x_n)-y_n)^2,$$ . We can write this concisely as $$ SE = sum_{i=1}^n (f(x_i)-y_i)^2.$$ . Another popular form is the &quot;mean squared error&quot; $MSE$, which is just $SE/n$: . $$ MSE = {1 over n} sum_{i=1}^n (f(x_i)-y_i)^2.$$ . The MSE has the nice feature that as you add more data points, it tends to hold a more-or-less consistent value (as opposed to the SE which gets bigger as you add more points). We&#39;ll use the MSE in the work that follows. . So expanding out $f(x)$, we see that the MSE is a function of $m$ and $b$, and these are the parameters we&#39;ll vary to minimize the MSE: $$ MSE(m,b) = {1 over n} sum_{i=1}^n (mx_i+b-y_i)^2.$$ . So, following our earlier work on multidimensional optimization, we start with guesses for $m$ and $b$ and then update according to gradient descent: . $$m_{new} = m_{old} + Delta m, Delta m = - alpha{ partial (MSE) over partial m} = - alpha{2 over n} sum_{i=1}^n (mx_i+b-y_i)(x_i) $$ $$b_{new} = b_{old} + Delta b, Delta b = - alpha{ partial (MSE) over partial b} = - alpha{2 over n} sum_{i=1}^n (mx_i+b-y_i)(1).$$ . So, to start off, let&#39;s get some data... . n = 20 np.random.seed(1) # for reproducability x_data = np.random.uniform(size=n) # random points for x m_exact = 2.0 b_exact = 1.5 y_data = m_exact * x_data + b_exact y_data += 0.3*np.random.normal(size=n) # add noise # Plot the data def plot_data(x_data, y_data, axis_labels=(&#39;x&#39;,&#39;y&#39;), zero_y=False): fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:axis_labels[0], &#39;ylabel&#39;:axis_labels[1]}) ax.plot(x_data, y_data,&#39;o&#39;) if (zero_y): ax.set_ylim([0,ax.get_ylim()[1]*1.1]) plt.show() plot_data(x_data,y_data, zero_y=True) . Note: in contrast to earlier parts of this document which include complete python programs in every code post, for brevity&#39;s sake we will start using the notebook &quot;as intended&quot;, relying on the internal state and adding successive bits of code which make use of the &quot;memory&quot; of previously-defined variables. . Let&#39;s map out the MSE for this group of points, as a function of possible $m$ and $b$ values... . def MSE(x,y,m,b): # Use Python array operations to compute sums return ((m*x + b - y)**2).mean() mm = bb = np.linspace(0, 4, 50) z = np.zeros([len(mm), len(bb)]) for i in range(len(mm)): for j in range(len(bb)): z[j, i] = MSE(x_data,y_data, mm[i],bb[j]) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;m&#39;, &#39;ylabel&#39;:&#39;b&#39;}) cs = ax.pcolor(mm, bb, np.log(z), cmap=plt.cm.afmhot) plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;) plt.show() . We see the minimum near the &quot;exact&quot; values chosen in the begininng. (Note that we&#39;ve plotted the logarithm of the MSE just to make the colors stand out better.) . Next we will choose starting guesses for $m$ and $b$, and use gradient descent to fit the line... . m = 3.5 # initial guess b = 3.5 m_arr = [] b_arr = [] def dMSEdm(x,y,m,b): return (2*(m*x + b - y) *x).mean() def dMSEdb(x,y,m,b): return (2*(m*x + b - y)).mean() alpha = 0.1 maxiter, printevery = 500, 4 for iter in range(maxiter): m_arr.append(m) b_arr.append(b) if (0 == iter % printevery): print(iter,&quot;: b, m = &quot;,b,m,&quot;, MSE = &quot;,MSE(x_data,y_data,m,b)) m = m - alpha * dMSEdm(x_data,y_data,m,b) b = b - alpha * dMSEdb(x_data,y_data,m,b) print(&quot;Final result: m = &quot;,m,&quot;, b = &quot;,b) # background image: plot the color background (remembered from before) fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;m&#39;, &#39;ylabel&#39;:&#39;b&#39;}) cs = ax.pcolor(mm, bb, np.log(z), cmap=plt.cm.afmhot) plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;) # plot the progress of our descent plt.plot(m_arr,b_arr,zorder=1) plt.scatter(m_arr,b_arr,zorder=2,c=range(len(m_arr)),cmap=plt.cm.viridis) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels) plt.show() . 0 : b, m = 3.5 3.5 , MSE = 6.86780331186 4 : b, m = 2.07377614457 2.89890882764 , MSE = 0.98222306593 8 : b, m = 1.55966423863 2.66310750082 , MSE = 0.194874325956 12 : b, m = 1.37811928553 2.56128194633 , MSE = 0.0877947061277 16 : b, m = 1.31767685375 2.50899769214 , MSE = 0.0718728682069 20 : b, m = 1.30118421505 2.47541762467 , MSE = 0.0683627241086 24 : b, m = 1.30049838878 2.44926336309 , MSE = 0.0666665839706 28 : b, m = 1.30535957938 2.4263945595 , MSE = 0.0653286635918 32 : b, m = 1.3120331485 2.40527655618 , MSE = 0.0641346050201 36 : b, m = 1.31916510087 2.38532649645 , MSE = 0.0630441987699 40 : b, m = 1.32626977167 2.36630978168 , MSE = 0.0620438837066 44 : b, m = 1.33317800067 2.34811979986 , MSE = 0.0611251920416 48 : b, m = 1.33983578596 2.33069751134 , MSE = 0.0602811821983 52 : b, m = 1.34623082683 2.31400206963 , MSE = 0.059505695069 56 : b, m = 1.35236573256 2.29800006645 , MSE = 0.0587931380437 60 : b, m = 1.35824825911 2.28266157418 , MSE = 0.0581383942813 64 : b, m = 1.36388775858 2.26795866993 , MSE = 0.0575367695918 68 : b, m = 1.3692938954 2.2538648666 , MSE = 0.0569839532221 72 : b, m = 1.374476189 2.2403548761 , MSE = 0.0564759850457 76 : b, m = 1.37944385765 2.22740449497 , MSE = 0.0560092265247 80 : b, m = 1.3842057718 2.21499053585 , MSE = 0.0555803344137 84 : b, m = 1.38877044688 2.20309077677 , MSE = 0.0551862367311 88 : b, m = 1.39314605011 2.19168391801 , MSE = 0.0548241107275 92 : b, m = 1.39734041211 2.18074954278 , MSE = 0.0544913626571 96 : b, m = 1.4013610397 2.17026808021 , MSE = 0.054185609197 100 : b, m = 1.40521512901 2.16022077017 , MSE = 0.0539046603748 104 : b, m = 1.40890957817 2.1505896296 , MSE = 0.0536465038826 108 : b, m = 1.41245099963 2.14135742035 , MSE = 0.0534092906637 112 : b, m = 1.41584573193 2.1325076183 , MSE = 0.0531913216685 116 : b, m = 1.41909985108 2.12402438375 , MSE = 0.0529910356848 120 : b, m = 1.42221918141 2.11589253313 , MSE = 0.0528069981559 124 : b, m = 1.42520930601 2.10809751176 , MSE = 0.0526378909052 128 : b, m = 1.42807557671 2.10062536785 , MSE = 0.052482502695 132 : b, m = 1.43082312365 2.09346272749 , MSE = 0.0523397205507 136 : b, m = 1.4334568645 2.08659677074 , MSE = 0.0522085217891 140 : b, m = 1.43598151321 2.08001520867 , MSE = 0.0520879666937 144 : b, m = 1.43840158849 2.07370626138 , MSE = 0.0519771917836 148 : b, m = 1.44072142187 2.0676586369 , MSE = 0.0518754036286 152 : b, m = 1.44294516546 2.06186151097 , MSE = 0.051781873167 156 : b, m = 1.44507679941 2.0563045077 , MSE = 0.0516959304829 160 : b, m = 1.44712013898 2.05097768097 , MSE = 0.0516169600082 164 : b, m = 1.44907884142 2.04587149665 , MSE = 0.0515443961135 168 : b, m = 1.45095641248 2.04097681551 , MSE = 0.0514777190569 172 : b, m = 1.45275621269 2.03628487687 , MSE = 0.0514164512613 176 : b, m = 1.45448146341 2.03178728296 , MSE = 0.0513601538933 180 : b, m = 1.45613525255 2.0274759838 , MSE = 0.0513084237208 184 : b, m = 1.45772054011 2.0233432629 , MSE = 0.0512608902243 188 : b, m = 1.4592401635 2.01938172337 , MSE = 0.051217212943 192 : b, m = 1.4606968426 2.0155842747 , MSE = 0.0511770790366 196 : b, m = 1.46209318462 2.01194412009 , MSE = 0.0511402010444 200 : b, m = 1.46343168877 2.00845474426 , MSE = 0.0511063148261 204 : b, m = 1.46471475077 2.00510990182 , MSE = 0.0510751776703 208 : b, m = 1.46594466707 2.00190360604 , MSE = 0.0510465665558 212 : b, m = 1.46712363904 1.99883011819 , MSE = 0.0510202765543 216 : b, m = 1.46825377682 1.99588393722 , MSE = 0.0509961193626 220 : b, m = 1.46933710318 1.99305978998 , MSE = 0.0509739219537 224 : b, m = 1.4703755571 1.99035262169 , MSE = 0.0509535253378 228 : b, m = 1.47137099724 1.98775758697 , MSE = 0.0509347834233 232 : b, m = 1.47232520527 1.98527004115 , MSE = 0.0509175619703 236 : b, m = 1.47323988906 1.98288553192 , MSE = 0.0509017376297 240 : b, m = 1.47411668575 1.98059979141 , MSE = 0.0508871970587 244 : b, m = 1.47495716466 1.97840872852 , MSE = 0.0508738361101 248 : b, m = 1.4757628301 1.97630842161 , MSE = 0.0508615590853 252 : b, m = 1.47653512409 1.97429511148 , MSE = 0.0508502780498 256 : b, m = 1.47727542891 1.97236519463 , MSE = 0.0508399122026 260 : b, m = 1.47798506956 1.97051521684 , MSE = 0.050830387298 264 : b, m = 1.47866531621 1.96874186695 , MSE = 0.0508216351133 268 : b, m = 1.47931738637 1.96704197096 , MSE = 0.0508135929607 272 : b, m = 1.47994244714 1.96541248634 , MSE = 0.050806203238 276 : b, m = 1.48054161728 1.96385049656 , MSE = 0.0507994130159 280 : b, m = 1.4811159692 1.96235320595 , MSE = 0.0507931736592 284 : b, m = 1.4816665309 1.96091793458 , MSE = 0.0507874404782 288 : b, m = 1.4821942878 1.95954211357 , MSE = 0.0507821724088 292 : b, m = 1.48270018448 1.95822328041 , MSE = 0.0507773317183 296 : b, m = 1.48318512642 1.95695907462 , MSE = 0.0507728837349 300 : b, m = 1.4836499816 1.95574723348 , MSE = 0.0507687965998 304 : b, m = 1.48409558201 1.954585588 , MSE = 0.0507650410387 308 : b, m = 1.48452272522 1.95347205901 , MSE = 0.0507615901523 312 : b, m = 1.48493217573 1.95240465348 , MSE = 0.0507584192235 316 : b, m = 1.4853246664 1.95138146095 , MSE = 0.0507555055402 320 : b, m = 1.48570089973 1.95040065006 , MSE = 0.0507528282332 324 : b, m = 1.48606154909 1.94946046531 , MSE = 0.0507503681262 328 : b, m = 1.48640726001 1.94855922395 , MSE = 0.0507481075984 332 : b, m = 1.48673865124 1.94769531289 , MSE = 0.0507460304589 336 : b, m = 1.48705631592 1.94686718587 , MSE = 0.0507441218299 340 : b, m = 1.48736082261 1.9460733607 , MSE = 0.0507423680409 344 : b, m = 1.48765271634 1.94531241654 , MSE = 0.0507407565302 348 : b, m = 1.48793251954 1.94458299143 , MSE = 0.0507392757554 352 : b, m = 1.48820073302 1.94388377984 , MSE = 0.0507379151103 356 : b, m = 1.48845783683 1.94321353027 , MSE = 0.0507366648493 360 : b, m = 1.48870429115 1.9425710431 , MSE = 0.0507355160173 364 : b, m = 1.48894053709 1.94195516839 , MSE = 0.0507344603858 368 : b, m = 1.48916699749 1.9413648038 , MSE = 0.0507334903937 372 : b, m = 1.48938407768 1.94079889271 , MSE = 0.0507325990934 376 : b, m = 1.48959216619 1.9402564222 , MSE = 0.050731780101 380 : b, m = 1.48979163547 1.93973642136 , MSE = 0.0507310275504 384 : b, m = 1.48998284254 1.93923795946 , MSE = 0.0507303360514 388 : b, m = 1.49016612963 1.93876014435 , MSE = 0.0507297006512 392 : b, m = 1.49034182478 1.93830212081 , MSE = 0.0507291167986 396 : b, m = 1.49051024246 1.93786306905 , MSE = 0.0507285803118 400 : b, m = 1.49067168412 1.93744220325 , MSE = 0.0507280873482 404 : b, m = 1.49082643871 1.93703877012 , MSE = 0.0507276343768 408 : b, m = 1.49097478321 1.9366520476 , MSE = 0.0507272181534 412 : b, m = 1.49111698313 1.9362813435 , MSE = 0.0507268356966 416 : b, m = 1.491253293 1.93592599432 , MSE = 0.0507264842671 420 : b, m = 1.49138395677 1.93558536407 , MSE = 0.0507261613477 424 : b, m = 1.49150920832 1.93525884305 , MSE = 0.0507258646256 428 : b, m = 1.49162927183 1.93494584685 , MSE = 0.0507255919755 432 : b, m = 1.4917443622 1.93464581526 , MSE = 0.0507253414444 436 : b, m = 1.4918546854 1.93435821128 , MSE = 0.050725111238 440 : b, m = 1.49196043891 1.93408252014 , MSE = 0.0507248997074 444 : b, m = 1.49206181201 1.93381824839 , MSE = 0.0507247053375 448 : b, m = 1.49215898613 1.93356492304 , MSE = 0.0507245267361 452 : b, m = 1.4922521352 1.93332209068 , MSE = 0.0507243626239 456 : b, m = 1.49234142595 1.93308931667 , MSE = 0.0507242118256 460 : b, m = 1.49242701819 1.93286618439 , MSE = 0.0507240732609 464 : b, m = 1.49250906511 1.93265229447 , MSE = 0.0507239459375 468 : b, m = 1.49258771357 1.93244726408 , MSE = 0.0507238289434 472 : b, m = 1.49266310433 1.93225072625 , MSE = 0.0507237214405 476 : b, m = 1.49273537233 1.93206232921 , MSE = 0.050723622659 480 : b, m = 1.49280464692 1.93188173576 , MSE = 0.0507235318913 484 : b, m = 1.49287105209 1.93170862267 , MSE = 0.0507234484872 488 : b, m = 1.49293470669 1.9315426801 , MSE = 0.0507233718493 492 : b, m = 1.49299572466 1.93138361102 , MSE = 0.0507233014288 496 : b, m = 1.4930542152 1.93123113075 , MSE = 0.0507232367213 Final result: m = 1.93108496636 , b = 1.49311028301 . Note that the optimized values $(m,b)$ that we find may not exactly match the &quot;exact&quot; values we used to make the data, because the noise we added to the data can throw this off. In the limit where the noise amplitude goes to zero, our optimized values will exactly match the &quot;exact&quot; values used to generated the data. . Let&#39;s see the results of our line fit... . # plot the points fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:&#39;x&#39;, &#39;ylabel&#39;:&#39;y&#39;}) ax.plot(x_data,y_data,&#39;o&#39;) ax.set_ylim([0,ax.get_ylim()[1]*1.1]) # and plot the line we fit xlim = ax.get_xlim() x_line = np.linspace(xlim[0],xlim[1],2) y_line = m*x_line + b ax.plot(x_line,y_line) plt.show() . Great! . Least Squares Fitting with Nonlinear Functions . We can generalize the technique describe above to fit polynomials $$ f(x) = c_0 + c_1 x + c_2 x^2 + ...c_k x^k,$$ where $c_0...c_k$ are the parameters we will tune, and $k$ is the order of the polynomial. (Typically people use the letter $a$ for polynomial coefficients, but in the math rendering of Jupter, $ alpha$ and $a$ look too much alike, so we&#39;ll use $c$.) Written more succinctly, $$ f(x) = sum_{j=0}^k c_j x^j.$$ . (Indeed, we could even try non-polynomial basis functions, e.g., $ f(x) = c_0 + c_1 g(x) + c_2 h(x) + ...,$ but let&#39;s stick to polynomials for now.) . The key thing to note is that for each parameter $c_j$, the update $ Delta c_j$ will be . $$ Delta c_j = - alpha { partial (MSE) over partial c_j} = - alpha { partial (MSE) over partial f}{ partial f over partial c_j}$$ $$= - alpha {2 over n} sum_{i=1}^n [f(x_i)-y_i](x_i)^{j} $$ . (Note that we are not taking the derivative with respect to $x_i$, but rather with respect to $c_j$. Thus there is no &quot;power rule&quot; that needs be applied to this derivative. Also there is no sum over j.) . The following is a complete code for doing this, along with some added refinements: . $ alpha$ is now $ alpha_j$, i.e. different learning rates for different directions | we initialise $ alpha_j$ such that larger powers of $x$ start with smaller coefficients | we put the fitting code inside a method (with a bunch of parameters) so we can call it later | . from __future__ import print_function # for backwards-compatibility w/ Python2 import numpy as np, matplotlib.pyplot as plt def f(x,c): y = 0*x # f will work on single floats or arrays for j in range(c.size): y += c[j]*(x**j) return y def polyfit(x_data,y_data, c_start=None, order=None, maxiter=500, printevery = 25, alpha_start=0.9, alpha_start_power=0.3): # function definitions def MSE(x_arr,y_arr,c): f_arr = f(x_arr,c) return ((f_arr - y_arr)**2).mean() def dMSEdcj(x_arr,y_arr,c,j): # deriviative of MSE wrt cj (*not* wrt x!) f_arr = f(x_arr,c) return ( 2* ( f_arr - y_arr) * x_arr**j ).mean() if ((c_start is None) and (order is None)): print(&quot;Error: Either specify initial guesses for coefficients,&quot;, &quot;or specify the order of the polynomial&quot;) raise # halt if c_start is not None: order = c_start.size-1 c = np.copy(c_start) elif order is not None: c = np.random.uniform(size=order+1) # random guess for starting point assert(c.size == order+1) # check against conflicting info k = order print(&quot; Initial guess: c = &quot; ,np.array_str(c, precision=2)) alpha = np.ones(c.size) for j in range(c.size): # start with smaller alphas for higher powers of x alpha[j] = alpha_start*(alpha_start_power)**(j) MSE_old = 1e99 for iter in range(maxiter+1): # do the descent for j in range(c.size): c[j] = c[j] - alpha[j] * dMSEdcj(x_data,y_data,c,j) MSE_cur = MSE(x_data,y_data,c) if (MSE_cur &gt; MSE_old): # adjust if runaway behavior starts alpha[j] *= 0.3 print(&quot; Notice: decreasing alpha[&quot;,j,&quot;] to &quot;,alpha[j]) MSE_old = MSE_cur if (0 == iter % printevery): # progress log print(&#39;{:4d}&#39;.format(iter),&quot;/&quot;,maxiter,&quot;: MSE =&quot;,&#39;{:9.6g}&#39;.format(MSE_cur), &quot;, c = &quot;,np.array_str(c, precision=3),sep=&#39;&#39;) print(&quot;&quot;) return c # Set up input data n = 100 np.random.seed(2) # for reproducability x_data = np.random.uniform(-2.5,3,size=n) # some random points for x c_data = np.array([-4,-3,5,.5,-2,.5]) # params to generate data (5th-degree polynomial) y_data = f(x_data, c_data) y_data += 0.02*np.random.normal(size=n)*y_data # add a (tiny) bit of noise #- Perform Least Squares Fit c = polyfit(x_data, y_data, c_start=c_data*np.random.random(), maxiter=500) #-- Plot the results def plot_data_and_curve(x_data,y_data,axis_labels=(&#39;x&#39;,&#39;y&#39;), ): # plot the points fig, ax = plt.subplots() ax.update({&#39;xlabel&#39;:axis_labels[0], &#39;ylabel&#39;:axis_labels[1]}) ax.plot(x_data,y_data,&#39;o&#39;) # and plot the curve we fit xlim = ax.get_xlim() x_line = np.linspace(xlim[0],xlim[1],100) y_line = f(x_line, c) ax.plot(x_line,y_line) plt.show() plot_data_and_curve(x_data,y_data) . Initial guess: c = [-3.52 -2.64 4.4 0.44 -1.76 0.44] Notice: decreasing alpha[ 3 ] to 0.00729 Notice: decreasing alpha[ 4 ] to 0.002187 Notice: decreasing alpha[ 5 ] to 0.0006561 0/500: MSE = 258.233, c = [-5.438 -1.633 4.24 0.555 -1.904 0.765] Notice: decreasing alpha[ 5 ] to 0.00019683 25/500: MSE = 0.529541, c = [-4.265 -1.545 5.668 -0.392 -2.146 0.612] 50/500: MSE = 0.424417, c = [-4.304 -1.808 5.659 -0.241 -2.137 0.595] 75/500: MSE = 0.335586, c = [-4.256 -2.034 5.552 -0.105 -2.115 0.578] 100/500: MSE = 0.275848, c = [-4.212 -2.218 5.457 0.006 -2.096 0.564] 125/500: MSE = 0.236521, c = [-4.175 -2.367 5.38 0.096 -2.08 0.553] 150/500: MSE = 0.21068, c = [-4.146 -2.488 5.317 0.17 -2.068 0.544] 175/500: MSE = 0.193702, c = [-4.122 -2.586 5.267 0.229 -2.058 0.537] 200/500: MSE = 0.182549, c = [-4.103 -2.665 5.226 0.277 -2.049 0.531] 225/500: MSE = 0.175222, c = [-4.087 -2.73 5.192 0.316 -2.042 0.526] 250/500: MSE = 0.170408, c = [-4.075 -2.782 5.165 0.347 -2.037 0.522] 275/500: MSE = 0.167245, c = [-4.064 -2.824 5.143 0.373 -2.033 0.519] 300/500: MSE = 0.165167, c = [-4.056 -2.859 5.126 0.393 -2.029 0.516] 325/500: MSE = 0.163802, c = [-4.049 -2.886 5.111 0.41 -2.026 0.514] 350/500: MSE = 0.162905, c = [-4.044 -2.909 5.1 0.424 -2.024 0.513] 375/500: MSE = 0.162316, c = [-4.039 -2.927 5.09 0.435 -2.022 0.511] 400/500: MSE = 0.161929, c = [-4.036 -2.942 5.083 0.444 -2.02 0.51 ] 425/500: MSE = 0.161675, c = [-4.033 -2.954 5.076 0.451 -2.019 0.509] 450/500: MSE = 0.161508, c = [-4.031 -2.964 5.071 0.457 -2.018 0.508] 475/500: MSE = 0.161398, c = [-4.029 -2.972 5.067 0.462 -2.017 0.508] 500/500: MSE = 0.161326, c = [-4.027 -2.978 5.064 0.465 -2.017 0.507] . Now, it turns out that polynomials are often terrible things to try to fit arbitrary data with, because they can &#39;blow up&#39; as $|x|$ increases, and this causes instability. But for a variety of physics problems (see below), polynomials can be just what we&#39;re after. Plus, that made a nice demonstration, for now. . (For more general functions, I actually wrote a multi-parameter SymPy gradient-descient that is completely general, but it&#39;s terrifically slow so I won&#39;t be posting it here. If you really want it, contact me.) . Learning Gravity . Ok. Now we&#39;re all we&#39;re going to do next is fit a parabola to the motion of a falling ball -- and that&#39;s supposed to tell us something deep about physics. Sounds silly, right? &#39;Everybody&#39; knows objects moving in a gravitational field follow parabolas (both in space &amp; time); the more math-savvy may complain that we&#39;re simply going to &#39;get out of this&#39; what we &#39;put into it.&#39; . Well, from a philosophical standpoint and from the way that these methods will generalize to other situations, there are significant implications from the methodology we&#39;re about to follow. . The Challenge: Given a set of one-dimensional data of position vs. time $y(t)$, can we find the underlying equation that gives rise to it? Better put, can we fit a model to it, and how well can we fit it, and what kind of model will it be anyway? . (This is the sort of thing that statisticians do, but it&#39;s also something physicists do, and one could argue, this is what everybody does all the time. ) . Let&#39;s get started. I&#39;m just going to specify y(t) at a series of $n+1$ time steps $t_i$ ($t_0$...$t_n$) and we&#39;ll make them evenly spaced, and we&#39;ll leave out any noise at all -- perfect data. :-) . g_exact = 9.8 # a physical parater we&#39;ll find a fit for dt = 0.01 tmax = 1 # number of time steps t_data = np.arange(0,tmax,step=dt) # time values nt = t_data.size print(&quot;dt = &quot;,dt,&quot;, nt = &quot;,nt) y0 = 1.234 # initial position, choose anything v0 = 3.1415 # initial velocity #assign the data y_data = y0 + v0*t_data - 0.5 * g_exact * t_data**2 # y_data *= np.random.uniform(low=.9, high=1.1, size=(y_data.size)) # for later; add noise in plot_data(t_data,y_data, axis_labels=(&#39;t&#39;,&#39;y&#39;)) . dt = 0.01 , nt = 100 . Can we fit this with a polynomial? Sure, let&#39;s do that, using the code from before... . c = polyfit(t_data, y_data, order=2, alpha_start = 10.0, maxiter=1000, printevery=100) print(&quot;Our fit: y(t) = &quot;,c[0],&quot; + &quot;,c[1],&quot;*t + &quot;,c[2],&quot;*t**2&quot;,sep=&#39;&#39;) print(&quot;Compare to exact: y(t) = &quot;,y0, &quot; + &quot;,v0, &quot;*t - &quot;,0.5*g_exact,&quot;*t**2&quot;,sep=&#39;&#39;) print(&quot;Estimate for g = &quot;,-2*c[2]) plot_data_and_curve(t_data,y_data, axis_labels=(&#39;t&#39;,&#39;y&#39;)) . Initial guess: c = [ 0.72 0.71 0.77] 0/1000: MSE = 5.41899, c = [-2.186 7.319 -1.042] Notice: decreasing alpha[ 0 ] to 3.0 Notice: decreasing alpha[ 0 ] to 0.9 100/1000: MSE =0.0314071, c = [ 1.642 0.749 -2.528] 200/1000: MSE =0.00280409, c = [ 1.356 2.427 -4.191] 300/1000: MSE =0.000250355, c = [ 1.27 2.928 -4.688] 400/1000: MSE =2.23522e-05, c = [ 1.245 3.078 -4.837] 500/1000: MSE =1.99565e-06, c = [ 1.237 3.122 -4.881] 600/1000: MSE =1.78176e-07, c = [ 1.235 3.136 -4.894] 700/1000: MSE =1.59079e-08, c = [ 1.234 3.14 -4.898] 800/1000: MSE =1.42029e-09, c = [ 1.234 3.141 -4.899] 900/1000: MSE =1.26806e-10, c = [ 1.234 3.141 -4.9 ] 1000/1000: MSE =1.13215e-11, c = [ 1.234 3.141 -4.9 ] Our fit: y(t) = 1.23400775143 + 3.14145457517*t + -4.89995497009*t**2 Compare to exact: y(t) = 1.234 + 3.1415*t - 4.9*t**2 Estimate for g = 9.79990994018 . What if we try fitting higher-order terms? Are their coefficients negligible? The system may converge, but it will take a lot more iterations... (be prepared to wait!) . c = polyfit(t_data, y_data, order=3, alpha_start = 1.0, maxiter=700000, printevery=10000) print(&quot;Our fit: y(t) = &quot;,c[0],&quot; + &quot;,c[1],&quot;*t + &quot;,c[2],&quot;*t**2 + &quot;,c[3],&quot;*t**3&quot;,sep=&#39;&#39;) print(&quot;Compare to exact: y(t) = &quot;,y0, &quot; + &quot;,v0, &quot;*t - &quot;,0.5*g_exact,&quot;*t**2&quot;,sep=&#39;&#39;) print(&quot;Estimate for g = &quot;,-2*c[2]) . Initial guess: c = [ 0.33 0.23 0.63 0.41] 0/700000: MSE = 0.828106, c = [ 1.189 -0.045 0.563 0.398] Notice: decreasing alpha[ 0 ] to 0.3 10000/700000: MSE =0.000464818, c = [ 1.291 2.454 -3.188 -1.138] 20000/700000: MSE =0.000369748, c = [ 1.285 2.528 -3.373 -1.015] 30000/700000: MSE =0.000294122, c = [ 1.279 2.594 -3.538 -0.906] 40000/700000: MSE =0.000233965, c = [ 1.275 2.654 -3.685 -0.808] 50000/700000: MSE =0.000186111, c = [ 1.27 2.706 -3.817 -0.72 ] 60000/700000: MSE =0.000148045, c = [ 1.266 2.753 -3.934 -0.642] 70000/700000: MSE =0.000117765, c = [ 1.263 2.795 -4.038 -0.573] 80000/700000: MSE =9.36783e-05, c = [ 1.26 2.833 -4.131 -0.511] 90000/700000: MSE =7.4518e-05, c = [ 1.257 2.866 -4.214 -0.456] 100000/700000: MSE =5.92766e-05, c = [ 1.254 2.896 -4.289 -0.407] 110000/700000: MSE =4.71526e-05, c = [ 1.252 2.922 -4.355 -0.363] 120000/700000: MSE =3.75083e-05, c = [ 1.25 2.946 -4.414 -0.323] 130000/700000: MSE =2.98366e-05, c = [ 1.248 2.967 -4.466 -0.288] 140000/700000: MSE =2.37341e-05, c = [ 1.247 2.986 -4.513 -0.257] 150000/700000: MSE =1.88797e-05, c = [ 1.246 3.003 -4.555 -0.229] 160000/700000: MSE =1.50182e-05, c = [ 1.244 3.018 -4.592 -0.205] 170000/700000: MSE =1.19465e-05, c = [ 1.243 3.031 -4.626 -0.183] 180000/700000: MSE =9.50301e-06, c = [ 1.242 3.043 -4.655 -0.163] 190000/700000: MSE =7.55933e-06, c = [ 1.241 3.054 -4.682 -0.145] 200000/700000: MSE =6.0132e-06, c = [ 1.24 3.063 -4.705 -0.129] 210000/700000: MSE =4.7833e-06, c = [ 1.24 3.072 -4.726 -0.115] 220000/700000: MSE =3.80496e-06, c = [ 1.239 3.079 -4.745 -0.103] 230000/700000: MSE =3.02672e-06, c = [ 1.239 3.086 -4.762 -0.092] 240000/700000: MSE =2.40766e-06, c = [ 1.238 3.092 -4.777 -0.082] 250000/700000: MSE =1.91521e-06, c = [ 1.238 3.097 -4.79 -0.073] 260000/700000: MSE =1.52349e-06, c = [ 1.237 3.102 -4.802 -0.065] 270000/700000: MSE =1.21188e-06, c = [ 1.237 3.106 -4.813 -0.058] 280000/700000: MSE =9.64014e-07, c = [ 1.237 3.11 -4.822 -0.052] 290000/700000: MSE =7.66841e-07, c = [ 1.236 3.114 -4.83 -0.046] 300000/700000: MSE =6.09997e-07, c = [ 1.236 3.117 -4.838 -0.041] 310000/700000: MSE =4.85233e-07, c = [ 1.236 3.119 -4.845 -0.037] 320000/700000: MSE =3.85987e-07, c = [ 1.236 3.122 -4.851 -0.033] 330000/700000: MSE =3.0704e-07, c = [ 1.235 3.124 -4.856 -0.029] 340000/700000: MSE =2.4424e-07, c = [ 1.235 3.126 -4.861 -0.026] 350000/700000: MSE =1.94285e-07, c = [ 1.235 3.127 -4.865 -0.023] 360000/700000: MSE =1.54547e-07, c = [ 1.235 3.129 -4.869 -0.021] 370000/700000: MSE =1.22937e-07, c = [ 1.235 3.13 -4.872 -0.019] 380000/700000: MSE =9.77925e-08, c = [ 1.235 3.132 -4.875 -0.017] 390000/700000: MSE =7.77907e-08, c = [ 1.235 3.133 -4.878 -0.015] 400000/700000: MSE =6.188e-08, c = [ 1.235 3.134 -4.88 -0.013] 410000/700000: MSE =4.92235e-08, c = [ 1.235 3.134 -4.882 -0.012] 420000/700000: MSE =3.91556e-08, c = [ 1.235 3.135 -4.884 -0.01 ] 430000/700000: MSE =3.1147e-08, c = [ 1.234 3.136 -4.886 -0.009] 440000/700000: MSE =2.47764e-08, c = [ 1.234 3.136 -4.887 -0.008] 450000/700000: MSE =1.97088e-08, c = [ 1.234 3.137 -4.889 -0.007] 460000/700000: MSE =1.56777e-08, c = [ 1.234 3.138 -4.89 -0.007] 470000/700000: MSE =1.24711e-08, c = [ 1.234 3.138 -4.891 -0.006] 480000/700000: MSE =9.92037e-09, c = [ 1.234 3.138 -4.892 -0.005] 490000/700000: MSE =7.89133e-09, c = [ 1.234e+00 3.139e+00 -4.893e+00 -4.691e-03] 500000/700000: MSE =6.27729e-09, c = [ 1.234e+00 3.139e+00 -4.894e+00 -4.184e-03] 510000/700000: MSE =4.99338e-09, c = [ 1.234e+00 3.139e+00 -4.894e+00 -3.731e-03] 520000/700000: MSE =3.97207e-09, c = [ 1.234e+00 3.139e+00 -4.895e+00 -3.328e-03] 530000/700000: MSE =3.15965e-09, c = [ 1.234e+00 3.140e+00 -4.896e+00 -2.968e-03] 540000/700000: MSE =2.5134e-09, c = [ 1.234e+00 3.140e+00 -4.896e+00 -2.647e-03] 550000/700000: MSE =1.99932e-09, c = [ 1.234e+00 3.140e+00 -4.896e+00 -2.361e-03] 560000/700000: MSE =1.5904e-09, c = [ 1.234e+00 3.140e+00 -4.897e+00 -2.106e-03] 570000/700000: MSE =1.26511e-09, c = [ 1.234e+00 3.140e+00 -4.897e+00 -1.878e-03] 580000/700000: MSE =1.00635e-09, c = [ 1.234e+00 3.140e+00 -4.897e+00 -1.675e-03] 590000/700000: MSE =8.0052e-10, c = [ 1.234e+00 3.141e+00 -4.898e+00 -1.494e-03] 600000/700000: MSE =6.36787e-10, c = [ 1.234e+00 3.141e+00 -4.898e+00 -1.332e-03] 610000/700000: MSE =5.06543e-10, c = [ 1.234e+00 3.141e+00 -4.898e+00 -1.188e-03] 620000/700000: MSE =4.02939e-10, c = [ 1.234e+00 3.141e+00 -4.898e+00 -1.060e-03] 630000/700000: MSE =3.20524e-10, c = [ 1.234e+00 3.141e+00 -4.899e+00 -9.454e-04] 640000/700000: MSE =2.54967e-10, c = [ 1.234e+00 3.141e+00 -4.899e+00 -8.432e-04] 650000/700000: MSE =2.02818e-10, c = [ 1.234e+00 3.141e+00 -4.899e+00 -7.520e-04] 660000/700000: MSE =1.61335e-10, c = [ 1.234e+00 3.141e+00 -4.899e+00 -6.707e-04] 670000/700000: MSE =1.28336e-10, c = [ 1.234e+00 3.141e+00 -4.899e+00 -5.982e-04] 680000/700000: MSE =1.02087e-10, c = [ 1.234e+00 3.141e+00 -4.899e+00 -5.335e-04] 690000/700000: MSE =8.12072e-11, c = [ 1.234e+00 3.141e+00 -4.899e+00 -4.758e-04] 700000/700000: MSE =6.45976e-11, c = [ 1.234e+00 3.141e+00 -4.899e+00 -4.244e-04] Our fit: y(t) = 1.23402130221 + 3.1412436463*t + -4.89936171114*t**2 + -0.000424401050714*t**3 Compare to exact: y(t) = 1.234 + 3.1415*t - 4.9*t**2 Estimate for g = 9.79872342227 . So, in this case, we were able to show not only that the data fits a parabola well, but that the higher order term (for $t^3$) is negigible!! Great science! In practice, however, for non-perfect data, this does not work out. The higher-order term introduces an extreme sensitivity to the noise, which can render the results inconclusive. . Exercise: Go back to where the data is generated, and uncomment the line that says &quot;# for later; add noise in&quot; and re-run the fitting. You will find that the coefficients for the cubic polynomial do not resemble the original values found at all, whereas the coefficients for a quadratic polynomial, while not being the same as before, will still be &quot;close.&quot; . Thus, by hypothesizing a parabolic dependence, we&#39;re able to correctly deduce the parameters of the motion (initial position &amp; velocity, and acceleration), and we get a very low error in doing so. :-) Trying to show that higher-order terms in a polynomial expansion don&#39;t contribute...that worked for &quot;perfect data&quot; but in a practical case it didn&#39;t work out because polynomials are &quot;ill behaved.&quot; Still, we got some useful physics out of it. And that works for many applications. We could stop here. . ...although... . What if our data wasn&#39;t parabolic? Sure, for motion in a uniform gravitational field this is fine, but what if we want to model the sinusoidal motion of a simple harmonic oscillator? In that case, guessing a parabola would only work for very early times (thanks to Taylor&#39;s theorem). Sure, we could fit a model where we&#39;ve explictly put in a sine function in the code -- and I encourage you to write your own code to do this -- but perhaps there&#39;s a way to deduce the motion, by looking at the local behavior and thereby &#39;learning&#39; the differential equation underlying the motion. . Exercise: Copy the polyfit() code elsewhere (e.g. to text file or a new cell in this Jupyter notebook or a new notebook) and rename it sinefit(), and modify it to fit a sine function instead of a polynomial: . $$y(t) = A sin( omega t + phi),$$ . where the fit parameters will be the amplitude $A$, frequency $ omega$ and phase constant $ phi$. Try fitting to data generated for $A=3$, $ omega=2$, $ phi=1.57$ on $0 le t le 10$. As an example, you can check your answer against this. . The discussion goes on, but I&#39;m breaking it off into a &quot;Part Ib&quot; for a separate post. In that post, we&#39;ll switch from fitting the data &quot;globally&quot; to looking &quot;locally,&quot; in preparation for work in &quot;Time Series Prediction.&quot; -SH . . Afterward: Alternatives to &quot;Simple&quot; Gradient Descent . There are lots of schemes that incorporate more sophisticated approaches in order to achieve convergence more reliabily and more quickly than the &quot;simple&quot; gradient descent we&#39;ve been doing. . Such schemes introduce concepts such as &quot;momentum&quot; and go by names such as Adagrad, Adadelta, Adam, RMSProp, etc... For an excellent overview of such methods, I recommend Sebastian Ruder&#39;s blog post which includes some great animations! .",
            "url": "https://drscotthawley.github.io/blog/2017/02/23/Following-Gravity-Colab.html",
            "relUrl": "/2017/02/23/Following-Gravity-Colab.html",
            "date": " • Feb 23, 2017"
        }
        
    
  
    
        ,"post18": {
            "title": "Machine Learning Reference List",
            "content": "This has been my personal reading list, first compiled ca. February 2016 &amp; updated very infrequently (e.g. Oct 2016, Feb 2017, Sept 2017). The field moves so quickly, much of this may have been superseded by now. If you find it useful as well, that’s great. . I’m mostly interested in audio processing, so… . Jump Right Into : Audio Processing via RNNs: . Generation/Synthesis of new sounds based on training set: . Jake Fiala: “Deep Learning and Sound” http://fiala.uk/notes/deep-learning-and-sound-01-intro | GRUV: https://github.com/MattVitelli/GRUV. Btw, found that LSTM worked better than GRU. | John Glover: http://www.johnglover.net/blog/generating-sound-with-rnns.html Glover used LSTM fed by phase vocoder (really just STFT). | Google Magenta for MIDI: https://magenta.tensorflow.org/welcome-to-magenta | Google WaveNet for Audio… https://deepmind.com/blog/wavenet-generative-model-raw-audio/ WaveNet is slow. “Fast Wavenet”: https://github.com/tomlepaine/fast-wavenet | WaveNet in Keras: https://github.com/basveeling/wavenet | . | . | . General Neural Network References: . Books/Guides on Deep/Machine Learning: (all excellent) http://neuralnetworksanddeeplearning.com | http://machinelearningmastery.com | https://www.deeplearningbook.org | Hacker’s Guide to Neural Nets by karpathy | . | Tutorials/Videos: Youtube Playlist on “Deep Learning”, t from Oxford U. by Nando de Freitas https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu | Andrew Ng’s online course on ML at Stanford comes highly recommended: http://www.youtube.com/view_play_list?p=A89DCFA6ADACE599 | Stanford Tutorial: http://ufldl.stanford.edu/tutorial/ | . | Concepts in NN/Deep Learning: Backpropagation (i.e. the chain rule): neuralnetworksanddeeplearning.org book, chapter 2 | . | Chris Olah on backprop: http://colah.github.io/posts/2015-08-Backprop/ | . | Karpathy on backprop: http://cs231n.github.io/optimization-2/ | . | . | . | . Recurrent Neural Networks (RNN) (which mostly feature LSTM nowadays): . RNNs in general: Karpathy post: http://karpathy.github.io/2015/05/21/rnn-effectiveness/, Karpathy talk: https://www.youtube.com/watch?v=yCC09vCHzF8 Excellent annotated Char-NN in Keras tutorial: http://ml4a.github.io/guides/recurrent_neural_networks/ | . | Andrew Trask post/tutorial: https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/ | Denny Britz post: http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/ | Class notes/tutorial (long!): http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf | CS class notes (short): https://www.willamette.edu/~gorr/classes/cs449/rnn1.html | Excellent post by Ross Goodwin RNNs: https://medium.com/@rossgoodwin/adventures-in-narrated-reality-6516ff395ba3#.q2xh8dp5t | Great List of references; https://handong1587.github.io/deep_learning/2015/10/09/rnn-and-lstm.html | . | in TensorFlow: https://www.tensorflow.org/versions/r0.8/tutorials/recurrent/index.html | Theano tutorial: http://deeplearning.net/tutorial/rnnslu.html | Batch Normalization for: https://arxiv.org/abs/1510.01378: “applying batch normalization to the hidden-to-hidden transitions of our RNNs doesn’t help the training procedure. We also show that when applied to the input-to-hidden transitions, batch normalization can lead to a faster convergence of the training criterion but doesn’t seem to improve the generalization performance” | . Traditional RNNs suffer from vanishing/exploding gradient. Hence LSTM &amp; others… . Long Short-Term Memory (LSTM): . Tutorial: http://nbviewer.jupyter.org/github/JonathanRaiman/theano_lstm/blob/master/Tutorial.ipynb | Chris Olah post: http://colah.github.io/posts/2015-08-Understanding-LSTMs | Zach Lipton post, “Demystifying LSTM” (with Tutorial theano code): http://blog.terminal.com/demistifying-long-short-term-memory-lstm-recurrent-neural-networks/ | Demo: Lightweight Theano-LSTM: https://github.com/JonathanRaiman/theano_lstm | Massive 33-page review article by Lipton et al: http://arxiv.org/abs/1506.00019 | As of March 2016, Keras forum posts show that “stated” RNNs are still an active dev issue. (As of last year, Keras has LSTM but was resetting the “state”, = inconvenient &amp; slow.)….Update Sept 2016: Seems to be fixed | LSTM tutorial in Tensorflow: https://www.tensorflow.org/versions/r0.10/tutorials/recurrent/index.html | Stateful LSTM in Keras for time-series prediction: https://github.com/fchollet/keras/blob/master/examples/stateful_lstm.py Much-need Docs on stateful LSTM in Keras: http://philipperemy.github.io/keras-stateful-lstm/ | . | Tensorflow sequence prediction: http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html | LSTM backpropagation tutorial :-) http://arunmallya.github.io/writeups/nn/lstm/index.html#/ | . LSTM Alternatives/advances: . GRU (Gated Recurrent Unit) by Cho et al, “Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation”, http://arxiv.org/pdf/1406.1078v3.pdf (2014) Chung et al. Good exp of GRU &amp; LSTM, say GRU comparable to LSTM, http://arxiv.org/abs/1412.3555 | But GRUV/MVitelli found that LSTM outperformed GRU for audio accuracy | GRU’s are a bit simpler than LSTM, Britz blog/tutorial: http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano | . | ClockWork-RNN by Koutnik et al, http://arxiv.org/pdf/1402.3511v1.pdf | Highway networks… | Echo State Networks (ESN). (2008) Comparison of MLP, RNN &amp; ESN for sequence modeling: https://www.researchgate.net/publication/224374378_A_comparison_of_MLP_RNN_and_ESN_in_determining_harmonic_contributions_from_nonlinear_loads | Undecimated Fully Convolutional Neural Networks (UFCNN): http://arxiv.org/pdf/1508.00317.pdf | ConvNet for Audio – Spotify analysis &amp; recommendation: http://benanne.github.io/2014/08/05/spotify-cnns.html | . LSTM for Sequence to Sequence Learning: . Main paper: http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf | There’s an encoder step and a decoder step | Example: https://bigaidream.gitbooks.io/subsets_ml_cookbook/content/dl/theano/theano_keras_sequence2sequence.html#keras-for-sequence-to-sequence-learning | Keras Seq2Seq extension: https://github.com/farizrahman4u/seq2seq | Multiple blog pages, re. language model: https://indico.io/blog/sequence-modeling-neuralnets-part1/ | Tensor flow tutorial: https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html WARNING: “It takes about 18GB of disk space and several hours to prepare the training corpus.” :-( | SE post on pitch-shift mapping http://stats.stackexchange.com/questions/220307/rnn-learning-sine-waves-of-different-frequencies | Denoising: http://mlsp.cs.cmu.edu/people/rsingh/docs/waspaa2015.pdf | Denoising Autoencoder: https://www.quora.com/Is-it-possible-to-create-an-adaptive-filter-using-neural-network-so-that-after-training-it-can-filter-noisy-signal-and-give-desired-output, https://www.quora.com/Can-a-denoising-autoencoder-remove-or-filter-noise-in-a-noisy-signal-like-audio-and-recover-the-clean-signal | Dereverberation: | . | . Extended Memory Architectures: . Memory Networks, Weston, First paper: https://arxiv.org/abs/1410.3916. | Tutorial: http://www.thespermwhale.com/jaseweston/icml2016/ | End-to-End version: http://arxiv.org/abs/1503.08895 | Keras version: https://github.com/fchollet/keras/blob/master/examples/babi_memnn.py | . | Stack-Augmented Recurrent Nets, Joulin &amp; Mikolov “Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets,” https://arxiv.org/pdf/1503.01007.pdf (2015) | Neural Turing Machines (NTM), Graves et al, https://arxiv.org/pdf/1410.5401v2.pdf | Neural Stack Machines: Original paper: “Learning to Transduce with Unbounded Memory” by Grefenstette et al.: https://arxiv.org/abs/1506.02516 | Trask’s tutorial blog on Neural Stack Machines | . | . Convolutional Neural Networks: . Video: “What is wrong with convolutional neural nets?” Geoffrey Hinton, Fields Institute, August 2017”https://www.youtube.com/watch?v=Mqt8fs6ZbHk&amp;feature=youtu.be | Excellent: “History of major convnet architectures” (LeNet, AlexNet, Inception ResNet, VGG,…) https://culurciello.github.io/tech/2016/06/04/nets.html | Excellent: “A guide to convolution arithmetic for deep learning” by Dumoulin and Visin https://arxiv.org/pdf/1603.07285.pdf | Glossary/Summary of conv net terms/concepts: Vector: not a true vector in the sense of vector calculus. Just a one-dimensional array. “N-dimensional vector” = 1-D array with N elements. | Tensor: not a true tensor in the sense of differential geometry. Just a multi-dimensional arry or “matrix”. | Affine Transformation: General math term; here we just mean multiplying by a tensor and (maybe) adding a constant bias (vector). Generalization of “linear transformation.” | Convolution: Pretty much what you’d normally think of “convolution” in the DSP sense. The following analogy helps me too: Evaluating a finite-difference stencil on a discretised scalar field via a banded (e.g. tridiagonal) matrix would be considered a convolution in the CNN sense, because said matrix is sparse and the same weights are used throughout. | Channel Axis: (quoting D&amp;V): “is used to access different views of the data (e.g., the red, green and blue channels of a color image, or the left and right channels of a stereo audio track).” | Feature Map: Generally, the output of running one particular convolution kernel over a data item (e.g over an image). However there are also input feature maps, examples of which are the “channels” referred to earlier (e.g. RGB, Left/Right). | Flattening: turn a tensor into a vector | Pooling: can think of it like a special type of convolution kernel (except it may not just add up the kernel’s inputs). Usually “Max Pooling”, as in: take the maximum value from the the kernel’s inputs. (On the other hand, “Average pooling” really is just a regular top-hat convolution.) In contrast to regular convolution, pooling does not involve zero padding, and pooling often takes place over non-overlapping regions of the input. | (Zero-)Padding: Pretty much like in the DSP sense: add zeros to the front or end of a data stream or image, so that you can run convolution kernel all the way up to &amp; over the boundaries of where they data’s defined. | Transposed Convolution: Analagous to transposing a matrix to get an output with oppositely-ordered shape, e.g. to go from an output feature map of one shape, back to the original shape of the input. There seems to be some confusion, whereby some people treat the transpose as if it’s an inverse, like ATA=IA^T A = IATA=I. ?? | 1x1: Actually 1x1xC, where C is, e.g. the number color channels in a RGB image (3). . | an observation: the bigger shape of the kernel, the smaller the shape of its output feature map | . | Related: Deconvolutional Networks http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf | . Reinforcement Learning: . OpenAIGym: https://openai.com/blog/openai-gym-beta/ Tutorial for cart-pole problem: http://kvfrans.com/simple-algoritms-for-solving-cartpole/ | . | For games: Giraffe | Atari DRL: Video: https://www.youtube.com/watch?v=V1eYniJ0Rnk | BetaGo: https://github.com/maxpumperla/betago | . Data Representation: . Scattering Hierarchy (multi-scale representation) by Mallat (2012-2014), Pablo Sprechman’s talk https://youtu.be/OS6rZXKVU1Y?t=20m44s . Related Approaches to Neural Networks: (historical) . Hidden Markov Models (HMM). Dahl used for text classification: George E. Dahl, Ryan P. Adams, and Hugo Larochelle. “Training restricted boltzmann machines on word observations.” arXiv:1202.5695v1 (2012) | Support Vector Machine (SVM). SVMs are globally convex, which is nice (whereas NNs are only locally convex). Very effective for classification tasks. But NNs have beat them out for complex datasets &amp; tasks. Audio app: e.g., Audio Classificiation by Gou &amp; Li (2003) http://www.ee.columbia.edu/~sfchang/course/spr-F05/papers/guo-li-svm-audio00.pdf | Restricted Boltzmann Machine (RBM). Hinton et al. mid-2000s | . Frameworks (too many to choose from!): . Main Ones: Theano - mature codebase, non-CUDA GPU support via libgpuarray | TensorFlow - Google-supported, awesome viz tool TensorBoard | Keras, runs on Theano or TensorFlow as backends. VERY popular | Torch - used by LeCun &amp; Karpathy, scripting in Lua. Not Python. * PyTorch Python bindings for Torch, includes ‘automatic differentiation’ . | Scikit-Learn - General system for many methods; some Keras support. Allows ‘easy’ swapping of different ML methods &amp; models | . | Others, not seeing these used as much: Caffe, supposed to be easy &amp; abstract | Lasagne - Another Theano front end for abstraction &amp; ease of use | Mozi, Another one build on Theano. Looks simple to use | DeepLearning4J: The “J” is for “Java” | scikits.neural - not popular | . | Which package to choose when starting out? I say Keras. Everything’s super-easy and automated compared to others. | . | . More Tutorials (e.g., app-specific): . Lots in http://maachinelearningmastery.com | Andrew Trask’s Blog: Andrew writes excellent tutorials. The first LSTM guide I read was his. | Tutorials on Theano, Keras, Lasagne, RNN: https://github.com/Vict0rSch/deep_learning | Theano: Theano basics: http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb | Then crash course via code: https://github.com/Newmu/Theano-Tutorials | LSTM in Theano: http://nbviewer.jupyter.org/github/JonathanRaiman/theano_lstm/blob/master/Tutorial.ipynb | . | Tensorflow: TensorFlow graph vis tutorial: https://www.tensorflow.org/versions/r0.8/how_tos/graph_viz/index.html | TensorFlow on AWS (tutorial video): https://www.youtube.com/watch?v=1QhCsO4jmoM | LSTM tutorial in Tensorflow: https://www.tensorflow.org/versions/r0.10/tutorials/recurrent/index.html | . | Torch: First, Lua: “Learn Lua in 15 Minutes”: http://tylerneylon.com/a/learn-lua/ | Deep Learning in Torch: a 60-minute Blitz by Soumith Chintala at CVPR2015: https://github.com/soumith/cvpr2015/blob/master/Deep%20Learning%20with%20Torch.ipynb | LSTM in Torch (by Zaremba) https://github.com/wojzaremba/lstm | Tutorial Videos: https://www.youtube.com/playlist?list=PLLHTzKZzVU9ebuL6DCclzI54MrPNFGqbW | . | . More Demos: . Anything by @karpathy | Lightweight Theano-LSTM: https://github.com/JonathanRaiman/theano_lstm | TensorFlow Playgrounds: http://playground.tensorflow.org | . Audio Applications: . Huang et al, “Deep Recurrent NNs for Source Separation” http://posenhuang.github.io/papers/Joint_Optimization_of_Masks_and_Deep%20Recurrent_Neural_Networks_for_Monaural_Source_Separation_TASLP2015.pdf Qutoe: “in parallel, for improving the efficiency of DRNN training, utterances are chopped into sequences of at most 100 time steps” | . | Ron Weiss (Google) talk: “Training neural network acoustic models on waveforms” https://www.youtube.com/watch?v=sI_8EA0_ha8 | Music comp: http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/ | Predict time sequence with LSTM &amp; Theano…GRUV | MULTI-RESOLUTION LINEAR PREDICTION BASED FEATURES FOR AUDIO ONSET DETECTION WITH BIDIRECTIONAL LSTM NEURAL NETWORKS Erik Marchi1 , Giacomo Ferroni2 , Florian Eyben1 , Leonardo Gabrielli2 , Stefano Squartini2 , Bjorn Schuller ¨ 3,1 http://mediatum.ub.tum.de/doc/1238131/865625.pdf | John Glover on generating instrument sounds with RNN: http://www.johnglover.net/blog/generating-sound-with-rnns.html | Example of using spectrograms as images (for an image-based classifier): http://stackoverflow.com/questions/37213388/keras-accuracy-does-not-change | . Datasets of (mostly musical) Audio for Machine Learning: . IRMAS: for musical Instrument recognition: http://www.mtg.upf.edu/download/datasets/irmas . | Fraunhofer IDMT datasets: (Scroll down to “Published Datasets” on http://www.idmt.fraunhofer.de/en/business_units/m2d/research.html) IDMT-SMT-Bass An audio database for bass transcription and signal processing | IDMT-SMT-Audio-Effects An audio database for automatic effect detection in recordings of electric guitar and bass | IDMT-SMT-Bass Synthesis A Digital Waveguide Model of the Electric Bass Guitar including Different Playing Techniques | IDMT-SMT-BASS-SINGLE-TRACK | Multi-track studio recordings of live performances in Swing, Blues and Funk styles | IDMT-SMT-Guitar An audio database for guitar transcription and signal processing | IDMT-SMT-Drums An audio database for drum transcription and source separation | Multicodec Invdec Tampering Dataset | . | Massive list of datasets (most are MIDI though): http://www.audiocontentanalysis.org/data-sets/ | Another massive list of datasets (with many repeats from above): http://wiki.schindler.eu.com/doku.php?id=datasets:overview . | Melody annotation dataset: http://medleydb.weebly.com/description.html . | Binaural audio: : Antoine Deleforge: http://perception.inrialpes.fr/~Deleforge/AVASM_Dataset/ | . | . Activations &amp; Optimizers . ELU: Exponential Linear Unit, seems to work better than ReLU in many cases https://arxiv.org/pdf/1511.07289v1.pdf | . In Physics: . “Fast cosmological parameter estimation using neural networks”, T. Auld, M. Bridges, M.P. Hobson and S.F. Gull, MNRAS. 000, 1–6 (2004), https://arxiv.org/pdf/astro-ph/0608174.pdf | “Parameterized Neural Networks for High-Energy Physics”, Baldi, P., Cranmer, K., Faucett, T., Sadowski, P., Whiteson, D. The European Physical Journal C. 76, 235, 1-7, May 2016, 2016, https://arxiv.org/pdf/1601.07913.pdf | . Hardware: . Amazon Web Services (AWS): Stanford disk image (AMI) with everything preinstalled: https://cs231n.github.io/aws-tutorial/ | …or just grab some other “Community AMI” with CUDA etc installed | Another AWS setup: https://github.com/andreasjansson/simple-aws-gpu-setup | TensorFlow on AWS (tutorial video): https://www.youtube.com/watch?v=1QhCsO4jmoM | My AWS Aetup: ami-a96285c4 ( AMI: old cuda but works: ami-63bf8209 do not like: 11777_MML (ami-37a58f5d) or DeepestLearning ) When u create your own AMI it brings your server down. :-( | . | . pip install --upgrade pip pip install -U numpy sudo pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps sudo pip install awscli h5py git clone &lt;https://github.com/fchollet/keras.git&gt; cd keras sudo python setup.py install . FACTOR OF 10 SPEEDUP using the g2.xlarge GPUs vs my Macbook Pro (no GPU)!! . Checkpointing: . run ‘watch’ command to execute AWS transfer to S3 ever seconds ...and spot instance went down without any checkpoint to allow uploading from EC2 to S3 it&#39;s convoluted: install aws cli create an &quot;IAM&quot; user. Grant the user permissions to upload to s3 via &lt;https://forums.aws.amazon.com/thread.jspa?messageID=600007&gt; aws configure ...good to go. . watch -n 550 aws s3 cp /tmp/weights.hdf5 s3://hawleymainbucket . Theano GPU setup guide: https://github.com/andreasjansson/simple-aws-gpu-setup | OpenMP: Don’t forget to enable multiple OpenMP threads! Can get you at least a factor of 2 speedup! In most ‘modern’ Python installations (e.g. anaconda) OpenMP is automatic | . | My proposed PC build: https://pcpartpicker.com/user/drscotthawley/saved/bFZ8dC | | . Self-Organizing Maps: . “Unsupervised Classification of Audio Signals by Self-Organizing Maps and Bayesian Labeling”: http://link.springer.com/chapter/10.1007%2F978-3-642-28942-2_6 | “Visualization of Tonal Content in the Symbolic and Audio Domains“ http://www.ccarh.org/publications/cm/15/cm15-10-toiviainen.pdf | . “Weird Stuff”: . Stochastic path Deep NN for image rec: http://arxiv.org/pdf/1603.09382v1.pdf | .",
            "url": "https://drscotthawley.github.io/blog/2017/02/06/Machine-Learning-Reference-List.html",
            "relUrl": "/2017/02/06/Machine-Learning-Reference-List.html",
            "date": " • Feb 6, 2017"
        }
        
    
  
    
        ,"post19": {
            "title": "How to Port-Forward Jupyter Notebooks",
            "content": "Jupyter notebooks and their related iTorch notebooks seem to be popular for tutorials, e.g the Deep Learning with Torch tutorial I started today, but I hated them and avoided them – not just for the reasons on this list, but mainly because I keep my “machine learning machine” inside my university’s firewall and this has made it a pain to run “notebooks” if you’re off-site. . When I work from home, I do so via a couple ssh hops, and then copy and paste my script files into the terminal window. But these “notebook” things require a web GUI, and X11 forwarding over mutliple ssh sessions is prohibitively slow. . I didn’t want to configure a NAT system, and was considering some kind of homegrown CGI-script system (which has “security breach” written all over it), but thankfully I stumbled upon this post on Coderwall, where the process was spelled out. For my set of systems, I needed an additional layer of port-forwarding. So for me it goes like this… . My machine-learning computer, which we’ll call “internal”, sits inside the firewall. On internal, I run the Jupyter notebook… me@internal$ jupyter notebook --no-browser --port=8889 or for torch, similarly, me@internal:~$ itorch notebook --no-browser --port=8889 This generates a bunch of text, including a URL with a token. It’ll say… . Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:8889/?token=96c92fc27f102995044da89ae111914c28e51757d57bebfc . | The server we’ll call “doorkeeper” is visible to the outside world, and so we forward its port 8889 to the one over on “internal” where the notebook is running: me@doorkeeper:~$ ssh -N -n -L 127.0.0.1:8889:127.0.0.1:8889 internal . | Then on my laptop, I run a similar port-forward so the browser will connected to the port on doorkeeper: me@laptop:~$ ssh -N -n -L 127.0.0.1:8889:127.0.0.1:8889 doorkeeper . | And then on my laptop, I paste the URL from the jupyter (or itorch) notebook into my web browser… http://localhost:8889/?token=96c92fc27f102995044da89ae111914c28e51757d57bebfc …and it works! The notebook comes right up, but the only lag involves sending text over ssh, as opposed to sending X11 graphics. . | (incidentally, I find ‘localhost’ sometimes doesn’t resolve, which is why I use 127.0.0.1 explicitly) . Wohoo! . . Extra: Remotely Editing Files via rmate &amp; Sublime Text . On laptop, in Sublime Text 3: Tools &gt; Command Pallete &gt; Install package &gt; rsub . On internal (server): sudo apt-get install ruby; sudo gem install rmate . Make two reverse-SHH tunnel hops from laptop to doorkeeper to internal: . ssh -R 52698:127.0.0.1:52698 doorkeeper . | (on doorkeeper) ssh -R 52698:127.0.0.1:52698 internal . | (then on internal) rmate [whatever file you want to edit] . | …and suddenly, your file appears in your Sublime Text window on your laptop! . To automate this, either alias ssh to be “ssh -R 52698:127.0.0.1:52698” or modify your ~/.ssh/config file(s) .",
            "url": "https://drscotthawley.github.io/blog/2017/02/03/How-To-Port-Forward-Jupyter-Notebooks.html",
            "relUrl": "/2017/02/03/How-To-Port-Forward-Jupyter-Notebooks.html",
            "date": " • Feb 3, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "I dabble in a variety of technologies, usually with the goal of improving the physics education of audio engineering students. Professorly Web Page . . Affiliations . Belmont University . ASPIRE Research Co-op . More Information . My music website . Contact me . How about a message on twitter? @drscotthawley . . This site was built using Fastpages by Hamel Husain .",
          "url": "https://drscotthawley.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://drscotthawley.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}