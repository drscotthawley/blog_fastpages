<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Scottergories: Making GPT-3 Generate 3 of a Kind | Scott H. Hawley</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Scottergories: Making GPT-3 Generate 3 of a Kind" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In which we test GPT-3’s ability generate members of a category" />
<meta property="og:description" content="In which we test GPT-3’s ability generate members of a category" />
<link rel="canonical" href="https://drscotthawley.github.io/blog/classification/scottergories/gpt-3/2021/02/15/Making-GPT3-Generate-3-Of-A-Kind.html" />
<meta property="og:url" content="https://drscotthawley.github.io/blog/classification/scottergories/gpt-3/2021/02/15/Making-GPT3-Generate-3-Of-A-Kind.html" />
<meta property="og:site_name" content="Scott H. Hawley" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-15T13:33:31-06:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://drscotthawley.github.io/blog/classification/scottergories/gpt-3/2021/02/15/Making-GPT3-Generate-3-Of-A-Kind.html"},"url":"https://drscotthawley.github.io/blog/classification/scottergories/gpt-3/2021/02/15/Making-GPT3-Generate-3-Of-A-Kind.html","@type":"BlogPosting","headline":"Scottergories: Making GPT-3 Generate 3 of a Kind","dateModified":"2021-02-15T13:33:31-06:00","datePublished":"2021-02-15T13:33:31-06:00","description":"In which we test GPT-3’s ability generate members of a category","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://drscotthawley.github.io/blog/feed.xml" title="Scott H. Hawley" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-173006102-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Scott H. Hawley</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
<style>
ol.bibliography li { list-style: none }

ol li li{
    list-style-type: lower-alpha;
}

ol li li li{
    list-style-type: lower-roman;
}
</style>

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Scottergories: Making GPT-3 Generate 3 of a Kind</h1><p class="page-description">In which we test GPT-3's ability generate members of a category</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-15T13:33:31-06:00" itemprop="datePublished">
        Feb 15, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      17 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#classification">classification</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#scottergories">scottergories</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#gpt-3">gpt-3</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#three-of-a-kind">Three of a Kind</a></li>
<li class="toc-entry toc-h2"><a href="#discussion-preliminary-thoughts">Discussion: Preliminary Thoughts</a></li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul><p>3170 words,  17 minute read, 11 paperback pages</p>

<p><em>I’m a computational physicist who spent most of his life only interested in curve-fitting, i.e. “regression.” But  due to my participation in interdisciplinary activities over the past few years, as well as a mere survey of the activities in the machine learning community, I’ve become fascinated in “classification” as performed by humans and machines, to the point where I’m in the process of writing a popular-level book on classification and categories.  (For a couple recent works of mine that include some of this, see <a class="citation" href="#hawleywhoserules">[1]</a> and <a class="citation" href="#hawleykruger">[2]</a>.)  It’s the “by machines” part that is the topic of this post, in which we “force a bot” to do something humans can do – play category-driven games!</em></p>

<p>Back in June of 2020, I applied for OpenAI API “GPT-3” <a class="citation" href="#gpt3">[3]</a> access immediately upon its announcement….and then heard nothing until February 13, 2021, <strong>eight months later</strong>.  My proposed work was to explore the GPT-3 model’s capacity for handling categories, something I’ll refer to loosely by the shorthand of “categorical reasoning” (but don’t take this to mean that I assert GPT-3 actually “reasons” anything.)  When I filled out the form requesting access, here’s what I wrote:</p>

<blockquote>
  <p>“I’m interested in the modeling and representation of categories using machine learning, within both a human historical context of the histories of philosophy and psychology, particularly as concepts are intertwined with language. I have been exploring how language models can embed conceptual knowledge such as categories. My sabbatical this fall includes writing a popular-level book about the history of classification with a goal towards relating this to the societal impact of deployed machine learning models. The eBook contains interactive graphical examples. (I gave a <a href="https://hedges.belmont.edu/~shawley/classy/Wheaton2019talk">talk at Wheaton on this</a> last fall.) This interdisciplinary work began in Oxford two years ago and has been partially supported by a grant from the Templeton and Blankmeyer foundations.</p>

  <p>One concrete problem related to this effort is the fairly well-posed question of to what extent sophisticated language models contain sufficient categorical ‘knowledge’ and ‘reasoning’ capacity to play (or officiate) a game such as “Scattergories” — in which unusual, <strong>never-before-seen</strong> categories are introduced during play (e.g., “things you’d find under a bed”). Humans do this fairly well. My experiments so far with Word2Vec embeddings and with GPT-2 (via the <a href="https://transformer.huggingface.co/">HuggingFace API</a>) have indicated that the various GPT-2 models are poorly-suited to this task. I am applying for access to the OpenAI API because I’m curious to see how GPT-3 fares. Categorical reasoning is foundational to intelligence, so such a task is relevant for both for understanding human intelligence, as well as for advancing the development of general artificial intelligence [the mission of OpenAI].”</p>
</blockquote>

<p>(For examples of just how poorly GPT-2—the older, smaller model—performs at this task, see the “Discussion” below.  Believe me when I say the results were disappointing.)</p>

<p>Now, with GPT-3 access, we can begin to see how this newer, bigger model performs.  The essence of using the API is the “Prompt”.  So, my job is to craft prompts, and then see how well the model responds.  My first experiment with the API involves getting the model to generate “three of a kind”:</p>

<h2 id="three-of-a-kind">
<a class="anchor" href="#three-of-a-kind" aria-hidden="true"><span class="octicon octicon-link"></span></a>Three of a Kind</h2>

<p>Let’s see if GPT-3 can expound in threes.  I’ll write my prompt in <strong>bold</strong>, and then <u>underlined</u> text will show the model’s completion/output. <em>Additional commentary from me will be in italics.</em></p>

<p><strong>This application generates example members of a category.  Examples of fruits are apples, oranges and bananas.  Examples of birds are</strong> <u>quails, crows and sparrows</u>.  <em>(Whoa! Not bad! This is with the “davinci” model, with Response Length = 10 tokens, Temperature = 0.7, Top P = 1, etc..all defaults.)</em></p>

<p><strong>This application generates three example members of a category.  Three examples of fruits are apples, oranges and bananas.  Three examples of dogs are</strong> <u>Labrador, Rottweiler and Husky</u>. <em>(Had to increase response length to 15 and then truncate the extra text which read “The input is”)</em></p>

<p><em>(For the following, just assume that I’m truncating unwanted extra text. Also, I’ll only show the last line of each prompt: The “This application…” and the fruits example are in each prompt, I just won’t print them, so as to save space:)</em></p>

<p><strong>Three examples of fish are</strong> <u>salmon, bass and tuna.</u></p>

<p><strong>Three examples of U.S. Presidents are</strong> <u>George Washington, Franklin Delano Roosevelt and Ronald Reagan</u>  <em>(Whoah, this one triggered the “This completion may contain sensitive content” warning!)</em></p>

<p><strong>Three examples of philosophers are</strong> <u>Plato, Descartes and Kant.</u></p>

<p><strong>Three examples of things you’d find in a refrigerator are</strong> <u>milk, orange juice and apple juice.</u> (<em>Sure. But since it’s a random sample, let’s go again:</em>) <strong>Three examples of things you find in a refrigerator are</strong> <u>milk, cheese and vegetable salsa.</u> <em>(Not bad!)</em></p>

<p><strong>Three examples of things you’d find under a bed are</strong> a <u>mouse, a handkerchief and a ball.</u> <em>(Ok, I guess…why not? Again:)</em>  <strong>Three examples of things you’d find under a bed are</strong> s<u>ocks, shoes and books.</u> <em>(ok…I can buy that.)</em></p>

<p><strong>Three examples of 80’s power ballads are</strong>: <u>"Faith" by George Michael, "I Want to Know What Love Is" by Foreigner and "The Power of Love" by Huey Lewis and the News.</u> (uh…if you say so. At least these are all in the 80s!)*</p>

<p><strong>Three examples of household chores are</strong> <u>cleaning, cooking and sweeping.</u> <em>(or again:)</em> <strong>Three examples of household chores are</strong> <u>washing, cleaning and ironing.  Three examples of sports are swimming, running and cycling.</u> <em>(The model added that second sentence all by itself!  Note all of these are gerunds, i.e. -ing words; it didn’t say, for example, something odd/wrong like “clean, mop, broom”)</em></p>

<p><strong>Three examples of things you don’t want your mom to know about you are</strong>: <u>a. how drunk you were at the last party b. that you smoke pot c. that you plagiarized on that paper for your English class.</u> <em>(Whoa, have we reached AGI? ;-) And extra points for the use of “a. b. c.”)</em></p>

<p><strong>Three examples of items you take on a trip are</strong> <u>a car, a suitcase and a book.</u> <em>(You CAN “take” a car, can’t you? hah! Nice linguistic anomaly.)</em></p>

<p><strong>Three examples of fruits of the Holy Spirit are</strong> <u>faith, hope and love.</u> <em>(I mean, I want share this on @FaithTechHub, so…right? LOL)</em></p>

<p><em>OK, I am sufficiently impressed and have now gone to get my official Hasbro(TM) Scattorgories game…The following prompts are from the game</em></p>

<p><strong>Three examples of things that are sticky are</strong> <u>gum, tape and bubble gum.</u> <em>(<a href="https://www.youtube.com/watch?v=SAfq55aiqPc">Nooice!</a>)</em></p>

<p><strong>Three examples of things you add water to are</strong> <u>eggs, rice and pasta.</u> <em>(eggs?  Hmm…go again:)</em> <strong>Three examples of things you add water to are</strong> <u>tea, coffee and milk.</u> <em>(nice. …wait, milk?)</em>  <strong>Three examples of things you add water to are</strong> <u>urns, cups, and buckets.</u> <em>(whoa!)</em></p>

<p><strong>Three examples of terms of measurement are</strong> <u>kilometers, centimeters and grams.  Three examples of colors are red, blue and green.  Three examples of adjectives are red, blue and green.  Three examples of verbs are run, jump</u> <em>(See? It will keep generating categories if you set Response Length longer. Let me make the Response Length even longer, to 200…)</em></p>

<p><strong>Three examples of terms of measurement are</strong> <u>3, 4 and 5 meters.  Three examples of angles are 20 degrees, 40 degrees and 60 degrees.  Three examples of countries are Japan, France and Germany.</u> <em>(Ok and beyond that it just repeats itself and/or repeats similar sentences, like: “The application generates the following sentences.  This application generates three examples of verbs.  The application generates the following sentences. This application generates three examples of pronouns.” and so on)</em></p>

<p><strong>Three examples of things found in the ocean are</strong> <u>fish, coral and whales.  Three examples of things found in the sky are clouds, stars and planets.</u> <em>(that second sentence was generated by the model)</em></p>

<p><strong>Three examples of things that are bright are</strong> <u>the sun, the moon and the stars.  Three examples of things that are made of wood are tables, chairs and fences.</u> <em>(Again, the second sentence was generated.  Just in case it’s got a heavenly-body fixation, let’s go again:)</em>  <strong>Three examples of things that are bright are</strong> <u>the moon, the sun and an electric bulb.  Three examples of furniture that a household might have are a sofa, a chair and a table.</u> <em>(Good job!  “furniture” is a tricky category as described by George Lakoff <a class="citation" href="#Lakoff87">[4]</a>)</em></p>

<p><strong>Three examples of leisure activities are</strong> <u>simple relaxation, snowboarding and surfing.  Three examples of countries are USA, Canada and Mexico.</u> <em>(Not bad)</em></p>

<p><strong>Three examples of reptiles/amphibians are</strong> <u>lizards, frogs and crocodiles.</u> <em>(Ok, that “slashed” construction of “reptiles/amphibians” was literally what was printed on the card. I had not expected GPT-3 to pull that off, but mixing reptiles (lizards &amp; crocodiles) with amphibians (frogs) is pretty cool.  Let’s try just reptiles:)</em>  <strong>Three examples of reptiles are</strong> <u>lizards, snakes and turtles.  Three examples of mammals are cows, sheep and giraffes. In each example, the category is written in bold letters and</u> <em>(Ok, now that’s just cheeky!)</em></p>

<p><strong>Three examples of things you do online are</strong> <u>search, chat and email.</u>  <em>(Note how it generates only things that are…arguably..verbs.  Again:)</em> <strong>Three examples of things you do online are</strong> <u>checking email, shopping and playing games.</u> <em>(My goodness, it’s full of gerunds!)</em></p>

<p><em>(Yea, let’s try gerunds, and let’s bump the Response Length up to 145…)</em></p>

<p><strong>Three examples of gerunds are</strong> <u>eating, running and swimming.  Three examples of adjectives are large, strong and important.  Three examples of verbs are write, jump and try.  Three examples of adverbs are quickly, happily and carefully.  Three examples of adjectives are big, wide and dangerous.  Three examples of prepositions are to, in and on. This application generates three examples of a category. All examples are of the same gender, number and case.  Three examples of nouns are dog, cat and bird.  Three examples of adjectives are tall, small and large.  Three examples of prepositions are by, for and under.  Three examples of adverbs are quickly, slowly and</u> <em>(SHOW OFF!)</em></p>

<p><strong>Three examples of terms of endearment are</strong> <u>honey, sweetheart and dear.</u> <em>(awww)</em></p>

<p><em>Ok, I’ve now gone and grabbed my boxed copy of <a href="https://www.thegameofthings.com/">The Game of Things</a>.  Now, keep in mind, some of these things can be of a “suggestive” nature…</em></p>

<p><strong>Three examples of things you would like to do with chocolate are</strong> <u>eat it, cook with</u>
<u>it and share it with your friend.</u>  <em>(Uh…ok, that was not at all suggestive, haha!)</em></p>

<p><strong>Three examples of things you shouldn’t do on your honeymoon are</strong> <u>drink, drive and shop.  Three examples of good economic policies are low interest rates, an independent central bank and low taxes. (COMPLETION MAY CONTAIN SENSITIVE CONTENT)</u>
<em>(All very sensible, though not likely to be what you’ll hear if you play in a group of humans.  Note that the “goal” of the humans would be to make each other laugh, whereas GPT-3 is just trying to generate high-probability matches. It’s basically playing <a href="https://www.familyfeud.com/">Family Feud</a>.)</em></p>

<p><strong>Three examples of things that warrant an apology are</strong> <u>spilling a drink on someone, saying something rude to someone, and being late for an appointment.</u> <em>(Yeah, good job GPT-3!)</em></p>

<p><strong>Three examples of things that would get you fired are</strong> <u>stealing, sleeping on the job and talking back to your boss.</u> <em>(Sure!)</em></p>

<p><strong>Three examples of things you would do if you had super-human powers are</strong> <u>play baseball, run in the Olympics and fly to the moon.</u> <em>(again:)</em> <strong>Three examples of things you would do if you had super-human powers are:</strong> <u>catch a falling satellite, fly from New York to Tokyo, and make a million dollars.</u> <em>(Ok.  Sensible answers, and note that all the verb-usage matches.)</em></p>

<p><strong>Three examples of things you shouldn’t try to hold on to are</strong> <u>a pen, a balloon and a cork.  The idea is that you decide whether the example is an example of the category, or not. You click</u>  <em>(Ok, wait a sec. Not sure about the pen, balloon and cork??, but it seems like it’s scraped some aspect of category-naming-games and included it in here! Let’s re-do a bunch)</em></p>

<p><strong>Three examples of things you shouldn’t try to hold on to are</strong></p>
<ul>
  <li><u>a taxi, a dead rat and a Volkswagen.</u></li>
  <li><u>a hot potato, a hot date and a hot temper.</u></li>
  <li><u>a rock, a bird and a balloon.</u></li>
  <li>
<u>a ball, a cat and a balloon.  Three examples of things you should be careful with are an axe, a knife and a gun.</u> <em>(2nd sentence is part of the output)</em>
</li>
  <li>
<u>a balloon, a basketball and a cactus. In the example, the Generate action is used to randomly generate a member of the category.</u> This <em>(Ok, kudos on the cactus. Not sure where it’s getting its category-gaming info from)</em>
</li>
</ul>

<p><em>(Speaking of <a href="https://www.youtube.com/watch?v=5OAaa7B_GmE">Something To Hold On To</a>, let’s give it an obscure music task just to see how it does:)</em></p>

<p><strong>Three examples of songs by Trevor Rabin are</strong> <u>"Racing in the Street", "Big Generator" and "Can I Play With Madness".</u> <em>(hahaaaha. The first is a Springsteen tune, the 2nd the <a href="https://www.youtube.com/watch?v=8W_VC_BgMjo">title track off Yes’ Big Generator album</a> when Rabin was in the band and supposedly they all shared writing credits, and I would PAY MONEY to see Trevor Rabin cover Iron Maiden’s “<a href="https://www.youtube.com/watch?v=Kvqr366Op3k">Can I Play With Madness</a>”!)</em></p>

<p><em>(BTW, if you want to hear an incredible cover of “Something To Hold On To,” please give <a href="https://soundcloud.com/rextunes/something-to-hold-on-to-master">my good friend Rex Schnelle’s version a listen</a>.  Moving on…)</em></p>

<p><strong>Three examples of things you shouldn’t share are</strong> <u>toothbrushes, clothes and pencils.</u> <em>(I did this over &amp; over and “toothbrushes” was in every output, no matter how high I made the Temperature)</em></p>

<p><strong>Three examples of things you shouldn’t try to do in the dark are</strong> <u>play tennis, play football and drive a car.</u> <em>(All my retries on this were similarly boring; GPT-3 rarely surprises one with its off-the-wall hilarity)</em></p>

<p><em>(Speaking of lack of originality, it’s amazing how many cards in The Game of Things start with “Things you shouldn’t…”)</em></p>

<p><strong>Three examples of things you wouldn’t want to find in your sandwich are</strong> <u>a brick, a piece of glass and a dead rat.</u> <em>(Spot on)</em></p>

<p><em>(I made this one up:)</em> <strong>Three examples of things you wouldn’t want a doctor to say to you are</strong></p>
<ul>
  <li>
<u>"you don't have anything to worry about", "one more time" and "we'll see how it goes".</u> <em>(The model knew to use quotation marks!  First example seems wrong.)</em>
</li>
  <li>
<u>I have good news and bad news, surgery is now a day care procedure and it's not cancer, it's a tumor.</u> <em>(Uh.. huh?)</em>
</li>
  <li>
<u>, "You're dead", "You're pregnant", "You're infected".  Three examples of things you wouldn't want a doctor to say to your wife are</u> <em>(Nice comma usage, and Whoa! Let’s do that last one…)</em>
</li>
</ul>

<p><strong>Three examples of things you wouldn’t want a doctor to say to your wife are</strong></p>
<ul>
  <li>
<u>"I'm sorry" "I love you" and "I'm sorry I love you."</u> <em>(hahaha!)</em>
</li>
  <li>
<u>, "I'm afraid you have cancer", "I'm afraid you have AIDS" and "I'm afraid you have syphilis."  Three examples of names of big cities are London, Paris and Los Angeles.  Three examples of books to read are "War and Peace", "Mein Kampf" and "The Catcher in the Rye".</u> <em>(Ok, I think it’s got the hang of this 3’s-generating now! LOL)</em>
</li>
</ul>

<p><em>Last up: I mentioned this to my friend Heather and she asked me how long I get to ‘keep playing with it’.  I said “My free trial tokens expire May 15, 2021, and I’ve already used 6000 out of 300k tokens. So it’s basically like joining the Chuck E. Cheese club.”  She said, “Nice! I wonder if you can teach it to win at skee ball 😅” Which means now I have to do…</em></p>

<p><strong>Three examples of things you’d find at Chuck E. Cheese are</strong> video games, a ball pit and tokens.  Three examples of dinosaurs are Tyrannosaurus Rex, Triceratops and Velociraptor. <em>(NOT BAD! I’m going to go again, and if it mentions skee ball, WE’RE DONE.)</em></p>
<ul>
  <li>
<u>tokens, pizza and soda.</u> <em>(good)</em>
</li>
  <li>
<u>Chuck E. Cheese himself, his brother, and a clown.</u>  <em>(Ohhh-kay that’s enough for now! haha)</em>
</li>
</ul>

<h2 id="discussion-preliminary-thoughts">
<a class="anchor" href="#discussion-preliminary-thoughts" aria-hidden="true"><span class="octicon octicon-link"></span></a>Discussion: Preliminary Thoughts</h2>
<p>“Surely this is not that ‘intelligent’ of a task,” you say.  “All it does is generate words that have some suitably high probability of being associated with the word in the prompt.”  “Since it was trained on the internet, such lists-of-things-in-a-category probably already exist as web pages somewhere anyway”</p>

<p>Right, one could imagine scraping a ton of web pages for text and then training some kind of non-Transformer-based method, maybe even a Bayesian bag-of-words model (like in a simple spam detector) to rank “related words” and pick the top choices.</p>

<p>That’s not exactly what’s happened here.  Yes, GPT-3 was trained on tons of web pages, yes it “knows” (has a probabilistic model for) word associations, but also…</p>

<p>It is a “few shot learner” so it essentially <em>instantly</em> starts giving me outputs like what I want, without having been pre-trained on the task I want it to do.  The fact that it gets singular vs. plural right, and verbs vs. nouns, and <em>gerunds</em> correct in its answers is noteworthy.</p>

<p>So far, this is just my first night of access to the API, and it blows away what I could get GPT-2 to do. One can try out various GPT-2 model sizes using the friendly interfaces at <a href="https://transformer.huggingface.co/">HuggingFace.com</a>. For the smallest GPT-2 model, I got</p>

<p><strong>Three examples of dogs are</strong> <u>cats.</u>”  <em>(<a href="https://media.giphy.com/media/1AHESbqpjV1AFSNJdA/giphy.gif">Womp womp!</a>)</em></p>

<p>The largest GPT-2 model size, however, with suitably adjusted parameters, <u>could generate 3 dog breeds  or 3 U.S. Presidents correctly</u>.  But any of the more “challenging,” “speculative” categories (e.g. “things you might…”) give even the largest GPT-2 at HuggingFace a hard time:</p>

<p><strong>Three examples of things that would get you fired are</strong> <u>the fire alarm, the bathroom sink and the bathroom mirror.</u> <em>(?? The other various completions generated by GPT-2 when I re-ran it were similarly irrelevant.)</em></p>

<p>One further possible remark to my post here: “You realize you aren’t really playing Scattergories(tm), right?” Yes of course.  Scattergories involves rolling the dice and getting a letter and having to name a bunch of things that start with that letter. <em>Pace.</em> I was never actually wanting to replicate the Scattergories(tm) game engine itself, rather the name of the game was a placeholder for the idea of “categorical reasoning.”  Now…could we get GPT-3 to <em>actually</em> <em>play</em> Scattergories? …I’m not sure I even …care?   I’m not sure how you’d write the prompt for that, but I’m open to trying it.  For now, at just my first night of trying things out, this seems remarkably encouraging.</p>

<p>Which means….based on my experience with AI models: If the initial trial is encouraging, that means there’s a TON of failure modes you just haven’t discovered yet! ;-)  We can probe for those too, another time – so far I’ve only used up 1.6% of my free tokens.   There’s a lot more that one can do to use the API, and so far I haven’t used any of the advanced prompt-making utilities. <strong>Still, for the first foray into the OpenAI API, this was a GREAT experience!</strong></p>

<h2 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>

    <p class="bibliography"><p><span id="hawleywhoserules">[1] S. H. Hawley, "<a href="https://medium.com/faithtech/who-makes-the-rules-whose-labels-to-use-a38cce3a60a7">Who “Makes” The Rules? Whose Labels to Use?
Living By the Spirit in the Age of Machine Learning</a>," <i>Winner of FaithTech Institute’s 2020 Writing Contest</i> (2020).</span></p>
<p><span id="hawleykruger">[2] S. H. Hawley &amp; E. Kruger, "<a href="https://philpapers.org/rec/HAWWDT">What Do Technology and Artificial Intelligence Mean Today?</a>," In H. Fernandez,ed., <i>Sociedad Tecnológica y Futuro Humano, vol. 1: Desafíos conceptuales</i> (2021, in press).</span></p>
<p><span id="gpt3">[3] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, &amp; D. Amodei, "<a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a>," <i>ArXiv</i> (2020).</span></p>
<p><span id="Lakoff87">[4] G. Lakoff, <i><a href="https://www.amazon.com/Women-Fire-Dangerous-Things-Categories/dp/0226468046">Women, Fire and Dangerous Things: What Categories Reveal About the Mind</a></i> (Chicago: University of Chicago Press, 1987).</span></p></p>
    <hr>
    <p>(c) 2020 Scott H. Hawley</p>
  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="drscotthawley/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/classification/scottergories/gpt-3/2021/02/15/Making-GPT3-Generate-3-Of-A-Kind.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Physics prof / Tinkerer. My blog about code and ideas.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/drscotthawley" title="drscotthawley"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/drscotthawley" title="drscotthawley"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/drscotthawley" title="drscotthawley"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
